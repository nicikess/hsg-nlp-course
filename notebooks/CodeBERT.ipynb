{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "history_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/nicikess/hsg-nlp-course/blob/main/notebooks/CodeBERT.ipynb",
      "authorship_tag": "ABX9TyPNQk3WYWiLTDM3PYrrNM2o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicikess/hsg-nlp-course/blob/main/notebooks/CodeBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference to Microsoft CodeBERT: A Pre-Trained Model for Programming and Natural Languages "
      ],
      "metadata": {
        "id": "YgrFUsZJAioe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/microsoft/CodeBERT/tree/master/CodeBERT/codesearch"
      ],
      "metadata": {
        "id": "Kw1sIpBUAlsh"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing the required modules\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jamtkqkVJvHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install utils\n",
        "!pip install wandb\n",
        "!pip install sentencepiece\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import logging\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from torch.utils.data import (DataLoader,\n",
        "                              RandomSampler,\n",
        "                              TensorDataset,\n",
        "                              Dataset,\n",
        "                              SequentialSampler)\n",
        "\n",
        "from transformers import (AdamW,\n",
        "                          RobertaConfig,\n",
        "                          RobertaModel,\n",
        "                          RobertaForSequenceClassification,\n",
        "                          RobertaTokenizer, \n",
        "                          get_linear_schedule_with_warmup)"
      ],
      "metadata": {
        "id": "uNVVCtR56LKA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Enable and login to weights & biases with key\n",
        "\"\"\"\n",
        "\n",
        "#!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "98gPb9IQDl9X",
        "outputId": "6d4db47c-589c-4a99-ade8-e048c7cd0b2c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEnable and login to weights & biases with key\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initialize Logger\n",
        "\"\"\"\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logging.basicConfig(level=logging.DEBUG)"
      ],
      "metadata": {
        "id": "Urhj7ANl9Qh3"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Set directories and file names, seed and hyperparameters"
      ],
      "metadata": {
        "id": "m00AapAe6FX8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "tb-sf0q9yYkh"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Set directories and file names\n",
        "\"\"\"\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/NLP/data/'\n",
        "output_dir = '/content/drive/MyDrive/NLP/data/output/'\n",
        "\n",
        "train_file = 'train.jsonl'\n",
        "test_file = 'test.jsonl'\n",
        "eval_file = 'valid.jsonl'\n",
        "codebase_file = 'codebase.jsonl'\n",
        "model_path = 'python_model/'\n",
        "model_config_path = 'python_model/config.json'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Init deterministic seed\n",
        "\"\"\"\n",
        "\n",
        "seed_value = 1234\n",
        "np.random.seed(seed_value) # set numpy seed\n",
        "torch.manual_seed(seed_value) # set pytorch seed CPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6hSPfND6X37",
        "outputId": "c3ab8f97-c3fe-4632-f8b6-05d96570b407"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f17e5f266b0>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Check if cuda is available and enable cuda\n",
        "\"\"\"\n",
        "\n",
        "# set cpu or gpu enabled device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
        "\n",
        "# init deterministic GPU seed\n",
        "torch.cuda.manual_seed(seed_value)\n",
        "\n",
        "# log type of device enabled\n",
        "print('[LOG] notebook with {} computation enabled'.format(str(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxLpBUlD27TH",
        "outputId": "13f2770e-0602-47bd-cdf8-bf8c74fed93c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG] notebook with cuda computation enabled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Set configurations for training and weights & biases\n",
        "\"\"\"\n",
        "\n",
        "config={\n",
        "\"batch_size\": 32,\n",
        "\"learning_rate\": 2e-5,\n",
        "\"num_training_epochs\": 10,\n",
        "\"run_information\": \"not finetuned model with: microsoft/codebert-base as model\"\n",
        "}"
      ],
      "metadata": {
        "id": "jSxrgHtsEawG"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initialize weights and bias run with defined configurations\n",
        "\"\"\"\n",
        "\n",
        "#import wandb\n",
        "#run = wandb.init(project=\"nlp-codebert\", entity=\"nicikess\", config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pGrblKQOEWkR",
        "outputId": "b6b0b435-2273-4095-c8d6-c30f7da1cd0f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nInitialize weights and bias run with defined configurations\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Define classes and functions used for training and evaluation"
      ],
      "metadata": {
        "id": "9qWT1oeqLAm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define features for training and testing\n",
        "\"\"\"\n",
        "\n",
        "class InputFeatures(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 code_tokens,\n",
        "                 code_ids,\n",
        "                 nl_tokens,\n",
        "                 nl_ids,\n",
        "                 url,\n",
        "\n",
        "    ):\n",
        "        self.code_tokens = code_tokens\n",
        "        self.code_ids = code_ids\n",
        "        self.nl_tokens = nl_tokens\n",
        "        self.nl_ids = nl_ids\n",
        "        self.url=url"
      ],
      "metadata": {
        "id": "xODAy5Eajy40"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Convert json data to feature for training and testing\n",
        "\"\"\"\n",
        "\n",
        "def convert_examples_to_features(js,tokenizer):\n",
        "\n",
        "    #Set maximum characters for natural language and code. Length were definied based on this notebook:\n",
        "    #https://github.com/github/CodeSearchNet/blob/master/notebooks/ExploreData.ipynb\n",
        "\n",
        "    code_length = 256\n",
        "    nl_length = 128\n",
        "\n",
        "    code=' '.join(js['code_tokens'])\n",
        "    code_tokens=tokenizer.tokenize(code)[:code_length-2]\n",
        "    code_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "    code_ids =  tokenizer.convert_tokens_to_ids(code_tokens)\n",
        "    padding_length = code_length - len(code_ids)\n",
        "    code_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    \n",
        "    nl=' '.join(js['docstring_tokens'])\n",
        "    nl_tokens=tokenizer.tokenize(nl)[:nl_length-2]\n",
        "    nl_tokens =[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]\n",
        "    nl_ids =  tokenizer.convert_tokens_to_ids(nl_tokens)\n",
        "    padding_length = nl_length - len(nl_ids)\n",
        "    nl_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "\n",
        "    return InputFeatures(code_tokens,code_ids,nl_tokens,nl_ids,js['url'])"
      ],
      "metadata": {
        "id": "M84JjFIi8AZU"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If first_run = true the jsonl training data is loaded from Google drive\n",
        "If first_run = false the data is loaded as ndarray from Google drive to save time when training\n",
        "When the script is executed for the first time set first_run = true\n",
        "\"\"\"\n",
        "\n",
        "first_run = False"
      ],
      "metadata": {
        "id": "gR9g0wHoLfvI"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dataset used for the training and evaluation process\n",
        "\"\"\"\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path=None):\n",
        "        file_path = os.path.join(data_dir,file_path)\n",
        "        self.examples = []\n",
        "        self.data=[]\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                line=line.strip()\n",
        "                js=json.loads(line)\n",
        "                self.data.append(js)\n",
        "\n",
        "        if 'train' not in file_path or first_run == True:\n",
        "          for js in self.data:\n",
        "            self.examples.append(convert_examples_to_features(js,tokenizer))\n",
        "        else:\n",
        "          self.examples = np.load('/content/drive/MyDrive/NLP/data/examples.npy', allow_pickle=True)\n",
        "\n",
        "        #Print first three examples\n",
        "        if 'train' in file_path:\n",
        "            for idx, example in enumerate(self.examples[:3]):\n",
        "                logger.info(\"*** Example ***\")\n",
        "                logger.info(\"idx: {}\".format(idx))\n",
        "                logger.info(\"code_tokens: {}\".format([x.replace('\\u0120','_') for x in example.code_tokens]))\n",
        "                logger.info(\"code_ids: {}\".format(' '.join(map(str, example.code_ids))))\n",
        "                logger.info(\"nl_tokens: {}\".format([x.replace('\\u0120','_') for x in example.nl_tokens]))\n",
        "                logger.info(\"nl_ids: {}\".format(' '.join(map(str, example.nl_ids))))                             \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):   \n",
        "        return (torch.tensor(self.examples[i].code_ids),torch.tensor(self.examples[i].nl_ids))"
      ],
      "metadata": {
        "id": "d9cgqIe0HCD5"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Finetune pretrained CodeBERT model"
      ],
      "metadata": {
        "id": "QLKWl-9K69rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, tokenizer):\n",
        "\n",
        "  #Get the training dataset\n",
        "  train_dataset=TextDataset(tokenizer, train_file)\n",
        "\n",
        "  #Decrease training set for testing\n",
        "  indices = np.arange(start = 0, stop = len(train_dataset), step = 1000)\n",
        "  train_dataset = data_utils.Subset(train_dataset, indices)\n",
        "\n",
        "  train_sampler = RandomSampler(train_dataset)\n",
        "  train_dataloader = DataLoader(train_dataset, sampler = train_sampler, batch_size = config.get(\"batch_size\"))\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=config.get(\"learning_rate\"), eps=1e-8)\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,num_training_steps=len(train_dataloader)*config.get(\"num_training_epochs\"))\n",
        "\n",
        "  logger.info(\"Training start\")\n",
        "\n",
        "  model.zero_grad()\n",
        "  model.train()\n",
        "\n",
        "  tr_num,tr_loss,best_mrr=0,0,0\n",
        "\n",
        "  wandb.watch(model)\n",
        "\n",
        "  for idx in range(config.get(\"num_training_epochs\")):\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "      #Get inputs\n",
        "      code_inputs = batch[0].to(device)    \n",
        "      nl_inputs = batch[1].to(device)\n",
        "\n",
        "      #Get code and nl vectors\n",
        "      code_vec = model(code_inputs=code_inputs)\n",
        "      nl_vec = model(nl_inputs=nl_inputs)\n",
        "\n",
        "      #Calculate scores and loss\n",
        "      scores=torch.einsum(\"ab,cb->ac\",nl_vec,code_vec)\n",
        "      loss_fct = CrossEntropyLoss()\n",
        "      loss = loss_fct(scores, torch.arange(code_inputs.size(0), device=scores.device))\n",
        "      wandb.log({\"loss\": loss})\n",
        "\n",
        "      #Report loss\n",
        "      tr_loss += loss.item()\n",
        "      tr_num+=1\n",
        "      if (step+1)% 100==0:\n",
        "          logger.info(\"epoch {} step {} loss {}\".format(idx,step+1,round(tr_loss/tr_num,5)))\n",
        "          tr_loss=0\n",
        "          tr_num=0\n",
        "\n",
        "      #Backward\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      scheduler.step()"
      ],
      "metadata": {
        "id": "gKB-F_YS6-Gc"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Reset cuda GPU to save RAM\n",
        "\"\"\"\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tEt0M0DTUziV"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Evaluate finetuned model"
      ],
      "metadata": {
        "id": "VzsH88JkOfvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, tokenizer):\n",
        "    query_dataset = TextDataset(tokenizer, test_file)\n",
        "\n",
        "    query_sampler = SequentialSampler(query_dataset)\n",
        "    query_dataloader = DataLoader(query_dataset, sampler=query_sampler, batch_size=config.get(\"batch_size\"),num_workers=4)\n",
        "\n",
        "    logger.info(\"  Num queries = %d\", len(query_dataset))\n",
        "\n",
        "    code_dataset = TextDataset(tokenizer, codebase_file)\n",
        "    code_sampler = SequentialSampler(code_dataset)\n",
        "    code_dataloader = DataLoader(code_dataset, sampler=code_sampler, batch_size=config.get(\"batch_size\"),num_workers=4)    \n",
        "\n",
        "    logger.info(\"***** Running evaluation *****\")\n",
        "    logger.info(\"  Num queries = %d\", len(query_dataset))\n",
        "    logger.info(\"  Num codes = %d\", len(code_dataset))\n",
        "    logger.info(\"  Batch size = %d\", config.get(\"batch_size\"))\n",
        "\n",
        "    \n",
        "    model.eval() \n",
        "    code_vecs=[] \n",
        "    nl_vecs=[]\n",
        "\n",
        "    logger.info(\" Go trough query\")\n",
        "\n",
        "    for batch in tqdm(query_dataloader):  \n",
        "        nl_inputs = batch[1].to(device)\n",
        "        with torch.no_grad():\n",
        "            nl_vec = model(nl_inputs=nl_inputs) \n",
        "            nl_vecs.append(nl_vec.cpu().numpy()) \n",
        "\n",
        "    logger.info(\" Go trough code\")\n",
        "\n",
        "    for batch in tqdm(code_dataloader):\n",
        "        code_inputs = batch[0].to(device)    \n",
        "        with torch.no_grad():\n",
        "            code_vec= model(code_inputs=code_inputs)\n",
        "            code_vecs.append(code_vec.cpu().numpy())\n",
        "\n",
        "    model.train()    \n",
        "    code_vecs=np.concatenate(code_vecs,0)\n",
        "    nl_vecs=np.concatenate(nl_vecs,0)\n",
        "\n",
        "    scores=np.matmul(nl_vecs,code_vecs.T)\n",
        "    \n",
        "    sort_ids=np.argsort(scores, axis=-1, kind='quicksort', order=None)[:,::-1]    \n",
        "    \n",
        "    nl_urls=[]\n",
        "    code_urls=[]\n",
        "    for example in query_dataset.examples:\n",
        "        nl_urls.append(example.url)\n",
        "        \n",
        "    for example in code_dataset.examples:\n",
        "        code_urls.append(example.url)\n",
        "        \n",
        "    ranks=[]\n",
        "    for url, sort_id in zip(nl_urls,sort_ids):\n",
        "        rank=0\n",
        "        find=False\n",
        "        for idx in sort_id[:1000]:\n",
        "            if find is False:\n",
        "                rank+=1\n",
        "            if code_urls[idx]==url:\n",
        "                find=True\n",
        "        if find:\n",
        "            ranks.append(1/rank)\n",
        "        else:\n",
        "            ranks.append(0)\n",
        "    \n",
        "    result = {\n",
        "        \"eval_mrr\":float(np.mean(ranks))\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "FuwZ9TxWOjsz"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Define model"
      ],
      "metadata": {
        "id": "pScWst5bTKsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):   \n",
        "    def __init__(self, encoder):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "      \n",
        "    def forward(self, code_inputs=None, nl_inputs=None): \n",
        "        if code_inputs is not None:\n",
        "            return self.encoder(code_inputs,attention_mask=code_inputs.ne(1))[1]\n",
        "        else:\n",
        "            return self.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[1]"
      ],
      "metadata": {
        "id": "HmeArHj7IN8X"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load pretrained model\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "#Pretrained model\n",
        "#model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "#Finetuned model\n",
        "model = RobertaModel.from_pretrained(os.path.join(data_dir, model_path)) \n",
        "\n",
        "model = Model(model)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "8jWEESe-Lu-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Train and Evaluate"
      ],
      "metadata": {
        "id": "XPZ1XJa1Ury-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Uncomment to train\n",
        "\"\"\"\n",
        "\n",
        "#train(model, tokenizer)"
      ],
      "metadata": {
        "id": "dl43LuCKLNcl"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Uncomment to evaluate\n",
        "\"\"\"\n",
        "\n",
        "#result = evaluate(model, tokenizer)\n",
        "#for key in sorted(result.keys()):\n",
        "    #logger.info(\"  %s = %s\", key, str(round(result[key],4)))"
      ],
      "metadata": {
        "id": "5oV1hqUSwBN8"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Make Embeddings\n"
      ],
      "metadata": {
        "id": "4DzCvfFyEI_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create embeddings of python code from the data\n",
        "Maybe change the data to the train file to have more embeddings\n",
        "\"\"\"\n",
        "\n",
        "code_dataset = TextDataset(tokenizer, codebase_file)\n",
        "code_sampler = SequentialSampler(code_dataset)\n",
        "code_dataloader = DataLoader(code_dataset, sampler=code_sampler, batch_size=config.get(\"batch_size\"),num_workers=4)    \n",
        "\n",
        "code_vecs=[] \n",
        "\n",
        "for batch in tqdm(code_dataloader):\n",
        "    code_inputs = batch[0].to(device)    \n",
        "    with torch.no_grad():\n",
        "        code_vec= model(code_inputs=code_inputs)\n",
        "        code_vecs.append(code_vec.cpu().numpy())\n",
        "\n",
        "#Delete last embedding to have a valid shape\n",
        "code_vecs = np.delete(code_vecs, -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxFTQkI5Buuf",
        "outputId": "bc5ad4d5-37aa-4c89-9c55-168e4f4d3484"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1370/1370 [05:30<00:00,  4.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Stack embeddings together\n",
        "\"\"\"\n",
        "\n",
        "vecs = np.vstack(code_vecs)\n",
        "vecs = torch.from_numpy(vecs)\n",
        "np.shape(vecs)"
      ],
      "metadata": {
        "id": "QCVKD0DTiWkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Make a query\n"
      ],
      "metadata": {
        "id": "5H1ucoz6k35a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define a natural language query\n",
        "\"\"\"\n",
        "\n",
        "query = \"Establish ssh tunnel\"\n",
        "query_vec = model(tokenizer(query,return_tensors='pt')['input_ids'].to(device))"
      ],
      "metadata": {
        "id": "FLWJRvT5mnXP"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate score\n",
        "\"\"\"\n",
        "\n",
        "query_vec = query_vec.to(device)\n",
        "vecs = vecs.to(device)\n",
        "\n",
        "scores=torch.einsum(\"ab,cb->ac\",query_vec,vecs)\n",
        "scores=torch.softmax(scores,-1)"
      ],
      "metadata": {
        "id": "Tsk-wL64k2YU"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Find index with the highest score\n",
        "\"\"\"\n",
        "\n",
        "index = torch.argmax(scores)"
      ],
      "metadata": {
        "id": "YFpIRVYoq2OK"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Return code of the result with the highest score\n",
        "\"\"\"\n",
        "\n",
        "code_dataset.data[index]"
      ],
      "metadata": {
        "id": "PiIeFjZkrC6V",
        "outputId": "625c32b2-da3e-4d24-ea6d-cb5764f091fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'code': 'def establish_ssh_tunnel(self):\\n    \"\"\"\\n    Establish an ssh tunnel for each local host and port\\n    that can be used to communicate with the state host.\\n    \"\"\"\\n    localportlist = []\\n    for (host, port) in self.hostportlist:\\n      localport = self.pick_unused_port()\\n      self.tunnel.append(subprocess.Popen(\\n          (\\'ssh\\', self.tunnelhost, \\'-NL127.0.0.1:%d:%s:%d\\' % (localport, host, port))))\\n      localportlist.append((\\'127.0.0.1\\', localport))\\n    return localportlist',\n",
              " 'code_tokens': ['def',\n",
              "  'establish_ssh_tunnel',\n",
              "  '(',\n",
              "  'self',\n",
              "  ')',\n",
              "  ':',\n",
              "  'localportlist',\n",
              "  '=',\n",
              "  '[',\n",
              "  ']',\n",
              "  'for',\n",
              "  '(',\n",
              "  'host',\n",
              "  ',',\n",
              "  'port',\n",
              "  ')',\n",
              "  'in',\n",
              "  'self',\n",
              "  '.',\n",
              "  'hostportlist',\n",
              "  ':',\n",
              "  'localport',\n",
              "  '=',\n",
              "  'self',\n",
              "  '.',\n",
              "  'pick_unused_port',\n",
              "  '(',\n",
              "  ')',\n",
              "  'self',\n",
              "  '.',\n",
              "  'tunnel',\n",
              "  '.',\n",
              "  'append',\n",
              "  '(',\n",
              "  'subprocess',\n",
              "  '.',\n",
              "  'Popen',\n",
              "  '(',\n",
              "  '(',\n",
              "  \"'ssh'\",\n",
              "  ',',\n",
              "  'self',\n",
              "  '.',\n",
              "  'tunnelhost',\n",
              "  ',',\n",
              "  \"'-NL127.0.0.1:%d:%s:%d'\",\n",
              "  '%',\n",
              "  '(',\n",
              "  'localport',\n",
              "  ',',\n",
              "  'host',\n",
              "  ',',\n",
              "  'port',\n",
              "  ')',\n",
              "  ')',\n",
              "  ')',\n",
              "  ')',\n",
              "  'localportlist',\n",
              "  '.',\n",
              "  'append',\n",
              "  '(',\n",
              "  '(',\n",
              "  \"'127.0.0.1'\",\n",
              "  ',',\n",
              "  'localport',\n",
              "  ')',\n",
              "  ')',\n",
              "  'return',\n",
              "  'localportlist'],\n",
              " 'docstring': '',\n",
              " 'docstring_tokens': [],\n",
              " 'func_name': 'StateManager.establish_ssh_tunnel',\n",
              " 'language': 'python',\n",
              " 'original_string': 'def establish_ssh_tunnel(self):\\n    \"\"\"\\n    Establish an ssh tunnel for each local host and port\\n    that can be used to communicate with the state host.\\n    \"\"\"\\n    localportlist = []\\n    for (host, port) in self.hostportlist:\\n      localport = self.pick_unused_port()\\n      self.tunnel.append(subprocess.Popen(\\n          (\\'ssh\\', self.tunnelhost, \\'-NL127.0.0.1:%d:%s:%d\\' % (localport, host, port))))\\n      localportlist.append((\\'127.0.0.1\\', localport))\\n    return localportlist',\n",
              " 'partition': 'valid',\n",
              " 'path': 'heron/statemgrs/src/python/statemanager.py',\n",
              " 'repo': 'apache/incubator-heron',\n",
              " 'sha': 'ad10325a0febe89ad337e561ebcbe37ec5d9a5ac',\n",
              " 'url': 'https://github.com/apache/incubator-heron/blob/ad10325a0febe89ad337e561ebcbe37ec5d9a5ac/heron/statemgrs/src/python/statemanager.py#L110-L121'}"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Paraphrase result"
      ],
      "metadata": {
        "id": "TOHKyy-d1Id9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_dataset.data[index]"
      ],
      "metadata": {
        "id": "0pxaGqxi1NHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docstring = 'Establish an ssh tunnel for each local host and port that can be used to communicate with the state host.'"
      ],
      "metadata": {
        "id": "qqWOBOSl6xid"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")  \n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
        "\n",
        "sentence = \"This is something which i cannot understand at all\"\n",
        "\n",
        "text =  \"paraphrase: \" + sentence + \" </s>\"\n",
        "\n",
        "encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
        "input_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n",
        "\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=input_ids, attention_mask=attention_masks,\n",
        "    max_length=256,\n",
        "    do_sample=True,\n",
        "    top_k=120,\n",
        "    top_p=0.95,\n",
        "    early_stopping=True,\n",
        "    num_return_sequences=5\n",
        ")\n",
        "\n",
        "for output in outputs:\n",
        "    line = tokenizer.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    print(line)"
      ],
      "metadata": {
        "id": "luuUkkU470IS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}