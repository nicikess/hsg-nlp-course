{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "history_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/nicikess/hsg-nlp-course/blob/main/notebooks/CodeBERT.ipynb",
      "authorship_tag": "ABX9TyMIQIu1zloG/EtHkvI/pZr0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicikess/hsg-nlp-course/blob/main/notebooks/CodeBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference to Microsoft CodeBERT: A Pre-Trained Model for Programming and Natural Languages "
      ],
      "metadata": {
        "id": "YgrFUsZJAioe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/microsoft/CodeBERT/tree/master/CodeBERT/codesearch"
      ],
      "metadata": {
        "id": "Kw1sIpBUAlsh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing the required modules\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jamtkqkVJvHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install utils\n",
        "!pip install wandb\n",
        "!pip install sentencepiece\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import logging\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from torch.utils.data import (DataLoader,\n",
        "                              RandomSampler,\n",
        "                              TensorDataset,\n",
        "                              Dataset,\n",
        "                              SequentialSampler)\n",
        "\n",
        "from transformers import (AdamW,\n",
        "                          RobertaConfig,\n",
        "                          RobertaModel,\n",
        "                          RobertaForSequenceClassification,\n",
        "                          RobertaTokenizer, \n",
        "                          get_linear_schedule_with_warmup)"
      ],
      "metadata": {
        "id": "uNVVCtR56LKA",
        "outputId": "b9d54fd0-51e8-4b19-db1b-eee6676f5845",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 50.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 39.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.18-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 57.9 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 12.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=152d1727a952af1ee3335acbe1551495801712b9ca3696da2eca506243713ed7\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.18\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 8.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Enable and login to weights & biases with key\n",
        "\"\"\"\n",
        "\n",
        "#!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "98gPb9IQDl9X",
        "outputId": "b2921382-6720-4251-c4ef-ca9d11a39dbd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEnable and login to weights & biases with key\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initialize Logger\n",
        "\"\"\"\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logging.basicConfig(level=logging.DEBUG)"
      ],
      "metadata": {
        "id": "Urhj7ANl9Qh3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Set directories and file names, seed and hyperparameters"
      ],
      "metadata": {
        "id": "m00AapAe6FX8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tb-sf0q9yYkh"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Set directories and file names\n",
        "\"\"\"\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/NLP/data/'\n",
        "output_dir = '/content/drive/MyDrive/NLP/data/output/'\n",
        "\n",
        "train_file = 'train.jsonl'\n",
        "test_file = 'test.jsonl'\n",
        "eval_file = 'valid.jsonl'\n",
        "codesearchnet_file = 'codesearchnet.jsonl'\n",
        "codebase_file = 'codebase.jsonl'\n",
        "model_path = 'python_model/'\n",
        "model_config_path = 'python_model/config.json'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Init deterministic seed\n",
        "\"\"\"\n",
        "\n",
        "seed_value = 1234\n",
        "np.random.seed(seed_value) # set numpy seed\n",
        "torch.manual_seed(seed_value) # set pytorch seed CPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6hSPfND6X37",
        "outputId": "18a43f6c-aaa4-4d04-c74b-c981f86ad772"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fce535d3710>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Check if cuda is available and enable cuda\n",
        "\"\"\"\n",
        "\n",
        "# set cpu or gpu enabled device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
        "\n",
        "# init deterministic GPU seed\n",
        "torch.cuda.manual_seed(seed_value)\n",
        "\n",
        "# log type of device enabled\n",
        "print('[LOG] notebook with {} computation enabled'.format(str(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxLpBUlD27TH",
        "outputId": "ab5da37b-349a-4711-f44d-30cbf91f274d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG] notebook with cuda computation enabled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Set configurations for training and weights & biases\n",
        "\"\"\"\n",
        "\n",
        "config={\n",
        "\"batch_size\": 32,\n",
        "\"learning_rate\": 2e-5,\n",
        "\"num_training_epochs\": 10,\n",
        "\"run_information\": \"not finetuned model with: microsoft/codebert-base as model\"\n",
        "}"
      ],
      "metadata": {
        "id": "jSxrgHtsEawG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initialize weights and bias run with defined configurations\n",
        "\"\"\"\n",
        "\n",
        "#import wandb\n",
        "#run = wandb.init(project=\"nlp-codebert\", entity=\"nicikess\", config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pGrblKQOEWkR",
        "outputId": "8d0ff1eb-da04-4b78-c82a-558fba772516"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nInitialize weights and bias run with defined configurations\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Define classes and functions used for training and evaluation"
      ],
      "metadata": {
        "id": "9qWT1oeqLAm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define features for training and testing\n",
        "\"\"\"\n",
        "\n",
        "class InputFeatures(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 code_tokens,\n",
        "                 code_ids,\n",
        "                 nl_tokens,\n",
        "                 nl_ids,\n",
        "                 url,\n",
        "\n",
        "    ):\n",
        "        self.code_tokens = code_tokens\n",
        "        self.code_ids = code_ids\n",
        "        self.nl_tokens = nl_tokens\n",
        "        self.nl_ids = nl_ids\n",
        "        self.url=url"
      ],
      "metadata": {
        "id": "xODAy5Eajy40"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Convert json data to feature for training and testing\n",
        "\"\"\"\n",
        "\n",
        "def convert_examples_to_features(js,tokenizer):\n",
        "\n",
        "    #Set maximum characters for natural language and code. Length were definied based on this notebook:\n",
        "    #https://github.com/github/CodeSearchNet/blob/master/notebooks/ExploreData.ipynb\n",
        "\n",
        "    code_length = 256\n",
        "    nl_length = 128\n",
        "\n",
        "    code=' '.join(js['code_tokens'])\n",
        "    code_tokens=tokenizer.tokenize(code)[:code_length-2]\n",
        "    code_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "    code_ids =  tokenizer.convert_tokens_to_ids(code_tokens)\n",
        "    padding_length = code_length - len(code_ids)\n",
        "    code_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    \n",
        "    nl=' '.join(js['docstring_tokens'])\n",
        "    nl_tokens=tokenizer.tokenize(nl)[:nl_length-2]\n",
        "    nl_tokens =[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]\n",
        "    nl_ids =  tokenizer.convert_tokens_to_ids(nl_tokens)\n",
        "    padding_length = nl_length - len(nl_ids)\n",
        "    nl_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "\n",
        "    return InputFeatures(code_tokens,code_ids,nl_tokens,nl_ids,js['url'])"
      ],
      "metadata": {
        "id": "M84JjFIi8AZU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If first_run = true the jsonl training data is loaded from Google drive\n",
        "If first_run = false the data is loaded as ndarray from Google drive to save time when training\n",
        "When the script is executed for the first time set first_run = true\n",
        "\"\"\"\n",
        "\n",
        "first_run = False"
      ],
      "metadata": {
        "id": "gR9g0wHoLfvI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dataset used for the training and evaluation process\n",
        "\"\"\"\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path=None):\n",
        "        file_path = os.path.join(data_dir,file_path)\n",
        "        self.examples = []\n",
        "        self.data=[]\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                line=line.strip()\n",
        "                js=json.loads(line)\n",
        "                self.data.append(js)\n",
        "\n",
        "        if 'train' not in file_path or first_run == True:\n",
        "          for js in self.data:\n",
        "            self.examples.append(convert_examples_to_features(js,tokenizer))\n",
        "        else:\n",
        "          self.examples = np.load('/content/drive/MyDrive/NLP/data/examples.npy', allow_pickle=True)\n",
        "\n",
        "        #Print first three examples\n",
        "        if 'train' in file_path:\n",
        "            for idx, example in enumerate(self.examples[:3]):\n",
        "                logger.info(\"*** Example ***\")\n",
        "                logger.info(\"idx: {}\".format(idx))\n",
        "                logger.info(\"code_tokens: {}\".format([x.replace('\\u0120','_') for x in example.code_tokens]))\n",
        "                logger.info(\"code_ids: {}\".format(' '.join(map(str, example.code_ids))))\n",
        "                logger.info(\"nl_tokens: {}\".format([x.replace('\\u0120','_') for x in example.nl_tokens]))\n",
        "                logger.info(\"nl_ids: {}\".format(' '.join(map(str, example.nl_ids))))                             \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):   \n",
        "        return (torch.tensor(self.examples[i].code_ids),torch.tensor(self.examples[i].nl_ids))"
      ],
      "metadata": {
        "id": "d9cgqIe0HCD5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Finetune pretrained CodeBERT model"
      ],
      "metadata": {
        "id": "QLKWl-9K69rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, tokenizer):\n",
        "\n",
        "  #Get the training dataset\n",
        "  train_dataset=TextDataset(tokenizer, train_file)\n",
        "\n",
        "  #Decrease training set for testing\n",
        "  indices = np.arange(start = 0, stop = len(train_dataset), step = 1000)\n",
        "  train_dataset = data_utils.Subset(train_dataset, indices)\n",
        "\n",
        "  train_sampler = RandomSampler(train_dataset)\n",
        "  train_dataloader = DataLoader(train_dataset, sampler = train_sampler, batch_size = config.get(\"batch_size\"))\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=config.get(\"learning_rate\"), eps=1e-8)\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,num_training_steps=len(train_dataloader)*config.get(\"num_training_epochs\"))\n",
        "\n",
        "  logger.info(\"Training start\")\n",
        "\n",
        "  model.zero_grad()\n",
        "  model.train()\n",
        "\n",
        "  tr_num,tr_loss,best_mrr=0,0,0\n",
        "\n",
        "  wandb.watch(model)\n",
        "\n",
        "  for idx in range(config.get(\"num_training_epochs\")):\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "      #Get inputs\n",
        "      code_inputs = batch[0].to(device)    \n",
        "      nl_inputs = batch[1].to(device)\n",
        "\n",
        "      #Get code and nl vectors\n",
        "      code_vec = model(code_inputs=code_inputs)\n",
        "      nl_vec = model(nl_inputs=nl_inputs)\n",
        "\n",
        "      #Calculate scores and loss\n",
        "      scores=torch.einsum(\"ab,cb->ac\",nl_vec,code_vec)\n",
        "      loss_fct = CrossEntropyLoss()\n",
        "      loss = loss_fct(scores, torch.arange(code_inputs.size(0), device=scores.device))\n",
        "      wandb.log({\"loss\": loss})\n",
        "\n",
        "      #Report loss\n",
        "      tr_loss += loss.item()\n",
        "      tr_num+=1\n",
        "      if (step+1)% 100==0:\n",
        "          logger.info(\"epoch {} step {} loss {}\".format(idx,step+1,round(tr_loss/tr_num,5)))\n",
        "          tr_loss=0\n",
        "          tr_num=0\n",
        "\n",
        "      #Backward\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      scheduler.step()"
      ],
      "metadata": {
        "id": "gKB-F_YS6-Gc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Reset cuda GPU to save RAM\n",
        "\"\"\"\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tEt0M0DTUziV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Evaluate finetuned model"
      ],
      "metadata": {
        "id": "VzsH88JkOfvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, tokenizer):\n",
        "    query_dataset = TextDataset(tokenizer, test_file)\n",
        "\n",
        "    query_sampler = SequentialSampler(query_dataset)\n",
        "    query_dataloader = DataLoader(query_dataset, sampler=query_sampler, batch_size=config.get(\"batch_size\"),num_workers=4)\n",
        "\n",
        "    logger.info(\"  Num queries = %d\", len(query_dataset))\n",
        "\n",
        "    code_dataset = TextDataset(tokenizer, codebase_file)\n",
        "    code_sampler = SequentialSampler(code_dataset)\n",
        "    code_dataloader = DataLoader(code_dataset, sampler=code_sampler, batch_size=config.get(\"batch_size\"),num_workers=4)    \n",
        "\n",
        "    logger.info(\"***** Running evaluation *****\")\n",
        "    logger.info(\"  Num queries = %d\", len(query_dataset))\n",
        "    logger.info(\"  Num codes = %d\", len(code_dataset))\n",
        "    logger.info(\"  Batch size = %d\", config.get(\"batch_size\"))\n",
        "\n",
        "    \n",
        "    model.eval() \n",
        "    code_vecs=[] \n",
        "    nl_vecs=[]\n",
        "\n",
        "    logger.info(\" Go trough query\")\n",
        "\n",
        "    for batch in tqdm(query_dataloader):  \n",
        "        nl_inputs = batch[1].to(device)\n",
        "        with torch.no_grad():\n",
        "            nl_vec = model(nl_inputs=nl_inputs) \n",
        "            nl_vecs.append(nl_vec.cpu().numpy()) \n",
        "\n",
        "    logger.info(\" Go trough code\")\n",
        "\n",
        "    for batch in tqdm(code_dataloader):\n",
        "        code_inputs = batch[0].to(device)    \n",
        "        with torch.no_grad():\n",
        "            code_vec= model(code_inputs=code_inputs)\n",
        "            code_vecs.append(code_vec.cpu().numpy())\n",
        "\n",
        "    model.train()    \n",
        "    code_vecs=np.concatenate(code_vecs,0)\n",
        "    nl_vecs=np.concatenate(nl_vecs,0)\n",
        "\n",
        "    scores=np.matmul(nl_vecs,code_vecs.T)\n",
        "    \n",
        "    sort_ids=np.argsort(scores, axis=-1, kind='quicksort', order=None)[:,::-1]    \n",
        "    \n",
        "    nl_urls=[]\n",
        "    code_urls=[]\n",
        "    for example in query_dataset.examples:\n",
        "        nl_urls.append(example.url)\n",
        "        \n",
        "    for example in code_dataset.examples:\n",
        "        code_urls.append(example.url)\n",
        "        \n",
        "    ranks=[]\n",
        "    for url, sort_id in zip(nl_urls,sort_ids):\n",
        "        rank=0\n",
        "        find=False\n",
        "        for idx in sort_id[:1000]:\n",
        "            if find is False:\n",
        "                rank+=1\n",
        "            if code_urls[idx]==url:\n",
        "                find=True\n",
        "        if find:\n",
        "            ranks.append(1/rank)\n",
        "        else:\n",
        "            ranks.append(0)\n",
        "    \n",
        "    result = {\n",
        "        \"eval_mrr\":float(np.mean(ranks))\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "FuwZ9TxWOjsz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Define model"
      ],
      "metadata": {
        "id": "pScWst5bTKsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):   \n",
        "    def __init__(self, encoder):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "      \n",
        "    def forward(self, code_inputs=None, nl_inputs=None): \n",
        "        if code_inputs is not None:\n",
        "            return self.encoder(code_inputs,attention_mask=code_inputs.ne(1))[1]\n",
        "        else:\n",
        "            return self.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[1]"
      ],
      "metadata": {
        "id": "HmeArHj7IN8X"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load pretrained model\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "#Pretrained model\n",
        "#model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "#Finetuned model\n",
        "model = RobertaModel.from_pretrained(os.path.join(data_dir, model_path)) \n",
        "\n",
        "model = Model(model)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "8jWEESe-Lu-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Train and Evaluate"
      ],
      "metadata": {
        "id": "XPZ1XJa1Ury-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Uncomment to train\n",
        "\"\"\"\n",
        "\n",
        "#train(model, tokenizer)"
      ],
      "metadata": {
        "id": "dl43LuCKLNcl",
        "outputId": "eff8b9e5-2c5e-4b46-e9b2-032c5f17979f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nUncomment to train\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Uncomment to evaluate\n",
        "\"\"\"\n",
        "\n",
        "#result = evaluate(model, tokenizer)\n",
        "#for key in sorted(result.keys()):\n",
        "    #logger.info(\"  %s = %s\", key, str(round(result[key],4)))"
      ],
      "metadata": {
        "id": "5oV1hqUSwBN8",
        "outputId": "3a5971a0-8e88-4d0d-b4fa-2921796bda3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nUncomment to evaluate\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Make Embeddings\n"
      ],
      "metadata": {
        "id": "4DzCvfFyEI_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create embeddings of python code from the data\n",
        "Maybe change the data to the train file to have more embeddings\n",
        "\"\"\"\n",
        "\n",
        "query_dataset = TextDataset(tokenizer, codebase_file)\n",
        "query_sampler = SequentialSampler(query_dataset)\n",
        "query_dataloader = DataLoader(query_dataset, sampler=query_sampler, batch_size=config.get(\"batch_size\"),num_workers=4)    \n",
        "\n",
        "code_vecs=[] \n",
        "\n",
        "for batch in tqdm(query_dataloader):\n",
        "    code_inputs = batch[0].to(device)    \n",
        "    with torch.no_grad():\n",
        "        code_vec= model(code_inputs=code_inputs)\n",
        "        code_vecs.append(code_vec.cpu().numpy())\n",
        "\n",
        "#Delete last embedding to have a valid shape\n",
        "code_vecs = np.delete(code_vecs, -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxFTQkI5Buuf",
        "outputId": "c79fc39e-abdb-4fac-a625-ad9322856beb"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1370/1370 [05:44<00:00,  3.98it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asarray(arr)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Stack embeddings together\n",
        "\"\"\"\n",
        "\n",
        "vecs = np.vstack(code_vecs)\n",
        "vecs = torch.from_numpy(vecs)\n",
        "np.shape(vecs)"
      ],
      "metadata": {
        "id": "QCVKD0DTiWkC",
        "outputId": "fc0560c7-6279-495b-becc-1bb336163b69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([43808, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Make a query\n"
      ],
      "metadata": {
        "id": "5H1ucoz6k35a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define a natural language query\n",
        "\"\"\"\n",
        "\n",
        "query = \"Create stream for write data to `destination\"\n",
        "query_vec = model(tokenizer(query,return_tensors='pt')['input_ids'].to(device))"
      ],
      "metadata": {
        "id": "FLWJRvT5mnXP"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(query_vec)"
      ],
      "metadata": {
        "id": "k7UR3pPAPI0N",
        "outputId": "add9d653-7720-43f7-c5a2-297100f6e006",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate score\n",
        "\"\"\"\n",
        "\n",
        "query_vec = query_vec.to(device)\n",
        "vecs = vecs.to(device)\n",
        "\n",
        "scores=torch.einsum(\"ab,cb->ac\",query_vec,vecs)\n",
        "scores=torch.softmax(scores,-1)"
      ],
      "metadata": {
        "id": "Tsk-wL64k2YU"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Convert scores\n",
        "\"\"\"\n",
        "\n",
        "scores = np.squeeze(scores)\n",
        "np.shape(scores)\n",
        "scores = scores.cpu().detach().numpy()\n",
        "np.shape(scores)"
      ],
      "metadata": {
        "id": "C6qLmYszZEMA",
        "outputId": "80ea9b57-c574-42e4-cc6b-8fc9985d314a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([43808])"
            ]
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Save top 5 scores\n",
        "\"\"\"\n",
        "\n",
        "scores = scores.argsort()[-5:][::-1]"
      ],
      "metadata": {
        "id": "rfYP3XK7XrLV"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "yx7rA8_VkUH2",
        "outputId": "7e5ebbb2-b3d4-4d4c-c505-2662573bedce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([19790, 28731, 33522, 20832, 39610])"
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Find index with the highest score\n",
        "\"\"\"\n",
        "\n",
        "#index = torch.argmax(scores)"
      ],
      "metadata": {
        "id": "YFpIRVYoq2OK",
        "outputId": "d4c0c5fc-2d84-43c7-8c58-ab602070442a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFind index with the highest score\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Return code of the result with the highest score\n",
        "\"\"\"\n",
        "\n",
        "query_dataset.data[scores[2]]"
      ],
      "metadata": {
        "id": "PiIeFjZkrC6V",
        "outputId": "432a50f5-9d4b-4949-befd-705229e66603",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'code': 'def upload_stream(self, destination, *, offset=0):\\n        \"\"\"\\n        Create stream for write data to `destination` file.\\n\\n        :param destination: destination path of file on server side\\n        :type destination: :py:class:`str` or :py:class:`pathlib.PurePosixPath`\\n\\n        :param offset: byte offset for stream start position\\n        :type offset: :py:class:`int`\\n\\n        :rtype: :py:class:`aioftp.DataConnectionThrottleStreamIO`\\n        \"\"\"\\n        return self.get_stream(\\n            \"STOR \" + str(destination),\\n            \"1xx\",\\n            offset=offset,\\n        )',\n",
              " 'code_tokens': ['def',\n",
              "  'upload_stream',\n",
              "  '(',\n",
              "  'self',\n",
              "  ',',\n",
              "  'destination',\n",
              "  ',',\n",
              "  '*',\n",
              "  ',',\n",
              "  'offset',\n",
              "  '=',\n",
              "  '0',\n",
              "  ')',\n",
              "  ':',\n",
              "  'return',\n",
              "  'self',\n",
              "  '.',\n",
              "  'get_stream',\n",
              "  '(',\n",
              "  '\"STOR \"',\n",
              "  '+',\n",
              "  'str',\n",
              "  '(',\n",
              "  'destination',\n",
              "  ')',\n",
              "  ',',\n",
              "  '\"1xx\"',\n",
              "  ',',\n",
              "  'offset',\n",
              "  '=',\n",
              "  'offset',\n",
              "  ',',\n",
              "  ')'],\n",
              " 'docstring': '',\n",
              " 'docstring_tokens': [],\n",
              " 'func_name': 'Client.upload_stream',\n",
              " 'language': 'python',\n",
              " 'original_string': 'def upload_stream(self, destination, *, offset=0):\\n        \"\"\"\\n        Create stream for write data to `destination` file.\\n\\n        :param destination: destination path of file on server side\\n        :type destination: :py:class:`str` or :py:class:`pathlib.PurePosixPath`\\n\\n        :param offset: byte offset for stream start position\\n        :type offset: :py:class:`int`\\n\\n        :rtype: :py:class:`aioftp.DataConnectionThrottleStreamIO`\\n        \"\"\"\\n        return self.get_stream(\\n            \"STOR \" + str(destination),\\n            \"1xx\",\\n            offset=offset,\\n        )',\n",
              " 'partition': 'valid',\n",
              " 'path': 'aioftp/client.py',\n",
              " 'repo': 'aio-libs/aioftp',\n",
              " 'sha': 'b45395b1aba41301b898040acade7010e6878a08',\n",
              " 'url': 'https://github.com/aio-libs/aioftp/blob/b45395b1aba41301b898040acade7010e6878a08/aioftp/client.py#L877-L893'}"
            ]
          },
          "metadata": {},
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Paraphrase result"
      ],
      "metadata": {
        "id": "TOHKyy-d1Id9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_dataset.data[index]"
      ],
      "metadata": {
        "id": "0pxaGqxi1NHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docstring = 'Establish an ssh tunnel for each local host and port that can be used to communicate with the state host.'"
      ],
      "metadata": {
        "id": "qqWOBOSl6xid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")  \n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
        "\n",
        "sentence = \"This is something which i cannot understand at all\"\n",
        "\n",
        "text =  \"paraphrase: \" + sentence + \" </s>\"\n",
        "\n",
        "encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
        "input_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n",
        "\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=input_ids, attention_mask=attention_masks,\n",
        "    max_length=256,\n",
        "    do_sample=True,\n",
        "    top_k=120,\n",
        "    top_p=0.95,\n",
        "    early_stopping=True,\n",
        "    num_return_sequences=5\n",
        ")\n",
        "\n",
        "for output in outputs:\n",
        "    line = tokenizer.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    print(line)"
      ],
      "metadata": {
        "id": "luuUkkU470IS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}