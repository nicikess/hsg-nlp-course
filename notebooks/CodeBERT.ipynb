{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "history_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/nicikess/hsg-nlp-course/blob/main/notebooks/CodeBERT.ipynb",
      "authorship_tag": "ABX9TyO1nTy3zleD0WRM0+5kWK8T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicikess/hsg-nlp-course/blob/main/notebooks/CodeBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference to Microsoft CodeBERT: A Pre-Trained Model for Programming and Natural Languages "
      ],
      "metadata": {
        "id": "YgrFUsZJAioe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/microsoft/CodeBERT/tree/master/CodeBERT/codesearch"
      ],
      "metadata": {
        "id": "Kw1sIpBUAlsh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing the required modules\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jamtkqkVJvHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install utils\n",
        "!pip install wandb\n",
        "!pip install sentencepiece\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import logging\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data_utils\n",
        "import ast\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from torch.utils.data import (DataLoader,\n",
        "                              RandomSampler,\n",
        "                              TensorDataset,\n",
        "                              Dataset,\n",
        "                              SequentialSampler)\n",
        "\n",
        "from transformers import (AdamW,\n",
        "                          RobertaConfig,\n",
        "                          RobertaModel,\n",
        "                          RobertaForSequenceClassification,\n",
        "                          RobertaTokenizer, \n",
        "                          get_linear_schedule_with_warmup,\n",
        "                          AutoTokenizer,\n",
        "                          AutoModelForSeq2SeqLM)"
      ],
      "metadata": {
        "id": "uNVVCtR56LKA",
        "outputId": "6907ae16-c78f-4c7c-ad34-78efed7e2967",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.18-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.1 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 85.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 71.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=78150bd0d2bc281c6ba78a692cf801a13cae55f1ae4c467fb827809857227e58\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.18\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Enable and login to weights & biases with key\n",
        "\"\"\"\n",
        "\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98gPb9IQDl9X",
        "outputId": "3d24117f-1c9a-4b1f-f0be-96a70399842f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initialize Logger\n",
        "\"\"\"\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logging.basicConfig(level=logging.DEBUG)"
      ],
      "metadata": {
        "id": "Urhj7ANl9Qh3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Set directories and file names, seed and hyperparameters"
      ],
      "metadata": {
        "id": "m00AapAe6FX8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tb-sf0q9yYkh"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Set directories and file names\n",
        "\"\"\"\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/NLP/data/'\n",
        "output_dir = '/content/drive/MyDrive/NLP/data/output/'\n",
        "\n",
        "train_file = 'train.jsonl'\n",
        "test_file = 'test.jsonl'\n",
        "eval_file = 'valid.jsonl'\n",
        "codesearchnet_file = 'codesearchnet.jsonl'\n",
        "codebase_file = 'codebase.jsonl'\n",
        "model_path = 'python_model/'\n",
        "model_config_path = 'python_model/config.json'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Init deterministic seed\n",
        "\"\"\"\n",
        "\n",
        "seed_value = 1234\n",
        "np.random.seed(seed_value) # set numpy seed\n",
        "torch.manual_seed(seed_value) # set pytorch seed CPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6hSPfND6X37",
        "outputId": "fe73f187-2d95-4f36-e9b4-be10d4f9b937"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f033d3eb870>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Check if cuda is available and enable cuda\n",
        "\"\"\"\n",
        "\n",
        "# set cpu or gpu enabled device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
        "\n",
        "# init deterministic GPU seed\n",
        "torch.cuda.manual_seed(seed_value)\n",
        "\n",
        "# log type of device enabled\n",
        "print('[LOG] notebook with {} computation enabled'.format(str(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxLpBUlD27TH",
        "outputId": "3580b93e-102b-4860-ac36-bbbe426081bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG] notebook with cuda computation enabled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Set configurations for training and weights & biases\n",
        "\"\"\"\n",
        "\n",
        "config={\n",
        "\"batch_size\": 32,\n",
        "\"learning_rate\": 2e-5,\n",
        "\"num_training_epochs\": 10,\n",
        "\"run_information\": \"not finetuned model with: microsoft/codebert-base as model\"\n",
        "}"
      ],
      "metadata": {
        "id": "jSxrgHtsEawG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initialize weights and bias run with defined configurations\n",
        "\"\"\"\n",
        "\n",
        "#import wandb\n",
        "#run = wandb.init(project=\"nlp-codebert\", entity=\"nicikess\", config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pGrblKQOEWkR",
        "outputId": "e746ec3c-fc6c-4c2a-be84-416fd09b9e45"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nInitialize weights and bias run with defined configurations\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Define classes and functions used for training and evaluation"
      ],
      "metadata": {
        "id": "9qWT1oeqLAm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define features for training and testing\n",
        "\"\"\"\n",
        "\n",
        "class InputFeatures(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 code_tokens,\n",
        "                 code_ids,\n",
        "                 nl_tokens,\n",
        "                 nl_ids,\n",
        "                 url,\n",
        "\n",
        "    ):\n",
        "        self.code_tokens = code_tokens\n",
        "        self.code_ids = code_ids\n",
        "        self.nl_tokens = nl_tokens\n",
        "        self.nl_ids = nl_ids\n",
        "        self.url=url"
      ],
      "metadata": {
        "id": "xODAy5Eajy40"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Convert json data to feature for training and testing\n",
        "\"\"\"\n",
        "\n",
        "def convert_examples_to_features(js,tokenizer):\n",
        "\n",
        "    #Set maximum characters for natural language and code. Length were definied based on this notebook:\n",
        "    #https://github.com/github/CodeSearchNet/blob/master/notebooks/ExploreData.ipynb\n",
        "\n",
        "    code_length = 256\n",
        "    nl_length = 128\n",
        "\n",
        "    code=' '.join(js['code_tokens'])\n",
        "    code_tokens=tokenizer.tokenize(code)[:code_length-2]\n",
        "    code_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "    code_ids =  tokenizer.convert_tokens_to_ids(code_tokens)\n",
        "    padding_length = code_length - len(code_ids)\n",
        "    code_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    \n",
        "    nl=' '.join(js['docstring_tokens'])\n",
        "    nl_tokens=tokenizer.tokenize(nl)[:nl_length-2]\n",
        "    nl_tokens =[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]\n",
        "    nl_ids =  tokenizer.convert_tokens_to_ids(nl_tokens)\n",
        "    padding_length = nl_length - len(nl_ids)\n",
        "    nl_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "\n",
        "    return InputFeatures(code_tokens,code_ids,nl_tokens,nl_ids,js['url'])"
      ],
      "metadata": {
        "id": "M84JjFIi8AZU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If first_run = true the jsonl training data is loaded from Google drive\n",
        "If first_run = false the data is loaded as ndarray from Google drive to save time when training\n",
        "When the script is executed for the first time set first_run = true\n",
        "\"\"\"\n",
        "\n",
        "first_run = False"
      ],
      "metadata": {
        "id": "gR9g0wHoLfvI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dataset used for the training and evaluation process\n",
        "\"\"\"\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path=None):\n",
        "        file_path = os.path.join(data_dir,file_path)\n",
        "        self.examples = []\n",
        "        self.data=[]\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                line=line.strip()\n",
        "                js=json.loads(line)\n",
        "                self.data.append(js)\n",
        "\n",
        "        if 'train' not in file_path or first_run == True:\n",
        "          for js in self.data:\n",
        "            self.examples.append(convert_examples_to_features(js,tokenizer))\n",
        "        else:\n",
        "          self.examples = np.load('/content/drive/MyDrive/NLP/data/examples.npy', allow_pickle=True)\n",
        "\n",
        "        #Print first three examples\n",
        "        if 'train' in file_path:\n",
        "            for idx, example in enumerate(self.examples[:3]):\n",
        "                logger.info(\"*** Example ***\")\n",
        "                logger.info(\"idx: {}\".format(idx))\n",
        "                logger.info(\"code_tokens: {}\".format([x.replace('\\u0120','_') for x in example.code_tokens]))\n",
        "                logger.info(\"code_ids: {}\".format(' '.join(map(str, example.code_ids))))\n",
        "                logger.info(\"nl_tokens: {}\".format([x.replace('\\u0120','_') for x in example.nl_tokens]))\n",
        "                logger.info(\"nl_ids: {}\".format(' '.join(map(str, example.nl_ids))))                             \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):   \n",
        "        return (torch.tensor(self.examples[i].code_ids),torch.tensor(self.examples[i].nl_ids))"
      ],
      "metadata": {
        "id": "d9cgqIe0HCD5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Finetune pretrained CodeBERT model"
      ],
      "metadata": {
        "id": "QLKWl-9K69rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, tokenizer):\n",
        "\n",
        "  #Get the training dataset\n",
        "  train_dataset=TextDataset(tokenizer, train_file)\n",
        "\n",
        "  #Decrease training set for testing\n",
        "  indices = np.arange(start = 0, stop = len(train_dataset), step = 1000)\n",
        "  train_dataset = data_utils.Subset(train_dataset, indices)\n",
        "\n",
        "  train_sampler = RandomSampler(train_dataset)\n",
        "  train_dataloader = DataLoader(train_dataset, sampler = train_sampler, batch_size = config.get(\"batch_size\"))\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=config.get(\"learning_rate\"), eps=1e-8)\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,num_training_steps=len(train_dataloader)*config.get(\"num_training_epochs\"))\n",
        "\n",
        "  logger.info(\"Training start\")\n",
        "\n",
        "  model.zero_grad()\n",
        "  model.train()\n",
        "\n",
        "  tr_num,tr_loss,best_mrr=0,0,0\n",
        "\n",
        "  wandb.watch(model)\n",
        "\n",
        "  for idx in range(config.get(\"num_training_epochs\")):\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "      #Get inputs\n",
        "      code_inputs = batch[0].to(device)    \n",
        "      nl_inputs = batch[1].to(device)\n",
        "\n",
        "      #Get code and nl vectors\n",
        "      code_vec = model(code_inputs=code_inputs)\n",
        "      nl_vec = model(nl_inputs=nl_inputs)\n",
        "\n",
        "      #Calculate scores and loss\n",
        "      scores=torch.einsum(\"ab,cb->ac\",nl_vec,code_vec)\n",
        "      loss_fct = CrossEntropyLoss()\n",
        "      loss = loss_fct(scores, torch.arange(code_inputs.size(0), device=scores.device))\n",
        "      wandb.log({\"loss\": loss})\n",
        "\n",
        "      #Report loss\n",
        "      tr_loss += loss.item()\n",
        "      tr_num+=1\n",
        "      if (step+1)% 100==0:\n",
        "          logger.info(\"epoch {} step {} loss {}\".format(idx,step+1,round(tr_loss/tr_num,5)))\n",
        "          tr_loss=0\n",
        "          tr_num=0\n",
        "\n",
        "      #Backward\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      scheduler.step()"
      ],
      "metadata": {
        "id": "gKB-F_YS6-Gc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Reset cuda GPU to save RAM\n",
        "\"\"\"\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tEt0M0DTUziV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Evaluate finetuned model"
      ],
      "metadata": {
        "id": "VzsH88JkOfvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, tokenizer):\n",
        "    query_dataset = TextDataset(tokenizer, test_file)\n",
        "\n",
        "    query_sampler = SequentialSampler(query_dataset)\n",
        "    query_dataloader = DataLoader(query_dataset, sampler=query_sampler, batch_size=config.get(\"batch_size\"),num_workers=4)\n",
        "\n",
        "    logger.info(\"  Num queries = %d\", len(query_dataset))\n",
        "\n",
        "    code_dataset = TextDataset(tokenizer, codebase_file)\n",
        "    code_sampler = SequentialSampler(code_dataset)\n",
        "    code_dataloader = DataLoader(code_dataset, sampler=code_sampler, batch_size=config.get(\"batch_size\"),num_workers=4)    \n",
        "\n",
        "    logger.info(\"***** Running evaluation *****\")\n",
        "    logger.info(\"  Num queries = %d\", len(query_dataset))\n",
        "    logger.info(\"  Num codes = %d\", len(code_dataset))\n",
        "    logger.info(\"  Batch size = %d\", config.get(\"batch_size\"))\n",
        "\n",
        "    \n",
        "    model.eval() \n",
        "    code_vecs=[] \n",
        "    nl_vecs=[]\n",
        "\n",
        "    logger.info(\" Go trough query\")\n",
        "\n",
        "    for batch in tqdm(query_dataloader):  \n",
        "        nl_inputs = batch[1].to(device)\n",
        "        with torch.no_grad():\n",
        "            nl_vec = model(nl_inputs=nl_inputs) \n",
        "            nl_vecs.append(nl_vec.cpu().numpy()) \n",
        "\n",
        "    logger.info(\" Go trough code\")\n",
        "\n",
        "    for batch in tqdm(code_dataloader):\n",
        "        code_inputs = batch[0].to(device)    \n",
        "        with torch.no_grad():\n",
        "            code_vec= model(code_inputs=code_inputs)\n",
        "            code_vecs.append(code_vec.cpu().numpy())\n",
        "\n",
        "    model.train()    \n",
        "    code_vecs=np.concatenate(code_vecs,0)\n",
        "    nl_vecs=np.concatenate(nl_vecs,0)\n",
        "\n",
        "    scores=np.matmul(nl_vecs,code_vecs.T)\n",
        "    \n",
        "    sort_ids=np.argsort(scores, axis=-1, kind='quicksort', order=None)[:,::-1]    \n",
        "    \n",
        "    nl_urls=[]\n",
        "    code_urls=[]\n",
        "    for example in query_dataset.examples:\n",
        "        nl_urls.append(example.url)\n",
        "        \n",
        "    for example in code_dataset.examples:\n",
        "        code_urls.append(example.url)\n",
        "        \n",
        "    ranks=[]\n",
        "    for url, sort_id in zip(nl_urls,sort_ids):\n",
        "        rank=0\n",
        "        find=False\n",
        "        for idx in sort_id[:1000]:\n",
        "            if find is False:\n",
        "                rank+=1\n",
        "            if code_urls[idx]==url:\n",
        "                find=True\n",
        "        if find:\n",
        "            ranks.append(1/rank)\n",
        "        else:\n",
        "            ranks.append(0)\n",
        "    \n",
        "    result = {\n",
        "        \"eval_mrr\":float(np.mean(ranks))\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "FuwZ9TxWOjsz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Define model"
      ],
      "metadata": {
        "id": "pScWst5bTKsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):   \n",
        "    def __init__(self, encoder):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "      \n",
        "    def forward(self, code_inputs=None, nl_inputs=None): \n",
        "        if code_inputs is not None:\n",
        "            return self.encoder(code_inputs,attention_mask=code_inputs.ne(1))[1]\n",
        "        else:\n",
        "            return self.encoder(nl_inputs,attention_mask=nl_inputs.ne(1))[1]"
      ],
      "metadata": {
        "id": "HmeArHj7IN8X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load pretrained model\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "#Pretrained model\n",
        "#model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "#Finetuned model\n",
        "model = RobertaModel.from_pretrained(os.path.join(data_dir, model_path)) \n",
        "\n",
        "model = Model(model)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "8jWEESe-Lu-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Train and Evaluate"
      ],
      "metadata": {
        "id": "XPZ1XJa1Ury-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Uncomment to train\n",
        "\"\"\"\n",
        "\n",
        "#train(model, tokenizer)"
      ],
      "metadata": {
        "id": "dl43LuCKLNcl",
        "outputId": "4b91fb98-87e7-4f0a-e409-90e5c3ec0f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nUncomment to train\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Uncomment to evaluate\n",
        "\"\"\"\n",
        "\n",
        "#result = evaluate(model, tokenizer)\n",
        "#for key in sorted(result.keys()):\n",
        "    #logger.info(\"  %s = %s\", key, str(round(result[key],4)))"
      ],
      "metadata": {
        "id": "5oV1hqUSwBN8",
        "outputId": "38ea3e01-3765-4411-ee12-3a51b4e5ac9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nUncomment to evaluate\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Make Embeddings\n"
      ],
      "metadata": {
        "id": "4DzCvfFyEI_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create embeddings of python code from the data\n",
        "Maybe change the data to the train file to have more embeddings\n",
        "\"\"\"\n",
        "\n",
        "query_dataset = TextDataset(tokenizer, codebase_file)\n",
        "query_sampler = SequentialSampler(query_dataset)\n",
        "query_dataloader = DataLoader(query_dataset, sampler=query_sampler, batch_size=config.get(\"batch_size\"),num_workers=4)    \n",
        "\n",
        "code_vecs=[] \n",
        "\n",
        "for batch in tqdm(query_dataloader):\n",
        "    code_inputs = batch[0].to(device)    \n",
        "    with torch.no_grad():\n",
        "        code_vec= model(code_inputs=code_inputs)\n",
        "        code_vecs.append(code_vec.cpu().numpy())\n",
        "\n",
        "#Delete last embedding to have a valid shape\n",
        "code_vecs = np.delete(code_vecs, -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxFTQkI5Buuf",
        "outputId": "df2939f6-db79-4c03-b41d-76c3d2f2fe95"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1370/1370 [11:06<00:00,  2.06it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asarray(arr)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Stack embeddings together\n",
        "\"\"\"\n",
        "\n",
        "vecs = np.vstack(code_vecs)\n",
        "vecs = torch.from_numpy(vecs)\n",
        "np.shape(vecs)"
      ],
      "metadata": {
        "id": "QCVKD0DTiWkC",
        "outputId": "15d01a3c-ac8b-4509-ea5b-e0ba5ceb9721",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([43808, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Make a query\n"
      ],
      "metadata": {
        "id": "5H1ucoz6k35a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define a natural language query\n",
        "\"\"\"\n",
        "\n",
        "query = \"Establish ssh tunnel\"\n",
        "query_vec = model(tokenizer(query,return_tensors='pt')['input_ids'].to(device))"
      ],
      "metadata": {
        "id": "FLWJRvT5mnXP"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(query_vec)"
      ],
      "metadata": {
        "id": "k7UR3pPAPI0N",
        "outputId": "4f86da6d-97a3-4492-ec66-dd168eb8fcba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate score\n",
        "\"\"\"\n",
        "\n",
        "query_vec = query_vec.to(device)\n",
        "vecs = vecs.to(device)\n",
        "\n",
        "scores=torch.einsum(\"ab,cb->ac\",query_vec,vecs)\n",
        "scores=torch.softmax(scores,-1)"
      ],
      "metadata": {
        "id": "Tsk-wL64k2YU"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Convert scores\n",
        "\"\"\"\n",
        "\n",
        "scores = np.squeeze(scores)\n",
        "np.shape(scores)\n",
        "scores = scores.cpu().detach().numpy()\n",
        "np.shape(scores)"
      ],
      "metadata": {
        "id": "C6qLmYszZEMA",
        "outputId": "97d8eb5c-9f94-4ef4-a411-6af5d5d66931",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43808,)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Save top 5 scores\n",
        "\"\"\"\n",
        "\n",
        "scores = scores.argsort()[-5:][::-1]"
      ],
      "metadata": {
        "id": "rfYP3XK7XrLV"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "yx7rA8_VkUH2",
        "outputId": "457f057c-4e24-49ac-9943-23c04ac5372f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   17, 34731, 40507, 37968,  2894])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Find index with the highest score\n",
        "\"\"\"\n",
        "\n",
        "#index = torch.argmax(scores)"
      ],
      "metadata": {
        "id": "YFpIRVYoq2OK",
        "outputId": "ac2da3a9-e8ae-488a-a0ee-39ee6559e557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFind index with the highest score\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Return code of the result with the highest score\n",
        "\"\"\"\n",
        "\n",
        "query_dataset.data[scores[0]]"
      ],
      "metadata": {
        "id": "PiIeFjZkrC6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Paraphrase result"
      ],
      "metadata": {
        "id": "TOHKyy-d1Id9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = query_dataset.data[scores[0]]"
      ],
      "metadata": {
        "id": "0pxaGqxi1NHP"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code = result['code']"
      ],
      "metadata": {
        "id": "-Um89kqH7VjN"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Return code of the result with the highest score\n",
        "\"\"\"\n",
        "\n",
        "docstring = ast.get_docstring(ast.parse(code).body[0])\n",
        "docstring = docstring.replace('\\n',\"\")"
      ],
      "metadata": {
        "id": "tX6sB_ia-EsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")  \n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
        "model.to(device)\n",
        "\n",
        "text =  \"paraphrase: \" + docstring + \" </s>\"\n",
        "\n",
        "encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
        "input_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=input_ids, attention_mask=attention_masks,\n",
        "    max_length=256,\n",
        "    do_sample=True,\n",
        "    top_k=120,\n",
        "    top_p=0.95,\n",
        "    early_stopping=True,\n",
        "    num_return_sequences=5\n",
        ")\n",
        "\n",
        "for output in outputs:\n",
        "    line = tokenizer.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    print(line)"
      ],
      "metadata": {
        "id": "luuUkkU470IS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d90133-1546-45fe-ae52-fa10c410d1b1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Vamsi/T5_Paraphrase_Paws/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Vamsi/T5_Paraphrase_Paws/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Vamsi/T5_Paraphrase_Paws/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Vamsi/T5_Paraphrase_Paws/resolve/main/spiece.model HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Vamsi/T5_Paraphrase_Paws/resolve/main/tokenizer.json HTTP/1.1\" 404 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Vamsi/T5_Paraphrase_Paws/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Vamsi/T5_Paraphrase_Paws/resolve/main/special_tokens_map.json HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Vamsi/T5_Paraphrase_Paws/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Vamsi/T5_Paraphrase_Paws/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Vamsi/T5_Paraphrase_Paws/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Vamsi/T5_Paraphrase_Paws/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Vamsi/T5_Paraphrase_Paws/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Establish an ssh tunnel for each local host and port that can be used to communicate with the state host.\n",
            "Ensure an ssh tunnel for each local host and port that can be used to communicate with the state host.\n",
            "Determine an ssh tunnel for each local host and port that can be used to communicate with the state host.\n",
            "Make an SSH tunnel for each local host and port that can be used to communicate with the state host.\n",
            "Each local host and port will use an ssh tunnel that can be used to communicate with the state host.\n"
          ]
        }
      ]
    }
  ]
}