{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicikess/hsg-nlp-course/blob/main/Part_I_Data_Collection_%26_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAvFru0C2R36"
      },
      "source": [
        "# Semantic Code Search using Transformers and BERT - Part I \n",
        "Author - Shashank Ramesh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVnoEmxe2R3-"
      },
      "source": [
        "<b>Objective</b> - To summarize we would be building a search engine which would take input a search query, find a function from a large pool of source code which implements the functionality semantically matching what is being queried."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61sPDRPC2R3_"
      },
      "source": [
        "<img src=\"images/2.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrVywXwU2R3_"
      },
      "source": [
        "In Part-I of the series where we discuss on gathering the data for training our model and preprocessing the raw data to make it useful for our models to gather insights from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG0U5VhT2R3_"
      },
      "source": [
        "## Data Collection and Preprocessing\n",
        "This notebook contains steps to gather the data for training our model and preprocessing the raw data to make it useful for our models to gather insights from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LnsBz_Rr2R4A"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "\n",
        "import ast\n",
        "import sqlite3\n",
        "\n",
        "import glob\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import astor\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlpnlDDD2jqe",
        "outputId": "dfeaf1bb-b828-4d53-fb20-8b906f1c0644"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdSx_xUn2R4B"
      },
      "source": [
        "### Collecting Data\n",
        "The data required for making our semantic search engine is large code bases which are made publicly available by GitHub on this [link](https://storage.googleapis.com/kubeflow-examplescode_search/raw_data/000000000000.csv). \n",
        "\n",
        "There are 10 such files which can be downloaded by changing the last digit to its successor in the [link](https://storage.googleapis.com/kubeflow-examplescode_search/raw_data/000000000000.csv). The data set contains code from repositories of diverse domains and can help in building a general purpose search engine.\n",
        "\n",
        "Once downloaded we get a corpus of data containing 12,41,664 files and we can use the below snippet to read all the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pUfJT3M2R4C",
        "outputId": "f790e604-e7b9-40e8-95f3-69953c4ae4ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 6s, sys: 12.4 s, total: 1min 19s\n",
            "Wall time: 1min 55s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Read the data into a pandas dataframe, and parse out some meta-data\n",
        "path_data = '/content/drive/MyDrive/NLP/data/raw/'\n",
        "\n",
        "df_list = list()\n",
        "\n",
        "for i in range(10):\n",
        "  df = pd.read_csv(path_data + f'00000000000{i}.csv')\n",
        "  df['nwo'] = df['repo_path'].apply(lambda r: r.split()[0]) # Get the name of repository with the owner name\n",
        "  df['path'] = df['repo_path'].apply(lambda r: r.split()[1]) # Path to the code file in the repository\n",
        "  df.drop(columns=['repo_path'], inplace=True)\n",
        "  df = df[['nwo', 'path', 'content']]\n",
        "  df_list.append(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "sY3tYohA2R4D",
        "outputId": "4c3877a3-5985-4016-86e1-9f52f51a834f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     nwo                                          path  \\\n",
              "0    bitsanity/rateboard                               krakenticker.py   \n",
              "1  rusty1s/embedded_gcnn                             lib/tf/convert.py   \n",
              "2          mackorone/mms                               util/ttf2png.py   \n",
              "3     nicksergeant/snipt                            accounts/models.py   \n",
              "4     huaxz1986/git_book  chapters/Model_Selection/validation_curve.py   \n",
              "\n",
              "                                             content  \n",
              "0  #!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n\\n...  \n",
              "1  import numpy as np\\nimport tensorflow as tf\\n\\...  \n",
              "2  import os\\nimport sys\\nimport string\\n\\ndef en...  \n",
              "3  from annoying.functions import get_object_or_N...  \n",
              "4  # -*- coding: utf-8 -*-\\n\"\"\"\\n    模型选择\\n    ~~...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c83785aa-41d7-42f6-bad8-8887c1b188d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nwo</th>\n",
              "      <th>path</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bitsanity/rateboard</td>\n",
              "      <td>krakenticker.py</td>\n",
              "      <td>#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rusty1s/embedded_gcnn</td>\n",
              "      <td>lib/tf/convert.py</td>\n",
              "      <td>import numpy as np\\nimport tensorflow as tf\\n\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mackorone/mms</td>\n",
              "      <td>util/ttf2png.py</td>\n",
              "      <td>import os\\nimport sys\\nimport string\\n\\ndef en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nicksergeant/snipt</td>\n",
              "      <td>accounts/models.py</td>\n",
              "      <td>from annoying.functions import get_object_or_N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>huaxz1986/git_book</td>\n",
              "      <td>chapters/Model_Selection/validation_curve.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\n    模型选择\\n    ~~...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c83785aa-41d7-42f6-bad8-8887c1b188d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c83785aa-41d7-42f6-bad8-8887c1b188d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c83785aa-41d7-42f6-bad8-8887c1b188d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqp6NzQd2R4E",
        "outputId": "653574b2-b6b6-4753-fdaf-c1090308bcf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(123998, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Inspect shape of the raw data\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLi9BmgB2R4E"
      },
      "source": [
        "### Extracting (Function,Docstring) Pairs \n",
        "\n",
        "Our goal is to parse the python files into (function, docstring) pairs. \n",
        "\n",
        "For this task we will be using the AST library which just as python compilers converts the source code into an abstract syntax tree for analysis. It identifies all different components in the source code and helps us extract them for processing. \n",
        "\n",
        "We are interested in extracting the function definitions along with its corresponding docstring removing other information like comments, decorators and function signatures. After we extract the function definition and its docstring we tokenize each of them to remove punctuation, decorators and convert all the tokens to lower case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rugidVVp2R4F"
      },
      "outputs": [],
      "source": [
        "def tokenize_docstring(text):\n",
        "    \"\"\"Gets filetered docstring tokens which help describe the function\"\"\"\n",
        "    \n",
        "    # Remove decorators and other parameter signatures in the docstring\n",
        "    before_keyword, keyword, after_keyword = text.partition(':')\n",
        "    before_keyword, keyword, after_keyword = before_keyword.partition('@param')\n",
        "    before_keyword, keyword, after_keyword = before_keyword.partition('param')\n",
        "    before_keyword, keyword, after_keyword = before_keyword.partition('@brief')\n",
        "    \n",
        "    if(after_keyword):    \n",
        "        words = RegexpTokenizer(r'[a-zA-Z0-9]+').tokenize(after_keyword)\n",
        "    else:\n",
        "        before_keyword, keyword, after_keyword = before_keyword.partition('@')\n",
        "        words = RegexpTokenizer(r'[a-zA-Z0-9]+').tokenize(before_keyword)\n",
        "        \n",
        "    # Convert all docstrings to lowercase\n",
        "    new_words= [word.lower() for word in words if word.isalnum()]\n",
        "    \n",
        "    return new_words\n",
        "\n",
        "\n",
        "def tokenize_code(text):\n",
        "    \"\"\"Gets filetered fucntion tokens\"\"\"\n",
        "    \n",
        "    # Remove decorators and function signatures till the def token\n",
        "    keyword = 'def '\n",
        "    before_keyword, keyword, after_keyword = text.partition(keyword)\n",
        "    words = RegexpTokenizer(r'[a-zA-Z0-9]+').tokenize(after_keyword)\n",
        "    \n",
        "    # Convert function tokens to lowercase and remove single alphabet variables\n",
        "    new_words= [word.lower() for word in words if (word.isalpha() and len(word)>1) or (word.isnumeric())]\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def get_function_docstring_pairs(blob):\n",
        "    \"Extracts (function/method, docstring) pairs from a given code blob.\"\n",
        "    \n",
        "    pairs = []\n",
        "    try:\n",
        "        module = ast.parse(blob) # Converts the python code into an abstract syntx tree\n",
        "        classes = [node for node in module.body if isinstance(node, ast.ClassDef)] # Retrieves classes from source code\n",
        "        functions = [node for node in module.body if isinstance(node, ast.FunctionDef)] # Retrieves functions from the source code\n",
        "        for _class in classes:\n",
        "            functions.extend([node for node in _class.body if isinstance(node, ast.FunctionDef)]) # Retrieves functions from the classes extracted\n",
        "\n",
        "        for f in functions:\n",
        "            source = astor.to_source(f) # Convert the functions extracted into ast format so as to remove comments\n",
        "            docstring = ast.get_docstring(f) if ast.get_docstring(f) else '' # Get docstring from fucntion definition if present\n",
        "            function = source.replace(ast.get_docstring(f, clean=False), '') if docstring else source # function definition without any comments\n",
        "            \n",
        "            # Extracts function name, line number of the function in the source code, original function, function tokens and docstring tokens \n",
        "            pairs.append((f.name,         \n",
        "                          f.lineno,\n",
        "                          source,\n",
        "                          ' '.join(tokenize_code(function)),\n",
        "                          ' '.join(tokenize_docstring(docstring.split('\\n\\n')[0]))\n",
        "                         ))\n",
        "    except (AssertionError, MemoryError, SyntaxError, UnicodeEncodeError):\n",
        "        pass\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def get_function_docstring_pairs_list(blob_list):\n",
        "    \"\"\"apply the function `get_function_docstring_pairs` on a list of blobs\"\"\"\n",
        "    return [get_function_docstring_pairs(b) for b in blob_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP-oCnRY2R4E"
      },
      "source": [
        "<img src=\"images/1.png\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": true,
        "id": "2i3mfFGi2R4G"
      },
      "outputs": [],
      "source": [
        "func_doc_list = list()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = get_function_docstring_pairs_list(df_list[0].content.tolist())\n"
      ],
      "metadata": {
        "id": "7sKBxAK-pDZz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(*df)"
      ],
      "metadata": {
        "id": "MhqatUcOVIAX",
        "outputId": "e6f5aafe-82fe-468f-9fe8-689e5c6d1265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
            "Traceback (most recent call last):\n",
            "  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
            "KeyboardInterrupt\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [] [] [('setup_dependent_environment', 53, \"def setup_dependent_environment(self, build_env, run_env, dep_spec):\\n    build_env.prepend_path('LD_LIBRARY_PATH', self.prefix.lib)\\n\", 'setup dependent environment self build env run env dep spec build env prepend path ld library path self prefix lib', ''), ('cmake_args', 62, \"def cmake_args(self):\\n    return ['-DBUILD_SHARED_LIBS:BOOL=ON']\\n\", 'cmake args self return dbuild shared libs bool on', '')] [('identity_block', 55, 'def identity_block(block, *args, **kwargs):\\n    return block\\n', 'identity block block args kwargs return block', ''), ('rename_sequence', 58, \"def rename_sequence(hdr, name):\\n    hdr['name'] = name\\n    return hdr\\n\", 'rename sequence hdr name hdr name name return hdr', ''), ('__init__', 37, 'def __init__(self, iring, seq_callback, data_callback, data_ref=None, *args,\\n    **kwargs):\\n    super(CallbackBlock, self).__init__(iring, *args, **kwargs)\\n    self.seq_callback = seq_callback\\n    self.data_callback = data_callback\\n    self.data_ref = data_ref\\n', 'init self iring seq callback data callback data ref none args kwargs super callbackblock self init iring args kwargs self seq callback seq callback self data callback data callback self data ref data ref', ''), ('on_sequence', 43, 'def on_sequence(self, iseq):\\n    self.seq_callback(iseq)\\n    return super(CallbackBlock, self).on_sequence(iseq)\\n', 'on sequence self iseq self seq callback iseq return super callbackblock self on sequence iseq', ''), ('on_data', 46, \"def on_data(self, ispan, ospan):\\n    self.data_callback(ispan, ospan)\\n    if self.data_ref is not None:\\n        self.data_ref['odata'] = ospan.data.copy()\\n    return super(CallbackBlock, self).on_data(ispan, ospan)\\n\", 'on data self ispan ospan self data callback ispan ospan if self data ref is not none self data ref odata ospan data copy return super callbackblock self on data ispan ospan', ''), ('__init__', 63, \"def __init__(self, fd):\\n    if fd.lower() == 'stdout':\\n        fd = 1\\n    elif fd.lower() == 'stderr':\\n        fd = 2\\n    else:\\n        assert isinstance(fd, int)\\n    self.fd = fd\\n    self.devnull = os.open(os.devnull, os.O_RDWR)\\n    self.stderr = os.dup(self.fd)\\n\", 'init self fd if fd lower stdout fd 1 elif fd lower stderr fd 2 else assert isinstance fd int self fd fd self devnull os open os devnull os rdwr self stderr os dup self fd', ''), ('__enter__', 70, 'def __enter__(self):\\n    os.dup2(self.devnull, self.fd)\\n', 'enter self os self devnull self fd', ''), ('__exit__', 72, 'def __exit__(self, type, value, tb):\\n    os.dup2(self.stderr, self.fd)\\n    os.close(self.devnull)\\n', 'exit self type value tb os self stderr self fd os close self devnull', ''), ('setUp', 77, \"def setUp(self):\\n    self.fil_file = './data/2chan16bitNoDM.fil'\\n\", 'setup self self fil file data fil', ''), ('test_read_sigproc', 82, \"def test_read_sigproc(self):\\n    gulp_nframe = 101\\n\\n    def check_sequence(seq):\\n        tensor = seq.header['_tensor']\\n        self.assertEqual(tensor['shape'], [-1, 1, 2])\\n        self.assertEqual(tensor['dtype'], 'u16')\\n        self.assertEqual(tensor['labels'], ['time', 'pol', 'freq'])\\n        self.assertEqual(tensor['units'], ['s', None, 'MHz'])\\n\\n    def check_data(ispan, ospan):\\n        self.assertLessEqual(ispan.nframe, gulp_nframe)\\n        self.assertEqual(ospan.nframe, ispan.nframe)\\n        self.assertEqual(ispan.data.shape, (ispan.nframe, 1, 2))\\n        self.assertEqual(ospan.data.shape, (ospan.nframe, 1, 2))\\n    with bf.Pipeline() as pipeline:\\n        data = read_sigproc([self.fil_file], gulp_nframe)\\n        data = CallbackBlock(data, check_sequence, check_data)\\n        pipeline.run()\\n\", 'test read sigproc self gulp nframe 101 def check sequence seq tensor seq header tensor self assertequal tensor shape 1 1 2 self assertequal tensor dtype self assertequal tensor labels time pol freq self assertequal tensor units none mhz def check data ispan ospan self assertlessequal ispan nframe gulp nframe self assertequal ospan nframe ispan nframe self assertequal ispan data shape ispan nframe 1 2 self assertequal ospan data shape ospan nframe 1 2 with bf pipeline as pipeline data read sigproc self fil file gulp nframe data callbackblock data check sequence check data pipeline run', ''), ('run_test_simple_copy', 99, \"def run_test_simple_copy(self, guarantee, test_views=False, gulp_nframe_inc=0):\\n\\n    def check_sequence(seq):\\n        hdr = seq.header\\n        tensor = hdr['_tensor']\\n        self.assertEqual(tensor['shape'], [-1, 1, 2])\\n        self.assertEqual(tensor['scales'], [[813283200.0, 8e-05], None, [\\n            433.968, -0.062]])\\n        self.assertEqual(tensor['dtype'], 'u16')\\n        self.assertEqual(tensor['labels'], ['time', 'pol', 'freq'])\\n        self.assertEqual(tensor['units'], ['s', None, 'MHz'])\\n\\n    def check_data(ispan, ospan):\\n        pass\\n    gulp_nframe = 101\\n    with bf.Pipeline() as pipeline:\\n        data = read_sigproc([self.fil_file], gulp_nframe)\\n        if test_views:\\n            data = bf.views.split_axis(data, 'freq', 2, 'fine_freq')\\n            data = bf.views.merge_axes(data, 'freq', 'fine_freq')\\n            data = bf.views.rename_axis(data, 'freq', 'chan')\\n            data = bf.views.rename_axis(data, 'chan', 'freq')\\n            data = bf.views.reverse_scale(data, 'freq')\\n            data = bf.views.reverse_scale(data, 'freq')\\n            data = bf.views.reinterpret_axis(data, 'freq', label='chan',\\n                scale=[0, 1], units='THz')\\n            data = bf.views.reinterpret_axis(data, 'chan', label='freq',\\n                scale=[433.968, -0.062], units='MHz')\\n            data = bf.views.astype(data, 'i16')\\n            data = bf.views.astype(data, 'u16')\\n            data = bf.views.add_axis(data, -1, 'phony_axis', scale=(0, 1),\\n                units='imaginary')\\n            data = bf.views.delete_axis(data, 'phony_axis')\\n            data = bf.views.add_axis(data, 0, 'phony_axis')\\n            data = bf.views.delete_axis(data, 'phony_axis')\\n            data = bf.views.custom(data, lambda hdr: rename_sequence(hdr,\\n                hdr['name']))\\n        for i in xrange(20):\\n            if gulp_nframe_inc != 0:\\n                data = copy(data, guarantee=guarantee, gulp_nframe=\\n                    gulp_nframe + i * gulp_nframe_inc)\\n            else:\\n                data = copy(data, guarantee=guarantee)\\n        data = copy(data, guarantee=guarantee, gulp_nframe=gulp_nframe)\\n        ref = {}\\n        data = CallbackBlock(data, check_sequence, check_data, data_ref=ref)\\n        pipeline.run()\\n        self.assertEqual(ref['odata'].dtype, 'uint16')\\n        self.assertEqual(ref['odata'].shape, (29, 1, 2))\\n\", 'run test simple copy self guarantee test views false gulp nframe inc 0 def check sequence seq hdr seq header tensor hdr tensor self assertequal tensor shape 1 1 2 self assertequal tensor scales 813283200 0 05 none 433 968 0 062 self assertequal tensor dtype self assertequal tensor labels time pol freq self assertequal tensor units none mhz def check data ispan ospan pass gulp nframe 101 with bf pipeline as pipeline data read sigproc self fil file gulp nframe if test views data bf views split axis data freq 2 fine freq data bf views merge axes data freq fine freq data bf views rename axis data freq chan data bf views rename axis data chan freq data bf views reverse scale data freq data bf views reverse scale data freq data bf views reinterpret axis data freq label chan scale 0 1 units thz data bf views reinterpret axis data chan label freq scale 433 968 0 062 units mhz data bf views astype data data bf views astype data data bf views add axis data 1 phony axis scale 0 1 units imaginary data bf views delete axis data phony axis data bf views add axis data 0 phony axis data bf views delete axis data phony axis data bf views custom data lambda hdr rename sequence hdr hdr name for in xrange 20 if gulp nframe inc 0 data copy data guarantee guarantee gulp nframe gulp nframe gulp nframe inc else data copy data guarantee guarantee data copy data guarantee guarantee gulp nframe gulp nframe ref data callbackblock data check sequence check data data ref ref pipeline run self assertequal ref odata dtype self assertequal ref odata shape 29 1 2', ''), ('test_simple_copy', 143, 'def test_simple_copy(self):\\n    self.run_test_simple_copy(guarantee=True)\\n', 'test simple copy self self run test simple copy guarantee true', ''), ('test_simple_copy_unguaranteed', 145, 'def test_simple_copy_unguaranteed(self):\\n    self.run_test_simple_copy(guarantee=False)\\n', 'test simple copy unguaranteed self self run test simple copy guarantee false', ''), ('test_simple_copy_mixed_gulp_nframe', 147, 'def test_simple_copy_mixed_gulp_nframe(self):\\n    self.run_test_simple_copy(guarantee=True, gulp_nframe_inc=1)\\n    self.run_test_simple_copy(guarantee=True, gulp_nframe_inc=3)\\n', 'test simple copy mixed gulp nframe self self run test simple copy guarantee true gulp nframe inc 1 self run test simple copy guarantee true gulp nframe inc 3', ''), ('test_simple_copy_mixed_gulp_nframe_unguaranteed', 150, 'def test_simple_copy_mixed_gulp_nframe_unguaranteed(self):\\n    self.run_test_simple_copy(guarantee=False, gulp_nframe_inc=1)\\n    self.run_test_simple_copy(guarantee=False, gulp_nframe_inc=3)\\n', 'test simple copy mixed gulp nframe unguaranteed self self run test simple copy guarantee false gulp nframe inc 1 self run test simple copy guarantee false gulp nframe inc 3', ''), ('test_simple_views', 153, 'def test_simple_views(self):\\n    self.run_test_simple_copy(guarantee=True, test_views=True)\\n', 'test simple views self self run test simple copy guarantee true test views true', ''), ('test_simple_views_unguaranteed', 155, 'def test_simple_views_unguaranteed(self):\\n    self.run_test_simple_copy(guarantee=False, test_views=True)\\n', 'test simple views unguaranteed self self run test simple copy guarantee false test views true', ''), ('test_block_chainer', 157, \"def test_block_chainer(self):\\n    with bf.Pipeline() as pipeline:\\n        bc = bf.BlockChainer()\\n        bc.blocks.read_sigproc([self.fil_file], gulp_nframe=100)\\n        bc.blocks.transpose(['freq', 'time', 'pol'])\\n        bc.views.split_axis('time', 1)\\n        bc.custom(identity_block)()\\n        pipeline.run()\\n\", 'test block chainer self with bf pipeline as pipeline bc bf blockchainer bc blocks read sigproc self fil file gulp nframe 100 bc blocks transpose freq time pol bc views split axis time 1 bc custom identity block pipeline run', ''), ('test_block_initialization_failure', 165, \"def test_block_initialization_failure(self):\\n\\n    def check_sequence(seq):\\n        raise ValueError('Intentional on_sequence failure')\\n\\n    def check_data(ispan, ospan):\\n        pass\\n    with bf.Pipeline() as pipeline:\\n        data = read_sigproc([self.fil_file], gulp_nframe=101)\\n        data = copy(data)\\n        data = copy(data)\\n        data = CallbackBlock(data, check_sequence, check_data)\\n        data = copy(data)\\n        data = copy(data)\\n        with suppress_fd('stderr'):\\n            self.assertRaises(bf.pipeline.PipelineInitError, pipeline.run)\\n\", 'test block initialization failure self def check sequence seq raise valueerror intentional on sequence failure def check data ispan ospan pass with bf pipeline as pipeline data read sigproc self fil file gulp nframe 101 data copy data data copy data data callbackblock data check sequence check data data copy data data copy data with suppress fd stderr self assertraises bf pipeline pipelineiniterror pipeline run', '')] [('test_start_in_thread', 349, '@pytest.mark.parametrize(\\'sd\\', [None, FakeSD()])\\ndef test_start_in_thread(self, sd):\\n    \"\"\"\\n        Threaded version starts and exits properly, passes on service\\n        discovery.\\n        \"\"\"\\n    Counter(\\'test_start_http_server_in_thread\\', \\'cnt\\').inc()\\n    t = aio.web.start_http_server_in_thread(addr=\\'127.0.0.1\\',\\n        service_discovery=sd)\\n    assert isinstance(t, aio.web.ThreadedMetricsHTTPServer)\\n    assert \\'PrometheusAsyncWebEndpoint\\' == t._thread.name\\n    assert t.url.startswith(\\'http\\')\\n    assert False is t.https\\n    assert t.is_registered is (sd is not None)\\n    if sd is not None:\\n        assert sd.registered_ms is t._http_server\\n    s = t.socket\\n    h = http.client.HTTPConnection(s.addr, port=s[1])\\n    h.request(\\'GET\\', \\'/metrics\\')\\n    rsp = h.getresponse()\\n    body = rsp.read().decode()\\n    rsp.close()\\n    h.close()\\n    assert \\'HELP test_start_http_server_in_thread cnt\\' in body\\n    t.close()\\n    assert False is t._thread.is_alive()\\n', 'test start in thread self sd counter test start http server in thread cnt inc aio web start http server in thread addr 127 0 0 1 service discovery sd assert isinstance aio web threadedmetricshttpserver assert prometheusasyncwebendpoint thread name assert url startswith http assert false is https assert is registered is sd is not none if sd is not none assert sd registered ms is http server socket http client httpconnection addr port 1 request get metrics rsp getresponse body rsp read decode rsp close close assert help test start http server in thread cnt in body close assert false is thread is alive', 'threaded version starts and exits properly passes on service discovery'), ('test_present', 408, '@pytest.mark.skipif(aiohttp is None, reason=\\'Needs aiohttp.\\')\\ndef test_present(self):\\n    \"\"\"\\n        If aiohttp is present, the original object is returned.\\n        \"\"\"\\n    o = object()\\n    assert o is aio.web._needs_aiohttp(o)\\n', 'test present self object assert is aio web needs aiohttp', 'if aiohttp is present the original object is returned'), ('test_missing', 416, '@pytest.mark.skipif(aiohttp is not None, reason=\\'Needs missing aiohttp.\\')\\ndef test_missing(self):\\n    \"\"\"\\n        If aiohttp is missing, raise RuntimeError if called.\\n        \"\"\"\\n    with pytest.raises(RuntimeError) as e:\\n        aio.web._needs_aiohttp(coro)()\\n    assert \"\\'coro\\' requires aiohttp.\" == str(e.value)\\n', 'test missing self with pytest raises runtimeerror as aio web needs aiohttp coro assert coro requires aiohttp str value', 'if aiohttp is missing raise runtimeerror if called'), ('test_sets_headers', 502, 'def test_sets_headers(self):\\n    \"\"\"\\n        If a token is passed, \"X-Consul-Token\" header is set.\\n        \"\"\"\\n    con = _LocalConsulAgentClient(token=\\'token42\\')\\n    assert \\'token42\\' == con.headers[\\'X-Consul-Token\\']\\n', 'test sets headers self con localconsulagentclient token assert con headers consul token', 'if a token is passed x consul token header is set')] [('markdown_reader', 44, \"def markdown_reader(path):\\n    meta, content = split_content(path)\\n    content = content.replace('\\\\n', '  \\\\n')\\n    if not meta:\\n        meta = ObjectDict()\\n    else:\\n        meta = parse_meta(meta, path)\\n    return Post(path=path, meta=meta, content=md.render(content))\\n\", 'markdown reader path meta content split content path content content replace if not meta meta objectdict else meta parse meta meta path return post path path meta meta content md render content', ''), ('block_code', 17, \"def block_code(self, text, lang):\\n    try:\\n        lexer = get_lexer_by_name(lang, stripall=True)\\n    except ClassNotFound:\\n        text = escape_html(text.strip())\\n        return '\\\\n<pre><code>%s</code></pre>\\\\n' % text\\n    else:\\n        formatter = HtmlFormatter()\\n        return highlight(text, lexer, formatter)\\n\", 'block code self text lang try lexer get lexer by name lang stripall true except classnotfound text escape html text strip return pre code code pre text else formatter htmlformatter return highlight text lexer formatter', ''), ('autolink', 27, 'def autolink(self, link, is_email):\\n    if is_email:\\n        s = \\'<a href=\"mailto:{link}\">{link}</a>\\'\\n    elif link.endswith((\\'.jpg\\', \\'.png\\', \\'.gif\\', \\'.jpeg\\')):\\n        s = \\'<a href=\"{link}\"><img src=\"{link}\" /></a>\\'\\n    else:\\n        s = \\'<a href=\"{link}\">{link}</a>\\'\\n    return s.format(link=link)\\n', 'autolink self link is email if is email href mailto link link elif link endswith jpg png gif jpeg href link img src link else href link link return format link link', '')] [('_is_field', 28, 'def _is_field(value):\\n    return isinstance(value, type) and issubclass(value, fields.Field)\\n', 'is field value return isinstance value type and issubclass value fields field', ''), ('_has_default', 35, 'def _has_default(column):\\n    return (column.default is not None or column.server_default is not None or\\n        _is_auto_increment(column))\\n', 'has default column return column default is not none or column server default is not none or is auto increment column', ''), ('_is_auto_increment', 43, 'def _is_auto_increment(column):\\n    return (column.table is not None and column is column.table.\\n        _autoincrement_column)\\n', 'is auto increment column return column table is not none and column is column table autoincrement column', ''), ('_postgres_array_factory', 50, 'def _postgres_array_factory(converter, data_type):\\n    return functools.partial(fields.List, converter.\\n        _get_field_class_for_data_type(data_type.item_type))\\n', 'postgres array factory converter data type return functools partial fields list converter get field class for data type data type item type', ''), ('_should_exclude_field', 57, 'def _should_exclude_field(column, fields=None, exclude=None):\\n    if fields and column.key not in fields:\\n        return True\\n    if exclude and column.key in exclude:\\n        return True\\n    return False\\n', 'should exclude field column fields none exclude none if fields and column key not in fields return true if exclude and column key in exclude return true return false', ''), ('__init__', 98, 'def __init__(self, schema_cls=None):\\n    self.schema_cls = schema_cls\\n', 'init self schema cls none self schema cls schema cls', ''), ('type_mapping', 101, '@property\\ndef type_mapping(self):\\n    if self.schema_cls:\\n        return self.schema_cls.TYPE_MAPPING\\n    else:\\n        return ma.Schema.TYPE_MAPPING\\n', 'type mapping self if self schema cls return self schema cls type mapping else return ma schema type mapping', ''), ('fields_for_model', 108, \"def fields_for_model(self, model, include_fk=False, fields=None, exclude=\\n    None, base_fields=None, dict_cls=dict):\\n    result = dict_cls()\\n    base_fields = base_fields or {}\\n    for prop in model.__mapper__.iterate_properties:\\n        if _should_exclude_field(prop, fields=fields, exclude=exclude):\\n            continue\\n        if hasattr(prop, 'columns'):\\n            if not include_fk:\\n                for column in prop.columns:\\n                    if not column.foreign_keys:\\n                        break\\n                else:\\n                    continue\\n        field = base_fields.get(prop.key) or self.property2field(prop)\\n        if field:\\n            result[prop.key] = field\\n    return result\\n\", 'fields for model self model include fk false fields none exclude none base fields none dict cls dict result dict cls base fields base fields or for prop in model mapper iterate properties if should exclude field prop fields fields exclude exclude continue if hasattr prop columns if not include fk for column in prop columns if not column foreign keys break else continue field base fields get prop key or self prop if field result prop key field return result', ''), ('fields_for_table', 129, 'def fields_for_table(self, table, include_fk=False, fields=None, exclude=\\n    None, base_fields=None, dict_cls=dict):\\n    result = dict_cls()\\n    base_fields = base_fields or {}\\n    for column in table.columns:\\n        if _should_exclude_field(column, fields=fields, exclude=exclude):\\n            continue\\n        if not include_fk and column.foreign_keys:\\n            continue\\n        field = base_fields.get(column.key) or self.column2field(column)\\n        if field:\\n            result[column.key] = field\\n    return result\\n', 'fields for table self table include fk false fields none exclude none base fields none dict cls dict result dict cls base fields base fields or for column in table columns if should exclude field column fields fields exclude exclude continue if not include fk and column foreign keys continue field base fields get column key or self column if field result column key field return result', ''), ('property2field', 143, \"def property2field(self, prop, instance=True, field_class=None, **kwargs):\\n    field_class = field_class or self._get_field_class_for_property(prop)\\n    if not instance:\\n        return field_class\\n    field_kwargs = self._get_field_kwargs_for_property(prop)\\n    field_kwargs.update(kwargs)\\n    ret = field_class(**field_kwargs)\\n    if hasattr(prop, 'direction') and self.DIRECTION_MAPPING[prop.direction\\n        .name] and prop.uselist is True:\\n        ret = fields.List(ret, **kwargs)\\n    return ret\\n\", 'self prop instance true field class none kwargs field class field class or self get field class for property prop if not instance return field class field kwargs self get field kwargs for property prop field kwargs update kwargs ret field class field kwargs if hasattr prop direction and self direction mapping prop direction name and prop uselist is true ret fields list ret kwargs return ret', ''), ('column2field', 158, 'def column2field(self, column, instance=True, **kwargs):\\n    field_class = self._get_field_class_for_column(column)\\n    if not instance:\\n        return field_class\\n    field_kwargs = self.get_base_kwargs()\\n    self._add_column_kwargs(field_kwargs, column)\\n    field_kwargs.update(kwargs)\\n    return field_class(**field_kwargs)\\n', 'self column instance true kwargs field class self get field class for column column if not instance return field class field kwargs self get base kwargs self add column kwargs field kwargs column field kwargs update kwargs return field class field kwargs', ''), ('field_for', 167, 'def field_for(self, model, property_name, **kwargs):\\n    prop = model.__mapper__.get_property(property_name)\\n    return self.property2field(prop, **kwargs)\\n', 'field for self model property name kwargs prop model mapper get property property name return self prop kwargs', ''), ('_get_field_class_for_column', 171, 'def _get_field_class_for_column(self, column):\\n    return self._get_field_class_for_data_type(column.type)\\n', 'get field class for column self column return self get field class for data type column type', ''), ('_get_field_class_for_data_type', 174, \"def _get_field_class_for_data_type(self, data_type):\\n    field_cls = None\\n    types = inspect.getmro(type(data_type))\\n    for col_type in types:\\n        if col_type in self.SQLA_TYPE_MAPPING:\\n            field_cls = self.SQLA_TYPE_MAPPING[col_type]\\n            if callable(field_cls) and not _is_field(field_cls):\\n                field_cls = field_cls(self, data_type)\\n            break\\n    else:\\n        try:\\n            python_type = data_type.python_type\\n        except NotImplementedError:\\n            python_type = None\\n        if python_type in self.type_mapping:\\n            field_cls = self.type_mapping[python_type]\\n        else:\\n            if hasattr(data_type, 'impl'):\\n                return self._get_field_class_for_data_type(data_type.impl)\\n            raise ModelConversionError(\\n                'Could not find field column of type {0}.'.format(types[0]))\\n    return field_cls\\n\", 'get field class for data type self data type field cls none types inspect getmro type data type for col type in types if col type in self sqla type mapping field cls self sqla type mapping col type if callable field cls and not is field field cls field cls field cls self data type break else try python type data type python type except notimplementederror python type none if python type in self type mapping field cls self type mapping python type else if hasattr data type impl return self get field class for data type data type impl raise modelconversionerror could not find field column of type 0 format types 0 return field cls', ''), ('_get_field_class_for_property', 200, \"def _get_field_class_for_property(self, prop):\\n    if hasattr(prop, 'direction'):\\n        field_cls = Related\\n    else:\\n        column = prop.columns[0]\\n        field_cls = self._get_field_class_for_column(column)\\n    return field_cls\\n\", 'get field class for property self prop if hasattr prop direction field cls related else column prop columns 0 field cls self get field class for column column return field cls', ''), ('_get_field_kwargs_for_property', 208, \"def _get_field_kwargs_for_property(self, prop):\\n    kwargs = self.get_base_kwargs()\\n    if hasattr(prop, 'columns'):\\n        column = prop.columns[0]\\n        self._add_column_kwargs(kwargs, column)\\n    if hasattr(prop, 'direction'):\\n        self._add_relationship_kwargs(kwargs, prop)\\n    if getattr(prop, 'doc', None):\\n        kwargs['description'] = prop.doc\\n    return kwargs\\n\", 'get field kwargs for property self prop kwargs self get base kwargs if hasattr prop columns column prop columns 0 self add column kwargs kwargs column if hasattr prop direction self add relationship kwargs kwargs prop if getattr prop doc none kwargs description prop doc return kwargs', ''), ('_add_column_kwargs', 219, 'def _add_column_kwargs(self, kwargs, column):\\n    \"\"\"Add keyword arguments to kwargs (in-place) based on the passed in\\n        `Column <sqlalchemy.schema.Column>`.\\n        \"\"\"\\n    if column.nullable:\\n        kwargs[\\'allow_none\\'] = True\\n    kwargs[\\'required\\'] = not column.nullable and not _has_default(column)\\n    if hasattr(column.type, \\'enums\\'):\\n        kwargs[\\'validate\\'].append(validate.OneOf(choices=column.type.enums))\\n    if hasattr(column.type, \\'length\\'):\\n        try:\\n            python_type = column.type.python_type\\n        except (AttributeError, NotImplementedError):\\n            python_type = None\\n        if not python_type or not issubclass(python_type, uuid.UUID):\\n            kwargs[\\'validate\\'].append(validate.Length(max=column.type.length))\\n    if hasattr(column.type, \\'scale\\'):\\n        kwargs[\\'places\\'] = getattr(column.type, \\'scale\\', None)\\n', 'add column kwargs self kwargs column if column nullable kwargs allow none true kwargs required not column nullable and not has default column if hasattr column type enums kwargs validate append validate oneof choices column type enums if hasattr column type length try python type column type python type except attributeerror notimplementederror python type none if not python type or not issubclass python type uuid uuid kwargs validate append validate length max column type length if hasattr column type scale kwargs places getattr column type scale none', 'add keyword arguments to kwargs in place based on the passed in column sqlalchemy schema column'), ('_add_relationship_kwargs', 244, 'def _add_relationship_kwargs(self, kwargs, prop):\\n    \"\"\"Add keyword arguments to kwargs (in-place) based on the passed in\\n        relationship `Property`.\\n        \"\"\"\\n    nullable = True\\n    for pair in prop.local_remote_pairs:\\n        if not pair[0].nullable:\\n            if prop.uselist is True:\\n                nullable = False\\n            break\\n    kwargs.update({\\'allow_none\\': nullable, \\'required\\': not nullable})\\n', 'add relationship kwargs self kwargs prop nullable true for pair in prop local remote pairs if not pair 0 nullable if prop uselist is true nullable false break kwargs update allow none nullable required not nullable', 'add keyword arguments to kwargs in place based on the passed in relationship property'), ('get_base_kwargs', 259, \"def get_base_kwargs(self):\\n    return {'validate': []}\\n\", 'get base kwargs self return validate', '')] [('__init__', 52, \"def __init__(self, *args, **kwargs):\\n    super(ChangeUserForm, self).__init__(*args, **kwargs)\\n    self.user = kwargs['instance']\\n    if self.user.is_managed:\\n        self.fields['username'] = ReadOnlyTextField(label='Username (managed)')\\n\", 'init self args kwargs super changeuserform self init args kwargs self user kwargs instance if self user is managed self fields username readonlytextfield label username managed', ''), ('clean_username', 58, \"def clean_username(self):\\n    if self.user.is_managed:\\n        return self.user.username\\n    return self.cleaned_data['username']\\n\", 'clean username self if self user is managed return self user username return self cleaned data username', ''), ('save', 84, 'def save(self, group, user, event=None):\\n    activity = Activity.objects.create(group=group, project=group.project,\\n        type=Activity.NOTE, user=user, data=self.cleaned_data)\\n    activity.send_notification()\\n    return activity\\n', 'save self group user event none activity activity objects create group group project group project type activity note user user data self cleaned data activity send notification return activity', '')] [('url_for_version', 50, \"def url_for_version(self, version):\\n    base_url = 'https://github.com/evaleev/libint/archive'\\n    if version == Version('1.0.0'):\\n        return '{0}/LIBINT_1_00.tar.gz'.format(base_url)\\n    elif version < Version('2.1.0'):\\n        return '{0}/release-{1}.tar.gz'.format(base_url, version.dashed)\\n    else:\\n        return '{0}/v{1}.tar.gz'.format(base_url, version)\\n\", 'url for version self version base url https github com evaleev libint archive if version version 1 0 0 return 0 libint 1 00 tar gz format base url elif version version 2 1 0 return 0 release 1 tar gz format base url version dashed else return 0 1 tar gz format base url version', ''), ('autoreconf', 59, \"def autoreconf(self, spec, prefix):\\n    libtoolize()\\n    aclocal('-I', 'lib/autoconf')\\n    autoconf()\\n\", 'autoreconf self spec prefix libtoolize aclocal lib autoconf autoconf', ''), ('optflags', 64, \"@property\\ndef optflags(self):\\n    flags = '-O2'\\n    if '%intel' in self.spec:\\n        flags += ' -xSSE2 -xAVX -axCORE-AVX2 -ipo'\\n    return flags\\n\", 'optflags self flags if intel in self spec flags xavx axcore ipo return flags', ''), ('setup_environment', 75, \"def setup_environment(self, build_env, run_env):\\n    build_env.set('CFLAGS', self.optflags)\\n    build_env.set('CXXFLAGS', self.optflags)\\n    if '%intel' in self.spec and which('xiar'):\\n        build_env.set('AR', 'xiar')\\n\", 'setup environment self build env run env build env set cflags self optflags build env set cxxflags self optflags if intel in self spec and which xiar build env set ar xiar', ''), ('configure_args', 85, \"def configure_args(self):\\n    config_args = ['--enable-shared']\\n    optflags = self.optflags\\n    if self.version < Version('2.0.0'):\\n        config_args.extend(['--with-cc-optflags={0}'.format(optflags),\\n            '--with-cxx-optflags={0}'.format(optflags)])\\n    else:\\n        config_args.extend(['--with-cxx-optflags={0}'.format(optflags),\\n            '--with-cxxgen-optflags={0}'.format(optflags)])\\n    if self.version < Version('2.0.0'):\\n        config_args.extend(['--with-libint-max-am=5',\\n            '--with-libderiv-max-am1=4'])\\n    return config_args\\n\", 'configure args self config args enable shared optflags self optflags if self version version 2 0 0 config args extend with cc optflags 0 format optflags with cxx optflags 0 format optflags else config args extend with cxx optflags 0 format optflags with cxxgen optflags 0 format optflags if self version version 2 0 0 config args extend with libint max am 5 with libderiv max 4 return config args', '')] [('on_key_press', 20, \"def on_key_press(self, symbol, modifiers):\\n    if symbol == key.V:\\n        visible = modifiers & key.MOD_SHIFT\\n        self.w.set_mouse_visible(visible)\\n        print('Mouse is now %s' % (visible and 'visible' or 'hidden'))\\n\", 'on key press self symbol modifiers if symbol key visible modifiers key mod shift self set mouse visible visible print mouse is now visible and visible or hidden', ''), ('on_mouse_motion', 26, \"def on_mouse_motion(self, x, y, dx, dy):\\n    print('on_mousemotion(x=%f, y=%f, dx=%f, dy=%f)' % (x, y, dx, dy))\\n\", 'on mouse motion self dx dy print on mousemotion dx dy dx dy', ''), ('test_set_visible', 29, 'def test_set_visible(self):\\n    print(__doc__)\\n    self.width, self.height = 200, 200\\n    self.w = w = window.Window(self.width, self.height)\\n    w.push_handlers(self)\\n    while not w.has_exit:\\n        w.dispatch_events()\\n    w.close()\\n', 'test set visible self print doc self width self height 200 200 self window window self width self height push handlers self while not has exit dispatch events close', '')] [('__str__', 50, \"def __str__(self):\\n    return 'Label Attribute'\\n\", 'str self return label attribute', '')] [('load_tile_positions', 16, 'def load_tile_positions(filename):\\n    \"\"\"Returns a dictionary of positions {name: (x, y), ..} parsed from the file\"\"\"\\n    tile_positions = {}\\n    with open(filename) as f:\\n        for row in csv.reader(f, delimiter=\\'\\\\t\\'):\\n            name = row[0]\\n            if not name.startswith(\\'REMARK\\'):\\n                x = int(row[1])\\n                y = int(row[2])\\n                rect = Rect(x * SIZE, y * SIZE, SIZE, SIZE)\\n                tile_positions[name] = rect\\n    return tile_positions\\n', 'load tile positions filename tile positions with open filename as for row in csv reader delimiter name row 0 if not name startswith remark int row 1 int row 2 rect rect size size size size tile positions name rect return tile positions', 'returns a dictionary of positions name')] [('test_main', 14, 'def test_main():\\n    \"\"\"Test function for verifying basic functionality.\"\"\"\\n    print(\\'Running test_main\\')\\n    i2c = I2C(1, I2C.MASTER)\\n    lcd = I2cLcd(i2c, DEFAULT_LCD_I2C_ADDR, 2, 16, DEFAULT_RGB_I2C_ADDR)\\n    lcd.putstr(\\'It Works!\\\\nSecond Line\\')\\n    lcd.backlight_rgb(255, 255, 255)\\n    lcd.blink_cursor_on()\\n    time.sleep_ms(1000)\\n    lcd.clear()\\n    lcd.putstr(\\'Seeed Grove LCD RGB Backlight\\')\\n    time.sleep_ms(1000)\\n    lcd.clear()\\n    lcd.putstr(\\'1234567890123456abcdefghijklmnop\\')\\n    time.sleep_ms(1000)\\n    lcd.hide_cursor()\\n    lcd.clear()\\n    lcd.putstr(\\'Red\\')\\n    lcd.backlight_rgb(255, 0, 0)\\n    time.sleep_ms(1000)\\n    lcd.clear()\\n    lcd.putstr(\\'Green\\')\\n    lcd.backlight_rgb(0, 255, 0)\\n    time.sleep_ms(1000)\\n    lcd.clear()\\n    lcd.putstr(\\'Blue\\')\\n    lcd.backlight_rgb(0, 0, 255)\\n    time.sleep_ms(1000)\\n    lcd.clear()\\n    lcd.putstr(\\'Blue\\')\\n    time.sleep_ms(1000)\\n    lcd.backlight_invert_on()\\n    lcd.putstr(\\' Inverted\\')\\n    time.sleep_ms(1000)\\n    lcd.clear()\\n    lcd.putstr(\\'Blue\\')\\n    lcd.backlight_invert_off()\\n    time.sleep_ms(1000)\\n    lcd.clear()\\n    lcd.putstr(\\'Blinking PWM\\\\nDuty=127 Freq=1\\')\\n    lcd.backlight_rgb(255, 0, 0)\\n    lcd.backlight_blink(127, 1)\\n    time.sleep_ms(2000)\\n    lcd.backlight_normal()\\n    lcd.clear()\\n    lcd.putstr(\\'Blinking PWM\\\\nDuty=100 Freq=10\\')\\n    lcd.backlight_rgb(255, 255, 0)\\n    lcd.backlight_blink(100, 10)\\n    time.sleep_ms(2000)\\n    lcd.backlight_normal()\\n    lcd.clear()\\n    lcd.putstr(\\'Blinking PWM\\\\nDuty=10 Freq=20\\')\\n    lcd.backlight_rgb(0, 255, 255)\\n    lcd.backlight_blink(10, 20)\\n    time.sleep_ms(5000)\\n    lcd.backlight_normal()\\n    lcd.clear()\\n    lcd.putstr(\\'Brightness 255\\')\\n    lcd.backlight_brightness(255)\\n    time.sleep_ms(1000)\\n    lcd.clear()\\n    lcd.putstr(\\'Brightness 127\\')\\n    lcd.backlight_brightness(127)\\n    time.sleep_ms(1000)\\n    lcd.clear()\\n    lcd.putstr(\\'Brightness 63\\')\\n    lcd.backlight_brightness(63)\\n    time.sleep_ms(1000)\\n    lcd.clear()\\n    lcd.putstr(\\'Brightness 1\\')\\n    lcd.backlight_brightness(1)\\n    time.sleep_ms(1000)\\n    lcd.clear()\\n    lcd.putstr(\\'Brightness 0\\')\\n    lcd.backlight_brightness(0)\\n    time.sleep_ms(1000)\\n    lcd.clear()\\n    lcd.backlight_normal()\\n    lcd.backlight_brightness(255)\\n    lcd.backlight_rgb(255, 0, 0)\\n    lcd.backlight_sleep()\\n    lcd.putstr(\\'low power mode\\\\n\\')\\n    time.sleep_ms(2000)\\n    lcd.backlight_rgb(0, 255, 255)\\n    lcd.putstr(\\'cant change bg\\')\\n    time.sleep_ms(2000)\\n    lcd.clear()\\n    lcd.putstr(\\'full power mode\\\\n\\')\\n    lcd.backlight_wake()\\n    lcd.putstr(\\'bg now changed\\')\\n    time.sleep_ms(2000)\\n    lcd.clear()\\n    lcd.putstr(\\'Rainbow\\')\\n    for i in range(2):\\n        rainbow(lcd, 10)\\n    time.sleep_ms(1000)\\n    lcd.clear()\\n    lcd.putstr(\\'Random\\')\\n    randomColors(lcd, 30, 100)\\n', 'test main print running test main 1 master lcd default lcd addr 2 16 default rgb addr lcd putstr it works nsecond line lcd backlight rgb 255 255 255 lcd blink cursor on time sleep ms 1000 lcd clear lcd putstr seeed grove lcd rgb backlight time sleep ms 1000 lcd clear lcd putstr time sleep ms 1000 lcd hide cursor lcd clear lcd putstr red lcd backlight rgb 255 0 0 time sleep ms 1000 lcd clear lcd putstr green lcd backlight rgb 0 255 0 time sleep ms 1000 lcd clear lcd putstr blue lcd backlight rgb 0 0 255 time sleep ms 1000 lcd clear lcd putstr blue time sleep ms 1000 lcd backlight invert on lcd putstr inverted time sleep ms 1000 lcd clear lcd putstr blue lcd backlight invert off time sleep ms 1000 lcd clear lcd putstr blinking pwm nduty 127 freq 1 lcd backlight rgb 255 0 0 lcd backlight blink 127 1 time sleep ms 2000 lcd backlight normal lcd clear lcd putstr blinking pwm nduty 100 freq 10 lcd backlight rgb 255 255 0 lcd backlight blink 100 10 time sleep ms 2000 lcd backlight normal lcd clear lcd putstr blinking pwm nduty 10 freq 20 lcd backlight rgb 0 255 255 lcd backlight blink 10 20 time sleep ms 5000 lcd backlight normal lcd clear lcd putstr brightness 255 lcd backlight brightness 255 time sleep ms 1000 lcd clear lcd putstr brightness 127 lcd backlight brightness 127 time sleep ms 1000 lcd clear lcd putstr brightness 63 lcd backlight brightness 63 time sleep ms 1000 lcd clear lcd putstr brightness 1 lcd backlight brightness 1 time sleep ms 1000 lcd clear lcd putstr brightness 0 lcd backlight brightness 0 time sleep ms 1000 lcd clear lcd backlight normal lcd backlight brightness 255 lcd backlight rgb 255 0 0 lcd backlight sleep lcd putstr low power mode time sleep ms 2000 lcd backlight rgb 0 255 255 lcd putstr cant change bg time sleep ms 2000 lcd clear lcd putstr full power mode lcd backlight wake lcd putstr bg now changed time sleep ms 2000 lcd clear lcd putstr rainbow for in range 2 rainbow lcd 10 time sleep ms 1000 lcd clear lcd putstr random randomcolors lcd 30 100', 'test function for verifying basic functionality'), ('wheel', 143, 'def wheel(offset):\\n    offset = 255 - offset\\n    if offset < 85:\\n        return 255 - offset * 3, 0, offset * 3\\n    if offset < 170:\\n        offset -= 85\\n        return 0, offset * 3, 255 - offset * 3\\n    offset -= 170\\n    return offset * 3, 255 - offset * 3, 0\\n', 'wheel offset offset 255 offset if offset 85 return 255 offset 3 0 offset 3 if offset 170 offset 85 return 0 offset 3 255 offset 3 offset 170 return offset 3 255 offset 3 0', ''), ('hue', 155, 'def hue(lcd, n):\\n    r, g, b = wheel(n & 255)\\n    lcd.backlight_rgb(r, g, b)\\n', 'hue lcd wheel 255 lcd backlight rgb', ''), ('rainbow', 160, 'def rainbow(lcd, sleep):\\n    for n in range(256):\\n        hue(lcd, n)\\n        time.sleep_ms(sleep)\\n', 'rainbow lcd sleep for in range 256 hue lcd time sleep ms sleep', ''), ('randomColors', 166, 'def randomColors(lcd, count, sleep):\\n    for r in range(count):\\n        r, g, b = getrandbits(8), getrandbits(8), getrandbits(8)\\n        lcd.backlight_rgb(r, g, b)\\n        time.sleep_ms(sleep)\\n', 'randomcolors lcd count sleep for in range count getrandbits 8 getrandbits 8 getrandbits 8 lcd backlight rgb time sleep ms sleep', '')] [('transferDate', 10, \"@staticmethod\\ndef transferDate(dateStr):\\n    try:\\n        year = int(dateStr[0:4])\\n        month = int(dateStr[5:7])\\n        day = int(dateStr[8:10])\\n        hour = int(dateStr[11:-2].split(':')[0])\\n        minute = int(dateStr[11:-2].split(':')[1])\\n        return datetime(year, month, day, hour, minute)\\n    except Exception as ex:\\n        return None\\n\", 'transferdate datestr try year int datestr 0 4 month int datestr 5 7 day int datestr 8 10 hour int datestr 11 2 split 0 minute int datestr 11 2 split 1 return datetime year month day hour minute except exception as ex return none', ''), ('tuple2dict', 22, '@staticmethod\\ndef tuple2dict(vec):\\n    res = {}\\n    for pair in vec:\\n        res[pair[0]] = pair[1]\\n    return res\\n', 'vec res for pair in vec res pair 0 pair 1 return res', ''), ('dict2tuple', 29, '@staticmethod\\ndef dict2tuple(src):\\n    res = []\\n    for key, val in src.items():\\n        res.append((key, val))\\n    return res\\n', 'src res for key val in src items res append key val return res', '')] [('setUp', 23, \"def setUp(self):\\n    super(DeprecatedConfigTestCase, self).setUp()\\n    self.logbuffer = ''\\n\\n    def local_log(msg):\\n        self.logbuffer = msg\\n    self.stubs.Set(deprecated.LOG, 'warn', local_log)\\n\", 'setup self super deprecatedconfigtestcase self setup self logbuffer def local log msg self logbuffer msg self stubs set deprecated log warn local log', ''), ('test_deprecated', 32, \"def test_deprecated(self):\\n    deprecated.warn('test')\\n    self.assertEqual(self.logbuffer, 'Deprecated Config: test')\\n\", 'test deprecated self deprecated warn test self assertequal self logbuffer deprecated config test', ''), ('test_deprecated_fatal', 36, \"def test_deprecated_fatal(self):\\n    self.flags(fatal_deprecations=True)\\n    self.assertRaises(exception.DeprecatedConfig, deprecated.warn, 'test2')\\n    self.assertEqual(self.logbuffer, 'Deprecated Config: test2')\\n\", 'test deprecated fatal self self flags fatal deprecations true self assertraises exception deprecatedconfig deprecated warn self assertequal self logbuffer deprecated config', ''), ('test_deprecated_logs_only_once', 42, \"def test_deprecated_logs_only_once(self):\\n    deprecated.warn('only once!')\\n    deprecated.warn('only once!')\\n    deprecated.warn('only once!')\\n    self.assertEqual(self.logbuffer, 'Deprecated Config: only once!')\\n\", 'test deprecated logs only once self deprecated warn only once deprecated warn only once deprecated warn only once self assertequal self logbuffer deprecated config only once', '')] [('supportsAggregation', 24, 'def supportsAggregation(self, handlerId):\\n    return False\\n', 'supportsaggregation self handlerid return false', ''), ('canRenderChart', 27, \"def canRenderChart(self):\\n    valueFields = self.getValueFields()\\n    if len(valueFields) != 1:\\n        return False, 'Can only specify one Value Field'\\n    for keyField in self.getKeyFields():\\n        if not self.dataHandler.isNumericField(keyField):\\n            return False, 'Column {0} is not numerical'.format(keyField)\\n    return True, None\\n\", 'canrenderchart self valuefields self getvaluefields if len valuefields 1 return false can only specify one value field for keyfield in self getkeyfields if not self datahandler isnumericfield keyfield return false column 0 is not numerical format keyfield return true none', ''), ('getPreferredDefaultValueFieldCount', 39, 'def getPreferredDefaultValueFieldCount(self, handlerId):\\n    return 2\\n', 'getpreferreddefaultvaluefieldcount self handlerid return 2', ''), ('matplotlibRender', 42, \"def matplotlibRender(self, fig, ax):\\n    keyFields = self.getKeyFields()\\n    for i, keyField in enumerate(keyFields):\\n        self.getWorkingPandasDataFrame().plot(kind='scatter', x=keyField, y\\n            =self.getValueFields()[0], label=keyField, ax=ax, color=Colors[\\n            1.0 * i / len(keyFields)])\\n\", 'matplotlibrender self fig ax keyfields self getkeyfields for keyfield in enumerate keyfields self getworkingpandasdataframe plot kind scatter keyfield self getvaluefields 0 label keyfield ax ax color colors 1 0 len keyfields', '')] [] [('test_appliance_json', 21, \"def test_appliance_json():\\n    a = Appliance(None, {'node_type': 'qemu', 'name': 'Test',\\n        'default_name_format': '{name}-{0}', 'category': 0, 'symbol':\\n        'qemu.svg', 'server': 'local', 'platform': None})\\n    assert a.__json__() == {'appliance_id': a.id, 'node_type': 'qemu',\\n        'builtin': False, 'name': 'Test', 'default_name_format':\\n        '{name}-{0}', 'category': 'router', 'symbol': 'qemu.svg',\\n        'compute_id': 'local', 'platform': None}\\n\", 'test appliance json appliance none node type qemu name test default name format name 0 category 0 symbol qemu svg server local platform none assert json appliance id id node type qemu builtin false name test default name format name 0 category router symbol qemu svg compute id local platform none', ''), ('test_appliance_json_with_not_known_category', 44, \"def test_appliance_json_with_not_known_category():\\n    a = Appliance(None, {'node_type': 'qemu', 'name': 'Test',\\n        'default_name_format': '{name}-{0}', 'category': 'Not known',\\n        'symbol': 'qemu.svg', 'server': 'local', 'platform': None})\\n    assert a.__json__() == {'appliance_id': a.id, 'node_type': 'qemu',\\n        'builtin': False, 'name': 'Test', 'default_name_format':\\n        '{name}-{0}', 'category': 'Not known', 'symbol': 'qemu.svg',\\n        'compute_id': 'local', 'platform': None}\\n\", 'test appliance json with not known category appliance none node type qemu name test default name format name 0 category not known symbol qemu svg server local platform none assert json appliance id id node type qemu builtin false name test default name format name 0 category not known symbol qemu svg compute id local platform none', ''), ('test_appliance_json_with_platform', 67, \"def test_appliance_json_with_platform():\\n    a = Appliance(None, {'node_type': 'dynamips', 'name': 'Test',\\n        'default_name_format': '{name}-{0}', 'category': 0, 'symbol':\\n        'dynamips.svg', 'server': 'local', 'platform': 'c3725'})\\n    assert a.__json__() == {'appliance_id': a.id, 'node_type': 'dynamips',\\n        'builtin': False, 'name': 'Test', 'default_name_format':\\n        '{name}-{0}', 'category': 'router', 'symbol': 'dynamips.svg',\\n        'compute_id': 'local', 'platform': 'c3725'}\\n\", 'test appliance json with platform appliance none node type dynamips name test default name format name 0 category 0 symbol dynamips svg server local platform assert json appliance id id node type dynamips builtin false name test default name format name 0 category router symbol dynamips svg compute id local platform', ''), ('test_appliance_fix_linked_base', 90, 'def test_appliance_fix_linked_base():\\n    \"\"\"\\n    Version of the gui before 2.1 use linked_base and the server\\n    linked_clone\\n    \"\"\"\\n    a = Appliance(None, {\\'node_type\\': \\'qemu\\', \\'name\\': \\'Test\\',\\n        \\'default_name_format\\': \\'{name}-{0}\\', \\'category\\': 0, \\'symbol\\':\\n        \\'qemu.svg\\', \\'server\\': \\'local\\', \\'linked_base\\': True})\\n    assert a.data[\\'linked_clone\\']\\n    assert \\'linked_base\\' not in a.data\\n', 'test appliance fix linked base appliance none node type qemu name test default name format name 0 category 0 symbol qemu svg server local linked base true assert data linked clone assert linked base not in data', 'version of the gui before 2 1 use linked base and the server linked clone')] [('do_GET', 12, \"def do_GET(self):\\n    file = open('%s/websocket.html' % os.path.dirname(os.path.realpath(\\n        __file__)), 'rb')\\n    html = file.read()\\n    self.send_response(200)\\n    self.send_header('Content-type', 'text/html; charset=%s' % 'UTF-8')\\n    self.send_header('Content-Length', str(len(html)))\\n    self.end_headers()\\n    self.wfile.write(html)\\n\", 'do get self file open websocket html os path dirname os path realpath file rb html file read self send response 200 self send header content type text html charset utf 8 self send header content length str len html self end headers self wfile write html', ''), ('__init__', 27, \"def __init__(self, host='0.0.0.0', port=80):\\n    super(__class__, self).__init__()\\n    self.port = port\\n    self.httpd = HTTPServer((host, self.port), HttpHandler)\\n    self.daemon = True\\n\", 'init self host 0 0 0 0 port 80 super class self init self port port self httpd httpserver host self port httphandler self daemon true', ''), ('run', 33, 'def run(self):\\n    self.httpd.serve_forever()\\n', 'run self self httpd serve forever', ''), ('__init__', 41, \"def __init__(self, pid_params, host='0.0.0.0', http_port=80, websocket_port\\n    =9000):\\n    WebsocketServer.__init__(self, host=host, port=websocket_port)\\n    Thread.__init__(self)\\n    self.httpd = HttpdThread(host=host, port=http_port)\\n    self.received = {'joystick': [0, 0], 'start': 1, 'pid': pid_params}\\n    self.daemon = True\\n\", 'init self pid params host 0 0 0 0 http port 80 websocket port 9000 websocketserver init self host host port websocket port thread init self self httpd httpdthread host host port http port self received joystick 0 0 start 1 pid pid params self daemon true', ''), ('run', 52, 'def run(self):\\n    self.httpd.setDaemon(True)\\n    self.httpd.start()\\n    self.run_forever()\\n', 'run self self httpd setdaemon true self httpd start self run forever', ''), ('send_data', 57, 'def send_data(self, balance_angle, balance_gyro, turn_gyro, pwm_left, pwm_right\\n    ):\\n    \"\"\"\\n        发送数据\\n        :param balance_angle: 平衡倾角\\n        :param balance_gyro: 平衡角速度\\n        :param turn_gyro: 转向角速度\\n        :param pwm_left: 左侧PWM\\n        :param pwm_right: 右侧PWM\\n        \"\"\"\\n    if len(self.clients):\\n        data = {\\'sensor\\': [\\'%.2f\\' % balance_angle, \\'%.2f\\' % balance_gyro, \\n            \\'%.2f\\' % turn_gyro], \\'pwm\\': [\\'%d\\' % pwm_left, \\'%d\\' % pwm_right]}\\n        self.send_message_to_all(json.dumps(data))\\n', 'send data self balance angle balance gyro turn gyro pwm left pwm right if len self clients data sensor balance angle balance gyro turn gyro pwm pwm left pwm right self send message to all json dumps data', ''), ('send_custom_info', 73, 'def send_custom_info(self, info):\\n    \"\"\"\\n        发送自定义文本\\n        :param info: 要发送的文本\\n        \"\"\"\\n    self.send_message_to_all(json.dumps({\\'info\\': info}))\\n', 'send custom info self info self send message to all json dumps info info', ''), ('message_received', 82, 'def message_received(self, client, server, message):\\n    \"\"\"\\n        接收数据后解码json信息\\n        :param message: {\\n                  \\'joystick\\': [power, turn],\\n                  \\'start\\': 0|1\\n                  \\'pid\\': [\\'balance|velocity|turn\\', \\'k|p|i|d\\', value]\\n                }\\n        \"\"\"\\n    data = json.loads(message)\\n    if \\'joystick\\' in data:\\n        self.received[\\'joystick\\'][0] = data[\\'joystick\\'][0]\\n        self.received[\\'joystick\\'][1] = data[\\'joystick\\'][1]\\n    if \\'start\\' in data:\\n        self.received[\\'start\\'] = data[\\'start\\']\\n    if \\'pid\\' in data:\\n        self.received[\\'pid\\'][data[\\'pid\\'][0]][data[\\'pid\\'][1]] = data[\\'pid\\'][2]\\n', 'message received self client server message data json loads message if joystick in data self received joystick 0 data joystick 0 self received joystick 1 data joystick 1 if start in data self received start data start if pid in data self received pid data pid 0 data pid 1 data pid 2', 'json'), ('new_client', 100, 'def new_client(self, client, server):\\n    \"\"\"\\n        新客户端回调函数\\n        :param client:\\n        :param server:\\n        :return:\\n        \"\"\"\\n    self.send_message(client, json.dumps({\\'pid\\': self.received[\\'pid\\']}))\\n', 'new client self client server self send message client json dumps pid self received pid', '')] [('get_comment_count', 170, 'def get_comment_count(parser, token):\\n    \"\"\"\\n    Gets the comment count for the given params and populates the template\\n    context with a variable containing that value, whose name is defined by the\\n    \\'as\\' clause.\\n\\n    Syntax::\\n\\n        {% get_comment_count for [object] as [varname]  %}\\n        {% get_comment_count for [app].[model] [object_id] as [varname]  %}\\n\\n    Example usage::\\n\\n        {% get_comment_count for event as comment_count %}\\n        {% get_comment_count for calendar.event event.id as comment_count %}\\n        {% get_comment_count for calendar.event 17 as comment_count %}\\n\\n    \"\"\"\\n    return CommentCountNode.handle_token(parser, token)\\n', 'get comment count parser token return commentcountnode handle token parser token', 'gets the comment count for the given'), ('get_comment_list', 191, 'def get_comment_list(parser, token):\\n    \"\"\"\\n    Gets the list of comments for the given params and populates the template\\n    context with a variable containing that value, whose name is defined by the\\n    \\'as\\' clause.\\n\\n    Syntax::\\n\\n        {% get_comment_list for [object] as [varname]  %}\\n        {% get_comment_list for [app].[model] [object_id] as [varname]  %}\\n\\n    Example usage::\\n\\n        {% get_comment_list for event as comment_list %}\\n        {% for comment in comment_list %}\\n            ...\\n        {% endfor %}\\n\\n    \"\"\"\\n    return CommentListNode.handle_token(parser, token)\\n', 'get comment list parser token return commentlistnode handle token parser token', 'gets the list of comments for the given'), ('get_comment_form', 213, 'def get_comment_form(parser, token):\\n    \"\"\"\\n    Get a (new) form object to post a new comment.\\n\\n    Syntax::\\n\\n        {% get_comment_form for [object] as [varname] %}\\n        {% get_comment_form for [app].[model] [object_id] as [varname] %}\\n    \"\"\"\\n    return CommentFormNode.handle_token(parser, token)\\n', 'get comment form parser token return commentformnode handle token parser token', 'get a new form object to post a new comment'), ('render_comment_form', 225, 'def render_comment_form(parser, token):\\n    \"\"\"\\n    Render the comment form (as returned by ``{% render_comment_form %}``) through\\n    the ``comments/form.html`` template.\\n\\n    Syntax::\\n\\n        {% render_comment_form for [object] %}\\n        {% render_comment_form for [app].[model] [object_id] %}\\n    \"\"\"\\n    return RenderCommentFormNode.handle_token(parser, token)\\n', 'render comment form parser token return rendercommentformnode handle token parser token', 'render the comment form as returned by render comment form through the comments form html template'), ('comment_form_target', 238, 'def comment_form_target():\\n    \"\"\"\\n    Get the target URL for the comment form.\\n\\n    Example::\\n\\n        <form action=\"{% comment_form_target %}\" method=\"POST\">\\n    \"\"\"\\n    return comments.get_form_target()\\n', 'comment form target return comments get form target', 'get the target url for the comment form'), ('handle_token', 18, 'def handle_token(cls, parser, token):\\n    \"\"\"Class method to parse get_comment_list/count/form and return a Node.\"\"\"\\n    tokens = token.contents.split()\\n    if tokens[1] != \\'for\\':\\n        raise template.TemplateSyntaxError(\\n            \"Second argument in %r tag must be \\'for\\'\" % tokens[0])\\n    if len(tokens) == 5:\\n        if tokens[3] != \\'as\\':\\n            raise template.TemplateSyntaxError(\\n                \"Third argument in %r must be \\'as\\'\" % tokens[0])\\n        return cls(object_expr=parser.compile_filter(tokens[2]), as_varname\\n            =tokens[4])\\n    elif len(tokens) == 6:\\n        if tokens[4] != \\'as\\':\\n            raise template.TemplateSyntaxError(\\n                \"Fourth argument in %r must be \\'as\\'\" % tokens[0])\\n        return cls(ctype=BaseCommentNode.lookup_content_type(tokens[2],\\n            tokens[0]), object_pk_expr=parser.compile_filter(tokens[3]),\\n            as_varname=tokens[5])\\n    else:\\n        raise template.TemplateSyntaxError(\\n            \\'%r tag requires 4 or 5 arguments\\' % tokens[0])\\n', 'handle token cls parser token tokens token contents split if tokens 1 for raise template templatesyntaxerror second argument in tag must be for tokens 0 if len tokens 5 if tokens 3 as raise template templatesyntaxerror third argument in must be as tokens 0 return cls object expr parser compile filter tokens 2 as varname tokens 4 elif len tokens 6 if tokens 4 as raise template templatesyntaxerror fourth argument in must be as tokens 0 return cls ctype basecommentnode lookup content type tokens 2 tokens 0 object pk expr parser compile filter tokens 3 as varname tokens 5 else raise template templatesyntaxerror tag requires 4 or 5 arguments tokens 0', 'class method to parse get comment list count form and return a node'), ('lookup_content_type', 49, 'def lookup_content_type(token, tagname):\\n    try:\\n        app, model = token.split(\\'.\\')\\n        return ContentType.objects.get(app_label=app, model=model)\\n    except ValueError:\\n        raise template.TemplateSyntaxError(\\n            \"Third argument in %r must be in the format \\'app.model\\'\" % tagname)\\n    except ContentType.DoesNotExist:\\n        raise template.TemplateSyntaxError(\\n            \"%r tag has non-existant content-type: \\'%s.%s\\'\" % (tagname, app,\\n            model))\\n', 'lookup content type token tagname try app model token split return contenttype objects get app label app model model except valueerror raise template templatesyntaxerror third argument in must be in the format app model tagname except contenttype doesnotexist raise template templatesyntaxerror tag has non existant content type tagname app model', ''), ('__init__', 59, \"def __init__(self, ctype=None, object_pk_expr=None, object_expr=None,\\n    as_varname=None, comment=None):\\n    if ctype is None and object_expr is None:\\n        raise template.TemplateSyntaxError(\\n            'Comment nodes must be given either a literal object or a ctype and object pk.'\\n            )\\n    self.comment_model = comments.get_model()\\n    self.as_varname = as_varname\\n    self.ctype = ctype\\n    self.object_pk_expr = object_pk_expr\\n    self.object_expr = object_expr\\n    self.comment = comment\\n\", 'init self ctype none object pk expr none object expr none as varname none comment none if ctype is none and object expr is none raise template templatesyntaxerror comment nodes must be given either literal object or ctype and object pk self comment model comments get model self as varname as varname self ctype ctype self object pk expr object pk expr self object expr object expr self comment comment', ''), ('render', 69, \"def render(self, context):\\n    qs = self.get_query_set(context)\\n    context[self.as_varname] = self.get_context_value_from_queryset(context, qs\\n        )\\n    return ''\\n\", 'render self context qs self get query set context context self as varname self get context value from queryset context qs return', ''), ('get_query_set', 74, \"def get_query_set(self, context):\\n    ctype, object_pk = self.get_target_ctype_pk(context)\\n    if not object_pk:\\n        return self.comment_model.objects.none()\\n    qs = self.comment_model.objects.filter(content_type=ctype, object_pk=\\n        smart_unicode(object_pk), site__pk=settings.SITE_ID, is_public=True)\\n    if getattr(settings, 'COMMENTS_HIDE_REMOVED', True):\\n        qs = qs.filter(is_removed=False)\\n    return qs\\n\", 'get query set self context ctype object pk self get target ctype pk context if not object pk return self comment model objects none qs self comment model objects filter content type ctype object pk smart unicode object pk site pk settings site id is public true if getattr settings comments hide removed true qs qs filter is removed false return qs', ''), ('get_target_ctype_pk', 90, 'def get_target_ctype_pk(self, context):\\n    if self.object_expr:\\n        try:\\n            obj = self.object_expr.resolve(context)\\n        except template.VariableDoesNotExist:\\n            return None, None\\n        return ContentType.objects.get_for_model(obj), obj.pk\\n    else:\\n        return self.ctype, self.object_pk_expr.resolve(context,\\n            ignore_failures=True)\\n', 'get target ctype pk self context if self object expr try obj self object expr resolve context except template variabledoesnotexist return none none return contenttype objects get for model obj obj pk else return self ctype self object pk expr resolve context ignore failures true', ''), ('get_context_value_from_queryset', 100, 'def get_context_value_from_queryset(self, context, qs):\\n    \"\"\"Subclasses should override this.\"\"\"\\n    raise NotImplementedError\\n', 'get context value from queryset self context qs raise notimplementederror', 'subclasses should override this'), ('get_context_value_from_queryset', 106, 'def get_context_value_from_queryset(self, context, qs):\\n    return list(qs)\\n', 'get context value from queryset self context qs return list qs', ''), ('get_context_value_from_queryset', 111, 'def get_context_value_from_queryset(self, context, qs):\\n    return qs.count()\\n', 'get context value from queryset self context qs return qs count', ''), ('get_form', 117, 'def get_form(self, context):\\n    ctype, object_pk = self.get_target_ctype_pk(context)\\n    if object_pk:\\n        return comments.get_form()(ctype.get_object_for_this_type(pk=object_pk)\\n            )\\n    else:\\n        return None\\n', 'get form self context ctype object pk self get target ctype pk context if object pk return comments get form ctype get object for this type pk object pk else return none', ''), ('render', 124, \"def render(self, context):\\n    context[self.as_varname] = self.get_form(context)\\n    return ''\\n\", 'render self context context self as varname self get form context return', ''), ('handle_token', 132, 'def handle_token(cls, parser, token):\\n    \"\"\"Class method to parse render_comment_form and return a Node.\"\"\"\\n    tokens = token.contents.split()\\n    if tokens[1] != \\'for\\':\\n        raise template.TemplateSyntaxError(\\n            \"Second argument in %r tag must be \\'for\\'\" % tokens[0])\\n    if len(tokens) == 3:\\n        return cls(object_expr=parser.compile_filter(tokens[2]))\\n    elif len(tokens) == 4:\\n        return cls(ctype=BaseCommentNode.lookup_content_type(tokens[2],\\n            tokens[0]), object_pk_expr=parser.compile_filter(tokens[3]))\\n', 'handle token cls parser token tokens token contents split if tokens 1 for raise template templatesyntaxerror second argument in tag must be for tokens 0 if len tokens 3 return cls object expr parser compile filter tokens 2 elif len tokens 4 return cls ctype basecommentnode lookup content type tokens 2 tokens 0 object pk expr parser compile filter tokens 3', 'class method to parse render comment form and return a node'), ('render', 150, \"def render(self, context):\\n    ctype, object_pk = self.get_target_ctype_pk(context)\\n    if object_pk:\\n        template_search_list = ['comments/%s/%s/form.html' % (ctype.\\n            app_label, ctype.model), 'comments/%s/form.html' % ctype.\\n            app_label, 'comments/form.html']\\n        context.push()\\n        formstr = render_to_string(template_search_list, {'form': self.\\n            get_form(context)}, context)\\n        context.pop()\\n        return formstr\\n    else:\\n        return ''\\n\", 'render self context ctype object pk self get target ctype pk context if object pk template search list comments form html ctype app label ctype model comments form html ctype app label comments form html context push formstr render to string template search list form self get form context context context pop return formstr else return', '')] [('test_request_attributes', 14, 'def test_request_attributes(self):\\n    \"\"\"\\n        Test that the request object is available in the template and that its\\n        attributes can\\'t be overridden by GET and POST parameters (#3828).\\n        \"\"\"\\n    url = \\'/request_attrs/\\'\\n    response = self.client.get(url)\\n    self.assertContains(response, \\'Have request\\')\\n    response = self.client.get(url)\\n    self.assertContains(response, \\'Not secure\\')\\n    response = self.client.get(url, {\\'is_secure\\': \\'blah\\'})\\n    self.assertContains(response, \\'Not secure\\')\\n    response = self.client.post(url, {\\'is_secure\\': \\'blah\\'})\\n    self.assertContains(response, \\'Not secure\\')\\n    response = self.client.get(url)\\n    self.assertContains(response, url)\\n    response = self.client.get(url, {\\'path\\': \\'/blah/\\'})\\n    self.assertContains(response, url)\\n    response = self.client.post(url, {\\'path\\': \\'/blah/\\'})\\n    self.assertContains(response, url)\\n', 'test request attributes self url request attrs response self client get url self assertcontains response have request response self client get url self assertcontains response not secure response self client get url is secure blah self assertcontains response not secure response self client post url is secure blah self assertcontains response not secure response self client get url self assertcontains response url response self client get url path blah self assertcontains response url response self client post url path blah self assertcontains response url', 'test that the request object is available in the template and that its attributes can t be overridden by get and post')] [('get_panel_info', 17, 'def get_panel_info(panel_lines, panel_id=None, institute=None, version=None,\\n    date=None, display_name=None):\\n    \"\"\"Parse metadata for a gene panel\\n\\n    For historical reasons it is possible to include all information about a gene panel in the\\n    header of a panel file. This function parses the header.\\n\\n    Args:\\n        panel_lines(iterable(str))\\n\\n    Returns:\\n        panel_info(dict): Dictionary with panel information\\n    \"\"\"\\n    panel_info = {\\'panel_id\\': panel_id, \\'institute\\': institute, \\'version\\':\\n        version, \\'date\\': date, \\'display_name\\': display_name}\\n    for line in panel_lines:\\n        line = line.rstrip()\\n        if not line.startswith(\\'##\\'):\\n            break\\n        info = line[2:].split(\\'=\\')\\n        field = info[0]\\n        value = info[1]\\n        if not panel_info.get(field):\\n            panel_info[field] = value\\n    panel_info[\\'date\\'] = get_date(panel_info[\\'date\\'])\\n    return panel_info\\n', 'get panel info panel lines panel id none institute none version none date none display name none panel info panel id panel id institute institute version version date date display name display name for line in panel lines line line rstrip if not line startswith break info line 2 split field info 0 value info 1 if not panel info get field panel info field value panel info date get date panel info date return panel info', 'parse metadata for a gene panel'), ('parse_gene', 56, 'def parse_gene(gene_info):\\n    \"\"\"Parse a gene line with information from a panel file\\n\\n        Args:\\n            gene_info(dict): dictionary with gene info\\n\\n        Returns:\\n            gene(dict): A dictionary with the gene information\\n                {\\n                \\'hgnc_id\\': int,\\n                \\'hgnc_symbol\\': str,\\n                \\'disease_associated_transcripts\\': list(str),\\n                \\'inheritance_models\\': list(str),\\n                \\'mosaicism\\': bool,\\n                \\'reduced_penetrance\\': bool,\\n                \\'database_entry_version\\': str,\\n                }\\n\\n    \"\"\"\\n    gene = {}\\n    identifier = None\\n    hgnc_id = None\\n    try:\\n        if \\'hgnc_id\\' in gene_info:\\n            hgnc_id = int(gene_info[\\'hgnc_id\\'])\\n        elif \\'hgnc_idnumber\\' in gene_info:\\n            hgnc_id = int(gene_info[\\'hgnc_idnumber\\'])\\n        elif \\'hgncid\\' in gene_info:\\n            hgnc_id = int(gene_info[\\'hgncid\\'])\\n    except ValueError as e:\\n        raise SyntaxError(\\'Invalid hgnc id: {0}\\'.format(hgnc_id))\\n    gene[\\'hgnc_id\\'] = hgnc_id\\n    identifier = hgnc_id\\n    hgnc_symbol = None\\n    if \\'hgnc_symbol\\' in gene_info:\\n        hgnc_symbol = gene_info[\\'hgnc_symbol\\']\\n    elif \\'hgncsymbol\\' in gene_info:\\n        hgnc_symbol = gene_info[\\'hgncsymbol\\']\\n    elif \\'symbol\\' in gene_info:\\n        hgnc_symbol = gene_info[\\'symbol\\']\\n    gene[\\'hgnc_symbol\\'] = hgnc_symbol\\n    if not identifier:\\n        if hgnc_symbol:\\n            identifier = hgnc_symbol\\n        else:\\n            raise SyntaxError(\\'No gene identifier could be found\\')\\n    gene[\\'identifier\\'] = identifier\\n    transcripts = \\'\\'\\n    if \\'disease_associated_transcripts\\' in gene_info:\\n        transcripts = gene_info[\\'disease_associated_transcripts\\']\\n    elif \\'disease_associated_transcript\\' in gene_info:\\n        transcripts = gene_info[\\'disease_associated_transcript\\']\\n    elif \\'transcripts\\' in gene_info:\\n        transcripts = gene_info[\\'transcripts\\']\\n    gene[\\'transcripts\\'] = [transcript.strip() for transcript in transcripts\\n        .split(\\',\\') if transcript]\\n    models = \\'\\'\\n    if \\'genetic_disease_models\\' in gene_info:\\n        models = gene_info[\\'genetic_disease_models\\']\\n    elif \\'genetic_disease_model\\' in gene_info:\\n        models = gene_info[\\'genetic_disease_model\\']\\n    elif \\'inheritance_models\\' in gene_info:\\n        models = gene_info[\\'inheritance_models\\']\\n    elif \\'genetic_inheritance_models\\' in gene_info:\\n        models = gene_info[\\'genetic_inheritance_models\\']\\n    gene[\\'inheritance_models\\'] = [model.strip() for model in models.split(\\n        \\',\\') if model.strip() in VALID_MODELS]\\n    gene[\\'mosaicism\\'] = True if gene_info.get(\\'mosaicism\\') else False\\n    gene[\\'reduced_penetrance\\'] = True if gene_info.get(\\'reduced_penetrance\\'\\n        ) else False\\n    gene[\\'database_entry_version\\'] = gene_info.get(\\'database_entry_version\\')\\n    return gene\\n', 'parse gene gene info gene identifier none hgnc id none try if hgnc id in gene info hgnc id int gene info hgnc id elif hgnc idnumber in gene info hgnc id int gene info hgnc idnumber elif hgncid in gene info hgnc id int gene info hgncid except valueerror as raise syntaxerror invalid hgnc id 0 format hgnc id gene hgnc id hgnc id identifier hgnc id hgnc symbol none if hgnc symbol in gene info hgnc symbol gene info hgnc symbol elif hgncsymbol in gene info hgnc symbol gene info hgncsymbol elif symbol in gene info hgnc symbol gene info symbol gene hgnc symbol hgnc symbol if not identifier if hgnc symbol identifier hgnc symbol else raise syntaxerror no gene identifier could be found gene identifier identifier transcripts if disease associated transcripts in gene info transcripts gene info disease associated transcripts elif disease associated transcript in gene info transcripts gene info disease associated transcript elif transcripts in gene info transcripts gene info transcripts gene transcripts transcript strip for transcript in transcripts split if transcript models if genetic disease models in gene info models gene info genetic disease models elif genetic disease model in gene info models gene info genetic disease model elif inheritance models in gene info models gene info inheritance models elif genetic inheritance models in gene info models gene info genetic inheritance models gene inheritance models model strip for model in models split if model strip in valid models gene mosaicism true if gene info get mosaicism else false gene reduced penetrance true if gene info get reduced penetrance else false gene database entry version gene info get database entry version return gene', 'parse a gene line with information from a panel file'), ('parse_genes', 153, 'def parse_genes(gene_lines):\\n    \"\"\"Parse a file with genes and return the hgnc ids\\n\\n    Args:\\n        gene_lines(iterable(str)): Stream with genes\\n\\n    Returns:\\n        genes(list(dict)): Dictionaries with relevant gene info\\n    \"\"\"\\n    genes = []\\n    header = []\\n    hgnc_identifiers = set()\\n    delimiter = \\'\\\\t\\'\\n    delimiters = [\\'\\\\t\\', \\' \\', \\';\\']\\n    for i, line in enumerate(gene_lines):\\n        line = line.rstrip()\\n        if not len(line) > 0:\\n            continue\\n        if line.startswith(\\'#\\'):\\n            if not line.startswith(\\'##\\'):\\n                line_length = 0\\n                delimiter = None\\n                for alt in delimiters:\\n                    head_line = line.split(alt)\\n                    if len(head_line) > line_length:\\n                        line_length = len(head_line)\\n                        delimiter = alt\\n                header = [word.lower() for word in line[1:].split(delimiter)]\\n        else:\\n            if i == 0:\\n                line_length = 0\\n                for alt in delimiters:\\n                    head_line = line.split(alt)\\n                    if len(head_line) > line_length:\\n                        line_length = len(head_line)\\n                        delimiter = alt\\n                if \\'hgnc\\' in line or \\'HGNC\\' in line:\\n                    header = [word.lower() for word in line.split(delimiter)]\\n                    continue\\n                if line.split(delimiter)[0].isdigit():\\n                    header = [\\'hgnc_id\\']\\n                else:\\n                    header = [\\'hgnc_symbol\\']\\n            splitted_line = line.split(delimiter)\\n            gene_info = dict(zip(header, splitted_line))\\n            info_found = False\\n            for key in gene_info:\\n                if gene_info[key]:\\n                    info_found = True\\n                    break\\n            if not info_found:\\n                continue\\n            try:\\n                gene = parse_gene(gene_info)\\n            except Exception as e:\\n                LOG.warning(e)\\n                raise SyntaxError(\\'Line {0} is malformed\\'.format(i + 1))\\n            identifier = gene.pop(\\'identifier\\')\\n            if not identifier in hgnc_identifiers:\\n                hgnc_identifiers.add(identifier)\\n                genes.append(gene)\\n    return genes\\n', 'parse genes gene lines genes header hgnc identifiers set delimiter delimiters for line in enumerate gene lines line line rstrip if not len line 0 continue if line startswith if not line startswith line length 0 delimiter none for alt in delimiters head line line split alt if len head line line length line length len head line delimiter alt header word lower for word in line 1 split delimiter else if 0 line length 0 for alt in delimiters head line line split alt if len head line line length line length len head line delimiter alt if hgnc in line or hgnc in line header word lower for word in line split delimiter continue if line split delimiter 0 isdigit header hgnc id else header hgnc symbol splitted line line split delimiter gene info dict zip header splitted line info found false for key in gene info if gene info key info found true break if not info found continue try gene parse gene gene info except exception as log warning raise syntaxerror line 0 is malformed format 1 identifier gene pop identifier if not identifier in hgnc identifiers hgnc identifiers add identifier genes append gene return genes', 'parse a file with genes and return the hgnc ids'), ('parse_gene_panel', 238, 'def parse_gene_panel(path, institute, panel_id, panel_type=\\'clinical\\', date\\n    =datetime.now(), version=1.0, display_name=None, genes=None):\\n    \"\"\"Parse the panel info and return a gene panel\\n\\n        Args:\\n            path(str): Path to panel file\\n            institute(str): Name of institute that owns the panel\\n            panel_id(str): Panel id\\n            date(datetime.datetime): Date of creation\\n            version(float)\\n            full_name(str): Option to have a long name\\n\\n        Returns:\\n            gene_panel(dict)\\n    \"\"\"\\n    LOG.info(\\'Parsing gene panel %s\\', panel_id)\\n    gene_panel = {}\\n    gene_panel[\\'path\\'] = path\\n    gene_panel[\\'type\\'] = panel_type\\n    gene_panel[\\'date\\'] = date\\n    gene_panel[\\'panel_id\\'] = panel_id\\n    gene_panel[\\'institute\\'] = institute\\n    version = version or 1.0\\n    gene_panel[\\'version\\'] = float(version)\\n    gene_panel[\\'display_name\\'] = display_name or panel_id\\n    if not path:\\n        panel_handle = genes\\n    else:\\n        panel_handle = get_file_handle(gene_panel[\\'path\\'])\\n    gene_panel[\\'genes\\'] = parse_genes(gene_lines=panel_handle)\\n    return gene_panel\\n', 'parse gene panel path institute panel id panel type clinical date datetime now version 1 0 display name none genes none log info parsing gene panel panel id gene panel gene panel path path gene panel type panel type gene panel date date gene panel panel id panel id gene panel institute institute version version or 1 0 gene panel version float version gene panel display name display name or panel id if not path panel handle genes else panel handle get file handle gene panel path gene panel genes parse genes gene lines panel handle return gene panel', 'parse the panel info and return a gene panel'), ('get_omim_panel_genes', 273, 'def get_omim_panel_genes(genemap2_lines, mim2gene_lines, alias_genes):\\n    \"\"\"Return all genes that should be included in the OMIM-AUTO panel\\n    Return the hgnc symbols\\n    \\n    Genes that have at least one \\'established\\' or \\'provisional\\' phenotype connection\\n    are included in the gene panel\\n    \\n    Args:\\n        genemap2_lines(iterable)\\n        mim2gene_lines(iterable)\\n        alias_genes(dict): A dictionary that maps hgnc_symbol to hgnc_id\\n    \\n    Yields:\\n        hgnc_symbol(str)\\n    \"\"\"\\n    parsed_genes = get_mim_genes(genemap2_lines, mim2gene_lines)\\n    STATUS_TO_ADD = set([\\'established\\', \\'provisional\\'])\\n    for hgnc_symbol in parsed_genes:\\n        try:\\n            gene = parsed_genes[hgnc_symbol]\\n            keep = False\\n            for phenotype_info in gene.get(\\'phenotypes\\', []):\\n                if phenotype_info[\\'status\\'] in STATUS_TO_ADD:\\n                    keep = True\\n                    break\\n            if keep:\\n                hgnc_id_info = alias_genes.get(hgnc_symbol)\\n                if not hgnc_id_info:\\n                    for symbol in gene.get(\\'hgnc_symbols\\', []):\\n                        if symbol in alias_genes:\\n                            hgnc_id_info = alias_genes[symbol]\\n                            break\\n                if hgnc_id_info:\\n                    yield {\\'hgnc_id\\': hgnc_id_info[\\'true\\'], \\'hgnc_symbol\\':\\n                        hgnc_symbol}\\n                else:\\n                    LOG.warning(\\'Gene symbol %s does not exist\\', hgnc_symbol)\\n        except KeyError:\\n            pass\\n', 'get omim panel genes lines lines alias genes parsed genes get mim genes lines lines status to add set established provisional for hgnc symbol in parsed genes try gene parsed genes hgnc symbol keep false for phenotype info in gene get phenotypes if phenotype info status in status to add keep true break if keep hgnc id info alias genes get hgnc symbol if not hgnc id info for symbol in gene get hgnc symbols if symbol in alias genes hgnc id info alias genes symbol break if hgnc id info yield hgnc id hgnc id info true hgnc symbol hgnc symbol else log warning gene symbol does not exist hgnc symbol except keyerror pass', 'return all genes that should be included in the omim auto panel return the hgnc symbols')] [('MME_Standard', 15, 'def MME_Standard(self, dt):\\n    \"\"\"\\n    Déplacement basé sur une direction disponnible au hazard, si pas bon\\n    prendre la direction par défaut\\n    \"\"\"\\n    directions = self.OnAvlDir(self.x, self.y)\\n    hasMoved = False\\n    if directions == []:\\n        return None\\n    while hasMoved == False:\\n        a = random.choice(directions)\\n        hasMoved = self.move(a)\\n        if hasMoved == False:\\n            directions.remove(a)\\n            if self.lastDir in directions:\\n                directions.remove(self.lastDir)\\n                hasMoved = self.move(self.lastDir)\\n            elif len(directions) == 0:\\n                self.moveToInitialPos()\\n                hasMoved = True\\n', 'mme standard self dt directions self onavldir self self hasmoved false if directions return none while hasmoved false random choice directions hasmoved self move if hasmoved false directions remove if self lastdir in directions directions remove self lastdir hasmoved self move self lastdir elif len directions 0 self movetoinitialpos hasmoved true', 'd placement bas sur une direction disponnible au hazard si pas bon prendre la direction par d faut'), ('MME_Foward', 47, 'def MME_Foward(self, dt):\\n    \"\"\"\\n    Déplacement basé sur une direction disponnible au hazard, si pas bon\\n    prendre la direction par défaut\\n    \"\"\"\\n    directions = self.OnAvlDir(self.x, self.y)\\n    hasMoved = False\\n    if self.lastDir in directions:\\n        directions.remove(self.lastDir)\\n        hasMoved = self.move(self.lastDir)\\n    if directions == []:\\n        return None\\n    while hasMoved == False:\\n        a = random.choice(directions)\\n        hasMoved = self.move(a)\\n        if hasMoved == False:\\n            directions.remove(a)\\n            if self.lastDir in directions:\\n                directions.remove(self.lastDir)\\n                hasMoved = self.move(self.lastDir)\\n            elif len(directions) == 0:\\n                self.moveToInitialPos()\\n                hasMoved = True\\n', 'mme foward self dt directions self onavldir self self hasmoved false if self lastdir in directions directions remove self lastdir hasmoved self move self lastdir if directions return none while hasmoved false random choice directions hasmoved self move if hasmoved false directions remove if self lastdir in directions directions remove self lastdir hasmoved self move self lastdir elif len directions 0 self movetoinitialpos hasmoved true', 'd placement bas sur une direction disponnible au hazard si pas bon prendre la direction par d faut'), ('MME_Basic', 79, 'def MME_Basic(self, dt):\\n    \"\"\"\\n    Déplacement basé sur une direction disponnible au hazard\\n    \"\"\"\\n    directions = self.OnAvlDir(self.x, self.y)\\n    hasMoved = False\\n    if directions == []:\\n        return None\\n    while hasMoved == False:\\n        a = random.choice(directions)\\n        hasMoved = self.move(a)\\n        if hasMoved == False:\\n            directions.remove(a)\\n        if len(directions) == 0:\\n            self.moveToInitialPos()\\n            hasMoved = True\\n', 'mme basic self dt directions self onavldir self self hasmoved false if directions return none while hasmoved false random choice directions hasmoved self move if hasmoved false directions remove if len directions 0 self movetoinitialpos hasmoved true', 'd placement bas sur une direction disponnible au hazard')] [('validate', 5, \"def validate(job):\\n    student_files = ['sum.cpp']\\n    assert_dont_raises(job.run_build, compiler=compiler.GPP, inputs=\\n        student_files, output='sum1')\\n    assert_dont_raises(job.run_compiler, compiler=compiler.GPP, inputs=\\n        student_files, output='sum2')\\n    assert_dont_raises(job.run_make, mandatory=False)\\n    assert_dont_raises(job.run_make, mandatory=True)\\n    assert_dont_raises(job.run_configure, mandatory=False)\\n    assert_raises(job.run_configure, mandatory=True)\\n\", 'validate job student files sum cpp assert dont raises job run build compiler compiler gpp inputs student files output assert dont raises job run compiler compiler compiler gpp inputs student files output assert dont raises job run make mandatory false assert dont raises job run make mandatory true assert dont raises job run configure mandatory false assert raises job run configure mandatory true', '')] [] [('__init__', 33, \"def __init__(self, ordered_list):\\n    if not isinstance(ordered_list, (list, tuple, types.GeneratorType)):\\n        raise ValueError(\\n            'ExplicitOrder sorting key takes a list, tuple or Generator, which holdthe paths of each gallery subfolder'\\n            )\\n    self.ordered_list = list(os.path.normpath(path) for path in ordered_list)\\n\", 'init self ordered list if not isinstance ordered list list tuple types generatortype raise valueerror explicitorder sorting key takes list tuple or generator which holdthe paths of each gallery subfolder self ordered list list os path normpath path for path in ordered list', ''), ('__call__', 42, \"def __call__(self, item):\\n    if item in self.ordered_list:\\n        return self.ordered_list.index(item)\\n    else:\\n        raise ValueError(\\n            'If you use an explicit folder ordering, you must specify all folders. Explicit order not found for {}'\\n            .format(item))\\n\", 'call self item if item in self ordered list return self ordered list index item else raise valueerror if you use an explicit folder ordering you must specify all folders explicit order not found for format item', '')] [('compact', 39, 'def compact(number):\\n    \"\"\"Convert the number to the minimal representation. This strips the\\n    number of any valid separators and removes surrounding whitespace.\"\"\"\\n    number = clean(number, \\' -\\').upper().strip()\\n    if number.startswith(\\'SK\\'):\\n        number = number[2:]\\n    return number\\n', 'compact number number clean number upper strip if number startswith sk number number 2 return number', 'convert the number to the minimal representation this strips the number of any valid separators and removes surrounding whitespace'), ('checksum', 48, 'def checksum(number):\\n    \"\"\"Calculate the checksum.\"\"\"\\n    return int(number) % 11\\n', 'checksum number return int number 11', 'calculate the checksum'), ('validate', 53, 'def validate(number):\\n    \"\"\"Check if the number is a valid VAT number. This checks the length,\\n    formatting and check digit.\"\"\"\\n    number = compact(number)\\n    if not number.isdigit():\\n        raise InvalidFormat()\\n    if len(number) != 10:\\n        raise InvalidLength()\\n    if rc.is_valid(number):\\n        return number\\n    if number[0] == \\'0\\' or int(number[2]) not in (2, 3, 4, 7, 8, 9):\\n        raise InvalidFormat()\\n    if checksum(number) != 0:\\n        raise InvalidChecksum()\\n    return number\\n', 'validate number number compact number if not number isdigit raise invalidformat if len number 10 raise invalidlength if rc is valid number return number if number 0 0 or int number 2 not in 2 3 4 7 8 9 raise invalidformat if checksum number 0 raise invalidchecksum return number', 'check if the number is a valid vat number this checks the length formatting and check digit'), ('is_valid', 71, 'def is_valid(number):\\n    \"\"\"Check if the number is a valid VAT number.\"\"\"\\n    try:\\n        return bool(validate(number))\\n    except ValidationError:\\n        return False\\n', 'is valid number try return bool validate number except validationerror return false', 'check if the number is a valid vat number')] [('__init__', 45, \"def __init__(self, value, format=ENG, radix=DEC, vtype=LONG, units='',\\n    defCal=True):\\n    self._value = value\\n    self._vtype = vtype\\n    if type(value) == int:\\n        self._vtype = LONG\\n    elif type(value) == float:\\n        self._vtype = FLOAT\\n    elif type(value) == str:\\n        self._vtype = STRING\\n    self._format = format\\n    self._radix = radix\\n    self._units = units\\n    self._defCal = defCal\\n\", 'init self value format eng radix dec vtype long units defcal true self value value self vtype vtype if type value int self vtype long elif type value float self vtype float elif type value str self vtype string self format format self radix radix self units units self defcal defcal', ''), ('set', 60, 'def set(self, value):\\n    self._value = value\\n', 'set self value self value value', ''), ('get', 64, 'def get(self):\\n    return self._value\\n', 'get self return self value', ''), ('format', 68, 'def format(self, fmt=None):\\n    if fmt is None:\\n        return self._format\\n    else:\\n        self._format = fmt\\n', 'format self fmt none if fmt is none return self format else self format fmt', ''), ('vtype', 75, 'def vtype(self, vt=None):\\n    if vt is None:\\n        return self._vtype\\n    else:\\n        self._vtype = vt\\n', 'vtype self vt none if vt is none return self vtype else self vtype vt', ''), ('radix', 82, 'def radix(self, rd=None):\\n    if rd is None:\\n        return self._radix\\n    else:\\n        self._radix = rd\\n', 'radix self rd none if rd is none return self radix else self radix rd', ''), ('units', 89, 'def units(self, u=None):\\n    if u is None:\\n        return self._units\\n    else:\\n        self._units = u\\n', 'units self none if is none return self units else self units', ''), ('__repr__', 96, \"def __repr__(self):\\n    return ('[' + repr(self._value) + ',VType: ' + self._vtype +\\n        ',Format: ' + self._format + ', Radix: ' + self._radix +\\n        ', Units: ' + self._units + ']')\\n\", 'repr self return repr self value vtype self vtype format self format radix self radix units self units', ''), ('evaluate', 101, \"def evaluate(self, radix=DEC):\\n    cnv = {DEC: '', HEX: '0x', OCT: '0'}\\n    trns = {HEX: hex, OCT: oct}\\n    res = None\\n    try:\\n        if isinstance(self._value, str):\\n            if self._radix == BIN:\\n                res = 0\\n                for c in self._value:\\n                    res = res * 2 + eval(c)\\n            elif self._radix in cnv:\\n                res = eval(cnv[self._radix] + self._value)\\n        elif isinstance(self._value, long) or isinstance(self._value, int\\n            ) or isinstance(self._value, float):\\n            res = self._value\\n    except:\\n        res = None\\n    if res is None:\\n        return None\\n    if radix in trns:\\n        res = trns[radix](res)\\n    elif radix == BIN:\\n        v = ''\\n        while res > 0:\\n            if res % 2 == 1:\\n                v = '1' + v\\n            if res % 2 == 0:\\n                v = '0' + v\\n            res >>= 1\\n        res = '0b' + v\\n    return res\\n\", 'evaluate self radix dec cnv dec hex oct 0 trns hex hex oct oct res none try if isinstance self value str if self radix bin res 0 for in self value res res 2 eval elif self radix in cnv res eval cnv self radix self value elif isinstance self value long or isinstance self value int or isinstance self value float res self value except res none if res is none return none if radix in trns res trns radix res elif radix bin while res 0 if res 2 1 1 if res 2 0 0 res 1 res return res', '')] [('test_get_returns_value_when_value_is_set', 7, 'def test_get_returns_value_when_value_is_set():\\n    assert Item(value=5).get() == 5\\n    assert Item(value=5).get(10) == 5\\n    assert Item(value=None).get(20) is None\\n', 'test get returns value when value is set assert item value 5 get 5 assert item value 5 get 10 5 assert item value none get 20 is none', ''), ('test_get_with_no_value_and_no_default_returns_not_set', 13, 'def test_get_with_no_value_and_no_default_returns_not_set():\\n    assert Item().get() is not_set\\n', 'test get with no value and no default returns not set assert item get is not set', ''), ('test_get_returns_fallback_when_no_value_and_no_default_is_set', 17, 'def test_get_returns_fallback_when_no_value_and_no_default_is_set():\\n    assert Item().get(None) is None\\n    assert Item(type=int).get(30) == 30\\n', 'test get returns fallback when no value and no default is set assert item get none is none assert item type int get 30 30', ''), ('test_get_required_with_no_value_and_no_default_returns_fallback_if_available', 22, \"def test_get_required_with_no_value_and_no_default_returns_fallback_if_available(\\n    ):\\n    assert Item(required=True).get('a') == 'a'\\n    with pytest.raises(RequiredValueMissing):\\n        Item(required=True).get()\\n\", 'test get required with no value and no default returns fallback if available assert item required true get with pytest raises requiredvaluemissing item required true get', ''), ('test_get_returns_default_value_when_available', 29, \"def test_get_returns_default_value_when_available():\\n    assert Item(default='a').get() == 'a'\\n    assert Item(default='b').get('c') == 'b'\\n    assert Item(default=None).get('d') is None\\n\", 'test get returns default value when available assert item default get assert item default get assert item default none get is none', ''), ('test_get_returns_value_when_value_and_default_available', 35, \"def test_get_returns_value_when_value_and_default_available():\\n    assert Item(default='a', value=None).get() is None\\n    assert Item(default='a', value='b').get() == 'b'\\n    assert Item(default=None, value=None).get(True) is None\\n    assert Item(default='a', value='b').get('c') == 'b'\\n\", 'test get returns value when value and default available assert item default value none get is none assert item default value get assert item default none value none get true is none assert item default value get', ''), ('test_value_calls_get_so_users_can_extend_item_class_by_overriding_just_get', 42, \"def test_value_calls_get_so_users_can_extend_item_class_by_overriding_just_get(\\n    ):\\n\\n\\n    class CustomItem(Item):\\n\\n        def get(self, fallback=not_set):\\n            return 55\\n    item = CustomItem(name='a', value=30, default=0)\\n    assert item.value == 55\\n    item.reset()\\n    assert item.value == 55\\n\", 'test value calls get so users can extend item class by overriding just get class customitem item def get self fallback not set return 55 item customitem name value 30 default 0 assert item value 55 item reset assert item value 55', ''), ('test_set_sets_value', 54, 'def test_set_sets_value():\\n    a = Item(type=int)\\n    assert not a.has_value\\n    a.set(5)\\n    assert a.has_value\\n    assert a.value == 5\\n    b = Item(default=55)\\n    b.set(5)\\n    assert b.value == 5\\n', 'test set sets value item type int assert not has value set 5 assert has value assert value 5 item default 55 set 5 assert value 5', ''), ('test_value_setting_calls_set_so_users_can_extend_item_class_by_overriding_just_set', 67, 'def test_value_setting_calls_set_so_users_can_extend_item_class_by_overriding_just_set(\\n    ):\\n\\n\\n    class CustomItem(Item):\\n\\n        def set(self, value):\\n            super(CustomItem, self).set(value * 2)\\n    item = CustomItem(value=0)\\n    item.value = 3\\n    assert item.value == 6\\n', 'test value setting calls set so users can extend item class by overriding just set class customitem item def set self value super customitem self set value 2 item customitem value 0 item value 3 assert item value 6', '')] [('__init__', 15, \"def __init__(self, log, ctable):\\n    wxPyGridTableBase.__init__(self)\\n    self.log = log\\n    self.casatab = ctable\\n    self.odd = wxGridCellAttr()\\n    self.odd.SetBackgroundColour('gray90')\\n    self.even = wxGridCellAttr()\\n    self.even.SetBackgroundColour('white')\\n\", 'init self log ctable wxpygridtablebase init self self log log self casatab ctable self odd wxgridcellattr self odd setbackgroundcolour self even wxgridcellattr self even setbackgroundcolour white', ''), ('GetAttr', 24, 'def GetAttr(self, row, col, kind):\\n    attr = [self.even, self.odd][row % 2]\\n    attr.IncRef()\\n    return attr\\n', 'getattr self row col kind attr self even self odd row 2 attr incref return attr', ''), ('GetColLabelValue', 29, 'def GetColLabelValue(self, col):\\n    colnames = self.casatab.colnames()\\n    return colnames[col]\\n', 'getcollabelvalue self col colnames self casatab colnames return colnames col', ''), ('GetNumberRows', 33, 'def GetNumberRows(self):\\n    return self.casatab.nrows()\\n', 'getnumberrows self return self casatab nrows', ''), ('GetNumberCols', 36, 'def GetNumberCols(self):\\n    return self.casatab.ncols()\\n', 'getnumbercols self return self casatab ncols', ''), ('IsEmptyCell', 39, 'def IsEmptyCell(self, row, col):\\n    return False\\n', 'isemptycell self row col return false', ''), ('GetValue', 42, \"def GetValue(self, row, col):\\n    coln = self.casatab.colnames()\\n    cell = 'array'\\n    return str(self.casatab.getcell(coln[col], row))\\n\", 'getvalue self row col coln self casatab colnames cell array return str self casatab getcell coln col row', ''), ('SetValue', 59, 'def SetValue(self, row, col, value):\\n    self.log.write(\\'SetValue(%d, %d, \"%s\") ignored.\\\\n\\' % (row, col, value))\\n', 'setvalue self row col value self log write setvalue ignored row col value', ''), ('__init__', 66, 'def __init__(self, parent, log, ctable):\\n    wxGrid.__init__(self, parent, -1)\\n    table = wxCasaTable(log, ctable)\\n    self.SetTable(table, True)\\n    EVT_GRID_CELL_RIGHT_CLICK(self, self.OnRightDown)\\n', 'init self parent log ctable wxgrid init self parent 1 table wxcasatable log ctable self settable table true evt grid cell right click self self onrightdown', ''), ('OnRightDown', 75, 'def OnRightDown(self, event):\\n    six.print_(self.GetSelectedRows())\\n', 'onrightdown self event six print self getselectedrows', ''), ('__init__', 82, \"def __init__(self, parent, log, ctable):\\n    wxFrame.__init__(self, parent, -1, 'Casa Table Browser', size=(640, 480))\\n    grid = wxCasaTableGrid(self, log, ctable)\\n    grid.EnableEditing(False)\\n\", 'init self parent log ctable wxframe init self parent 1 casa table browser size 640 480 grid wxcasatablegrid self log ctable grid enableediting false', '')] [('deprecate_init', 25, 'def deprecate_init(Klass):\\n\\n\\n    class NewKlass(Klass):\\n\\n        def __init__(self, *args, **kwargs):\\n            import warnings\\n            warnings.simplefilter(\\'once\\')\\n            warnings.warn(\\n                \"\\'slugify.get_slugify\\' is deprecated; use \\'slugify.Slugify\\' instead.\"\\n                , DeprecationWarning, stacklevel=2)\\n            super(NewKlass, self).__init__(*args, **kwargs)\\n    return NewKlass\\n', 'deprecate init klass class newklass klass def init self args kwargs import warnings warnings simplefilter once warnings warn slugify get slugify is deprecated use slugify slugify instead deprecationwarning stacklevel 2 super newklass self init args kwargs return newklass', '')] [('_run_exitfuncs', 35, 'def _run_exitfuncs():\\n    \"\"\"run any registered exit functions\\n\\n    _exithandlers is traversed in reverse order so functions are executed\\n    last in, first out.\\n    \"\"\"\\n    while _exithandlers:\\n        func, targs, kargs = _exithandlers.pop()\\n        apply(func, targs, kargs)\\n', 'run exitfuncs while exithandlers func targs kargs exithandlers pop apply func targs kargs', 'run any registered exit functions'), ('register', 46, 'def register(func, *targs, **kargs):\\n    \"\"\"register a function to be executed upon normal program termination\\n\\n    func - function to be called at exit\\n    targs - optional arguments to pass to func\\n    kargs - optional keyword arguments to pass to func\\n    \"\"\"\\n    _exithandlers.append((func, targs, kargs))\\n', 'register func targs kargs exithandlers append func targs kargs', 'register a function to be executed upon normal program termination')] [('get_floating_ip', 14, \"def get_floating_ip(vm):\\n    addrs = vm.addresses\\n    for net_name, ifaces in addrs.items():\\n        for iface in ifaces:\\n            if iface.get('OS-EXT-IPS:type') == 'floating':\\n                return iface['addr']\\n\", 'get floating ip vm addrs vm addresses for net name ifaces in addrs items for iface in ifaces if iface get os ext ips type floating return iface addr', ''), ('discover_vms', 22, \"def discover_vms(client, search_opts):\\n    user, password, key = parse_creds(search_opts.pop('auth'))\\n    servers = client.servers.list(search_opts=search_opts)\\n    logger.debug('Found %s openstack vms' % len(servers))\\n    return [Node(get_floating_ip(server), ['test_vm'], username=user,\\n        password=password, key_path=key) for server in servers if\\n        get_floating_ip(server)]\\n\", 'discover vms client search opts user password key parse creds search opts pop auth servers client servers list search opts search opts logger debug found openstack vms len servers return node get floating ip server test vm username user password password key path key for server in servers if get floating ip server', ''), ('discover_services', 32, \"def discover_services(client, opts):\\n    user, password, key = parse_creds(opts.pop('auth'))\\n    services = []\\n    if opts['service'] == 'all':\\n        services = client.services.list()\\n    else:\\n        if isinstance(opts['service'], basestring):\\n            opts['service'] = [opts['service']]\\n        for s in opts['service']:\\n            services.extend(client.services.list(binary=s))\\n    host_services_mapping = {}\\n    for service in services:\\n        ip = socket.gethostbyname(service.host)\\n        host_services_mapping[ip].append(service.binary)\\n    logger.debug('Found %s openstack service nodes' % len(\\n        host_services_mapping))\\n    return [Node(host, services, username=user, password=password, key_path\\n        =key) for host, services in host_services_mapping.items()]\\n\", 'discover services client opts user password key parse creds opts pop auth services if opts service all services client services list else if isinstance opts service basestring opts service opts service for in opts service services extend client services list binary host services mapping for service in services ip socket gethostbyname service host host services mapping ip append service binary logger debug found openstack service nodes len host services mapping return node host services username user password password key path key for host services in host services mapping items', ''), ('discover_openstack_nodes', 58, 'def discover_openstack_nodes(conn_details, conf):\\n    \"\"\"Discover vms running in openstack\\n    :param conn_details - dict with openstack connection details -\\n        auth_url, api_key (password), username\\n    \"\"\"\\n    client = Client(version=\\'1.1\\', **conn_details)\\n    nodes = []\\n    if conf.get(\\'discover\\'):\\n        services_to_discover = conf[\\'discover\\'].get(\\'nodes\\')\\n        if services_to_discover:\\n            nodes.extend(discover_services(client, services_to_discover))\\n    return nodes\\n', 'discover openstack nodes conn details conf client client version 1 1 conn details nodes if conf get discover services to discover conf discover get nodes if services to discover nodes extend discover services client services to discover return nodes', 'discover vms running in openstack')] [] [('switch', 14, 'def switch(n, m):\\n    \"\"\"\\n    Generates a generic switch symbol for an nPsT sort of switch.\\n    Probably won\\'t generate a useful pin numbering when T>2.\\n    \"\"\"\\n    out = []\\n    name_letters = {(1): \\'S\\', (2): \\'D\\'}\\n    name_n = name_letters[n] if n in name_letters else str(n)\\n    name_m = name_letters[m] if m in name_letters else str(m)\\n    n_pins_right = n * m + n - 1\\n    if n_pins_right % 2 == 0:\\n        n_pins_right += 1\\n    height = 100 * (n_pins_right - 1)\\n    hheight = height // 2\\n    refheight = hheight + 100\\n    if m == 1:\\n        refheight += 50\\n    valheight = -(hheight + 100)\\n    if n % 2 == 1 and m % 2 == 0:\\n        valheight += 100\\n    name = \\'SWITCH_{}P{}T\\'.format(name_n, name_m)\\n    out.append(\\'#\\\\n# {}\\\\n#\\'.format(name))\\n    out.append(\\'DEF {} SW 0 1 Y N 1 F N\\'.format(name))\\n    out.append(\\'F0 \"SW\" 0 {} 50 H V C CNN\\'.format(refheight))\\n    out.append(\\'F1 \"{}\" 0 {} 50 H V C CNN\\'.format(name, valheight))\\n    out.append(\\'F2 \"\" 0 0 50 H I C CNN\\')\\n    out.append(\\'F3 \"\" 0 0 50 H I C CNN\\')\\n    out.append(\\'DRAW\\')\\n    pole_top = hheight\\n    for pole in range(n):\\n        pole_num = pole * (m + 1) + 2\\n        pole_y = pole_top - 100 * (m - 1) // 2\\n        if m % 2 == 0:\\n            pole_y -= 50\\n        out.append(\\'X \"~\" {} -100 {} 40 R 50 50 1 1 P\\'.format(pole_num, pole_y)\\n            )\\n        out.append(\\'C -50 {} 10 1 1 0 N\\'.format(pole_y))\\n        out.append(\\'P 2 1 1 0 -50 {} 50 {} N\\'.format(pole_y + 10, pole_y + 90))\\n        for throw in range(m):\\n            throw_num = pole_num + throw - 1\\n            throw_y = pole_top - 100 * throw\\n            if throw > 0:\\n                throw_num += 1\\n            out.append(\\'X \"~\" {} 100 {} 40 L 50 50 1 1 P\\'.format(throw_num,\\n                throw_y))\\n            out.append(\\'C 50 {} 10 1 1 0 N\\'.format(throw_y))\\n        pole_top -= 100 * (m + 1)\\n    if n > 1:\\n        pole_y = hheight - 100 * (m - 1) // 2 + 50\\n        if m % 2 == 0:\\n            pole_y -= 50\\n        for _ in range(5 * (m + 1) * (n - 1)):\\n            out.append(\\'P 2 1 1 0 0 {} 0 {} N\\'.format(pole_y, pole_y - 5))\\n            pole_y -= 20\\n    out.append(\\'ENDDRAW\\\\nENDDEF\\\\n\\')\\n    return out\\n', 'switch out name letters 1 2 name name letters if in name letters else str name name letters if in name letters else str pins right 1 if pins right 2 0 pins right 1 height 100 pins right 1 hheight height 2 refheight hheight 100 if 1 refheight 50 valheight hheight 100 if 2 1 and 2 0 valheight 100 name switch format name name out append format name out append def sw 0 1 1 format name out append sw 0 50 cnn format refheight out append 0 50 cnn format name valheight out append 0 0 50 cnn out append 0 0 50 cnn out append draw pole top hheight for pole in range pole num pole 1 2 pole pole top 100 1 2 if 2 0 pole 50 out append 100 40 50 50 1 1 format pole num pole out append 50 10 1 1 0 format pole out append 2 1 1 0 50 50 format pole 10 pole 90 for throw in range throw num pole num throw 1 throw pole top 100 throw if throw 0 throw num 1 out append 100 40 50 50 1 1 format throw num throw out append 50 10 1 1 0 format throw pole top 100 1 if 1 pole hheight 100 1 2 50 if 2 0 pole 50 for in range 5 1 1 out append 2 1 1 0 0 0 format pole pole 5 pole 20 out append enddraw nenddef return out', 'generates a generic switch symbol for an npst sort of switch probably won t generate a useful pin numbering when t 2'), ('main', 100, \"def main(libpath, verify=False):\\n    out = []\\n    out.append('EESchema-LIBRARY Version 2.3')\\n    out.append('#encoding utf-8\\\\n')\\n    out.append('#============================================================')\\n    out.append('# Automatically generated by agg-kicad build_lib_switch.py')\\n    out.append('# See github.com/adamgreig/agg-kicad')\\n    out.append('#============================================================')\\n    out.append('')\\n    for n in (1, 2, 3):\\n        for m in (1, 2, 3):\\n            out += switch(n, m)\\n    out.append('# End Library\\\\n')\\n    lib = '\\\\n'.join(out)\\n    if os.path.isfile(libpath):\\n        with open(libpath) as f:\\n            oldlib = f.read()\\n            if lib == oldlib:\\n                return True\\n    if verify:\\n        return False\\n    else:\\n        with open(libpath, 'w') as f:\\n            f.write(lib)\\n\", 'main libpath verify false out out append eeschema library version 2 3 out append encoding utf 8 out append out append automatically generated by agg kicad build lib switch py out append see github com adamgreig agg kicad out append out append for in 1 2 3 for in 1 2 3 out switch out append end library lib join out if os path isfile libpath with open libpath as oldlib read if lib oldlib return true if verify return false else with open libpath as write lib', '')] [('__init__', 48, 'def __init__(self, result, errors=None):\\n    \"\"\"Constructor.\"\"\"\\n    self.result = result\\n    self.errors = errors if errors else {}\\n', 'init self result errors none self result result self errors errors if errors else', 'constructor'), ('__str__', 53, 'def __str__(self):\\n    return str(self.result)\\n', 'str self return str self result', ''), ('__bool__', 57, 'def __bool__(self):\\n    return self.result\\n', 'bool self return self result', ''), ('message', 61, '@property\\ndef message(self):\\n    \"\"\"Human readable message of all errors.\\n\\n        :return string:\\n        \"\"\"\\n    return \\' \\'.join(map(text_type, self.errors))\\n', 'message self return join map text type self errors', 'human readable message of all errors'), ('reason', 69, '@property\\ndef reason(self):\\n    \"\"\"Reason.\\n\\n        For backwards compatibility. Returns list of text messages.\\n\\n        :return list:\\n        \"\"\"\\n    return map(text_type, self.errors)\\n', 'reason self return map text type self errors', 'reason'), ('__init__', 93, 'def __init__(self, signature, auth_user, valid_until, extra=None):\\n    \"\"\"Constructor.\"\"\"\\n    self.signature = signature\\n    self.auth_user = auth_user\\n    self.valid_until = valid_until\\n    self.extra = extra if extra else {}\\n', 'init self signature auth user valid until extra none self signature signature self auth user auth user self valid until valid until self extra extra if extra else', 'constructor'), ('__str__', 100, 'def __str__(self):\\n    return self.signature\\n', 'str self return self signature', ''), ('__bool__', 104, 'def __bool__(self):\\n    return not self.is_expired()\\n', 'bool self return not self is expired', ''), ('validate_signature', 108, '@classmethod\\ndef validate_signature(cls, signature, auth_user, secret_key, valid_until,\\n    extra=None, return_object=False):\\n    \"\"\"Validates the signature.\\n\\n        :param str signature:\\n        :param str auth_user:\\n        :param str secret_key:\\n        :param float|str valid_until: Unix timestamp.\\n        :param dict extra: Extra arguments to be validated.\\n        :param bool return_object: If set to True, an instance of\\n            ``SignatureValidationResult`` is returned.\\n        :return bool:\\n\\n        :example:\\n        >>> Signature.validate_signature(\\n        >>>     \\'EBS6ipiqRLa6TY5vxIvZU30FpnM=\\',\\n        >>>     \\'user\\',\\n        >>>     \\'your-secret-key\\',\\n        >>>     \\'1377997396.0\\'\\n        >>> )\\n        False\\n        \"\"\"\\n    if isinstance(signature, str):\\n        signature = signature.encode()\\n    if not extra:\\n        extra = {}\\n    sig = cls.generate_signature(auth_user=auth_user, secret_key=secret_key,\\n        valid_until=valid_until, extra=extra)\\n    if not return_object:\\n        return sig.signature == signature and not sig.is_expired()\\n    else:\\n        result = sig.signature == signature and not sig.is_expired()\\n        errors = []\\n        if sig.signature != signature:\\n            errors.append(error_codes.INVALID_SIGNATURE)\\n        if sig.is_expired():\\n            errors.append(error_codes.SIGNATURE_TIMESTAMP_EXPIRED)\\n        return SignatureValidationResult(result, errors)\\n', 'validate signature cls signature auth user secret key valid until extra none return object false if isinstance signature str signature signature encode if not extra extra sig cls generate signature auth user auth user secret key secret key valid until valid until extra extra if not return object return sig signature signature and not sig is expired else result sig signature signature and not sig is expired errors if sig signature signature errors append error codes invalid signature if sig is expired errors append error codes signature timestamp expired return signaturevalidationresult result errors', 'validates the signature'), ('is_expired', 158, 'def is_expired(self):\\n    \"\"\"Checks if current signature is expired.\\n\\n        Returns True if signature is expired and False otherwise.\\n\\n        :return bool:\\n\\n        :example:\\n        >>> # Generating the signature\\n        >>> sig = Signature.generate_signature(\\'user\\', \\'your-secret-key\\')\\n        >>> sig.is_expired()\\n        False\\n        \"\"\"\\n    now = datetime.datetime.now()\\n    valid_util = self.__class__.unix_timestamp_to_date(self.valid_until)\\n    res = valid_util > now\\n    return not res\\n', 'is expired self now datetime datetime now valid util self class unix timestamp to date self valid until res valid util now return not res', 'checks if current signature is expired'), ('get_base', 180, '@classmethod\\ndef get_base(cls, auth_user, timestamp, extra=None):\\n    \"\"\"Get base string.\\n\\n        Add something here so that timestamp to signature conversion is not\\n        that obvious.\\n\\n        :param string auth_user:\\n        :param int timestamp:\\n        :param dict extra:\\n        \"\"\"\\n    if not extra:\\n        extra = {}\\n    _base = [str(timestamp), auth_user]\\n    if extra:\\n        urlencoded_extra = sorted_urlencode(extra)\\n        if urlencoded_extra:\\n            _base.append(urlencoded_extra)\\n    return \\'_\\'.join(_base).encode()\\n', 'get base cls auth user timestamp extra none if not extra extra base str timestamp auth user if extra urlencoded extra sorted urlencode extra if urlencoded extra base append urlencoded extra return join base encode', 'get base string'), ('make_secret_key', 203, '@staticmethod\\ndef make_secret_key(secret_key):\\n    \"\"\"The secret key how its\\' supposed to be used in generate signature.\\n\\n        :param str secret_key:\\n        :return str:\\n        \"\"\"\\n    return secret_key.encode()\\n', 'make secret key secret key return secret key encode', 'the secret key how its supposed to be used in generate signature'), ('make_hash', 212, '@classmethod\\ndef make_hash(cls, auth_user, secret_key, valid_until=None, extra=None):\\n    \"\"\"Make hash.\\n\\n        You should implement this method in your signature class.\\n\\n        :param str auth_user:\\n        :param str secret_key:\\n        :param float|str valid_until: Unix timestamp, valid until.\\n        :param dict extra: Additional variables to be added.\\n        :return str:\\n        \"\"\"\\n    raise NotImplementedError(\\'You should implement this method!\\')\\n', 'make hash cls auth user secret key valid until none extra none raise notimplementederror you should implement this method', 'make hash'), ('generate_signature', 226, '@classmethod\\ndef generate_signature(cls, auth_user, secret_key, valid_until=None,\\n    lifetime=SIGNATURE_LIFETIME, extra=None):\\n    \"\"\"Generates the signature.\\n\\n        If timestamp is given, the signature is created using the given\\n        timestamp. Otherwise current time is used.\\n\\n        :param str auth_user:\\n        :param str secret_key:\\n        :param float|str valid_until: Unix timestamp, valid until.\\n        :param int lifetime: Lifetime of the signature in seconds.\\n        :param dict extra: Additional variables to be added.\\n        :return str:\\n\\n        :example:\\n        >>> sig = Signature.generate_signature(\\'user\\', \\'your-secret-key\\')\\n        EBS6ipiqRLa6TY5vxIvZU30FpnM=\\n        \"\"\"\\n    if not extra:\\n        extra = {}\\n    if not valid_until:\\n        valid_until = time.mktime((datetime.datetime.now() + datetime.\\n            timedelta(seconds=lifetime)).timetuple())\\n    else:\\n        try:\\n            cls.unix_timestamp_to_date(valid_until)\\n        except Exception:\\n            return None\\n    signature = b64encode(cls.make_hash(auth_user, secret_key, valid_until,\\n        extra))\\n    return cls(signature=signature, auth_user=auth_user, valid_until=\\n        valid_until, extra=extra)\\n', 'generate signature cls auth user secret key valid until none lifetime signature lifetime extra none if not extra extra if not valid until valid until time mktime datetime datetime now datetime timedelta seconds lifetime timetuple else try cls unix timestamp to date valid until except exception return none signature cls make hash auth user secret key valid until extra return cls signature signature auth user auth user valid until valid until extra extra', 'generates the signature'), ('datetime_to_timestamp', 268, '@staticmethod\\ndef datetime_to_timestamp(dtv):\\n    \"\"\"Human readable datetime according to the format specified.\\n\\n         Format is specified in ``TIMESTAMP_FORMAT``.\\n\\n        :param datetime.datetime dtv:\\n        :return str:\\n        \"\"\"\\n    try:\\n        return dtv.strftime(TIMESTAMP_FORMAT)\\n    except Exception:\\n        pass\\n', 'datetime to timestamp dtv try return dtv strftime timestamp format except exception pass', 'human readable datetime according to the format specified'), ('datetime_to_unix_timestamp', 282, '@staticmethod\\ndef datetime_to_unix_timestamp(dtv):\\n    \"\"\"Convert ``datetime.datetime`` to Unix timestamp.\\n\\n        :param datetime.datetime dtv:\\n        :return float: Unix timestamp.\\n        \"\"\"\\n    try:\\n        return time.mktime(dtv.timetuple())\\n    except Exception:\\n        pass\\n', 'datetime to unix timestamp dtv try return time mktime dtv timetuple except exception pass', 'convert datetime datetime to unix timestamp'), ('timestamp_to_date', 294, '@classmethod\\ndef timestamp_to_date(cls, timestamp, fail_silently=True):\\n    \"\"\"Converts the given timestamp to date.\\n\\n        If ``fail_silently`` is set to False, raises exceptions if timestamp\\n        is not valid timestamp (according to the format we have specified in\\n        the ``TIMESTAMP_FORMAT``). Mainly used internally.\\n\\n        :param str timestamp:\\n        :param bool fail_silently:\\n        :return str:\\n        \"\"\"\\n    try:\\n        return datetime.datetime.strptime(timestamp, TIMESTAMP_FORMAT)\\n    except Exception as err:\\n        if fail_silently is not True:\\n            raise err\\n        else:\\n            return None\\n', 'timestamp to date cls timestamp fail silently true try return datetime datetime strptime timestamp timestamp format except exception as err if fail silently is not true raise err else return none', 'converts the given timestamp to date'), ('unix_timestamp_to_date', 314, '@classmethod\\ndef unix_timestamp_to_date(cls, timestamp, fail_silently=True):\\n    \"\"\"Converts the given Unix timestamp to date.\\n        If ``fail_silently`` is set to False, raises exceptions if timestamp\\n        is not valid timestamp.\\n\\n        :param float|str timestamp: UNIX timestamp. Possible to parse to float.\\n        :param bool fail_silently:\\n        :return str:\\n        \"\"\"\\n    try:\\n        return datetime.datetime.fromtimestamp(float(timestamp))\\n    except Exception as err:\\n        if fail_silently is not True:\\n            raise err\\n        else:\\n            return None\\n', 'unix timestamp to date cls timestamp fail silently true try return datetime datetime fromtimestamp float timestamp except exception as err if fail silently is not true raise err else return none', 'converts the given unix timestamp to date if fail silently is set to false raises exceptions if timestamp is not valid timestamp')] [('enable', 33, 'def enable(self):\\n    pass\\n', 'enable self pass', ''), ('disable', 36, 'def disable(self):\\n    pass\\n', 'disable self pass', '')] [('test_loading_unicode_files_with_bad_global_charset', 193, 'def test_loading_unicode_files_with_bad_global_charset(monkeypatch, tmpdir):\\n    dirname = str(tmpdir.mkdir(\\'jedi-test\\'))\\n    filename1 = os.path.join(dirname, \\'test1.py\\')\\n    filename2 = os.path.join(dirname, \\'test2.py\\')\\n    if sys.version_info < (3, 0):\\n        data = \"# coding: latin-1\\\\nfoo = \\'möp\\'\\\\n\"\\n    else:\\n        data = \"# coding: latin-1\\\\nfoo = \\'möp\\'\\\\n\".encode(\\'latin-1\\')\\n    with open(filename1, \\'wb\\') as f:\\n        f.write(data)\\n    s = Script(\\'from test1 import foo\\\\nfoo.\\', line=2, column=4, path=filename2)\\n    s.completions()\\n', 'test loading unicode files with bad global charset monkeypatch tmpdir dirname str tmpdir mkdir jedi test os path join dirname py os path join dirname py if sys version info 3 0 data coding latin 1 nfoo else data coding latin 1 nfoo encode latin 1 with open wb as write data script from import foo nfoo line 2 column 4 path completions', ''), ('test_goto_definition_cursor', 25, 'def test_goto_definition_cursor(self):\\n    s = \"\"\"class A():\\n    def _something(self):\\n        return\\n    def different_line(self,\\n                   b):\\n        return\\nA._something\\nA.different_line\"\"\"\\n    in_name = 2, 9\\n    under_score = 2, 8\\n    cls = 2, 7\\n    should1 = 7, 10\\n    diff_line = 4, 10\\n    should2 = 8, 10\\n\\n    def get_def(pos):\\n        return [d.description for d in Script(s, *pos).goto_definitions()]\\n    in_name = get_def(in_name)\\n    under_score = get_def(under_score)\\n    should1 = get_def(should1)\\n    should2 = get_def(should2)\\n    diff_line = get_def(diff_line)\\n    assert should1 == in_name\\n    assert should1 == under_score\\n    assert should2 == diff_line\\n    assert get_def(cls) == []\\n', 'test goto definition cursor self class def something self return def different line self return something different line in name 2 9 under score 2 8 cls 2 7 7 10 diff line 4 10 8 10 def get def pos return description for in script pos goto definitions in name get def in name under score get def under score get def get def diff line get def diff line assert in name assert under score assert diff line assert get def cls', ''), ('test_add_dynamic_mods', 61, \"@pytest.mark.skipif('True', reason=\\n    'Skip for now, test case is not really supported.')\\n@cwd_at('jedi')\\ndef test_add_dynamic_mods(self):\\n    fname = '__main__.py'\\n    api.settings.additional_dynamic_modules = [fname]\\n    src1 = 'def r(a): return a'\\n    src2 = 'from .. import setup; setup.r(1)'\\n    script = Script(src1, path='../setup.py')\\n    imports.load_module(script._evaluator, os.path.abspath(fname), src2)\\n    result = script.goto_definitions()\\n    assert len(result) == 1\\n    assert result[0].description == 'class int'\\n\", 'test add dynamic mods self fname main py api settings additional dynamic modules fname def return from import setup setup 1 script script path setup py imports load module script evaluator os path abspath fname result script goto definitions assert len result 1 assert result 0 description class int', ''), ('test_os_nowait', 76, 'def test_os_nowait(self):\\n    \"\"\" github issue #45 \"\"\"\\n    s = Script(\\'import os; os.P_\\').completions()\\n    assert \\'P_NOWAIT\\' in [i.name for i in s]\\n', 'test os nowait self script import os os completions assert nowait in name for in', 'github issue 45'), ('test_points_in_completion', 81, 'def test_points_in_completion(self):\\n    \"\"\"At some point, points were inserted into the completions, this\\n        caused problems, sometimes.\\n        \"\"\"\\n    c = Script(\\'if IndentationErr\\').completions()\\n    assert c[0].name == \\'IndentationError\\'\\n    self.assertEqual(c[0].complete, \\'or\\')\\n', 'test points in completion self script if indentationerr completions assert 0 name indentationerror self assertequal 0 complete or', 'at some point points were inserted into the completions this caused problems sometimes'), ('test_no_statement_parent', 89, 'def test_no_statement_parent(self):\\n    source = textwrap.dedent(\\n        \"\"\"\\n        def f():\\n            pass\\n\\n        class C:\\n            pass\\n\\n        variable = f if random.choice([0, 1]) else C\"\"\"\\n        )\\n    defs = Script(source, column=3).goto_definitions()\\n    defs = sorted(defs, key=lambda d: d.line)\\n    self.assertEqual([d.description for d in defs], [\\'def f\\', \\'class C\\'])\\n', 'test no statement parent self source textwrap dedent def pass class pass variable if random choice 0 1 else defs script source column 3 goto definitions defs sorted defs key lambda line self assertequal description for in defs def class', ''), ('test_end_pos_line', 103, \"def test_end_pos_line(self):\\n    s = u('x()\\\\nx( )\\\\nx(  )\\\\nx (  )')\\n    parser = ParserWithRecovery(load_grammar(), s)\\n    for i, s in enumerate(parser.module.statements):\\n        assert s.end_pos == (i + 1, i + 3)\\n\", 'test end pos line self nx nx nx parser parserwithrecovery load grammar for in enumerate parser module statements assert end pos 1 3', ''), ('check_definition_by_marker', 110, 'def check_definition_by_marker(self, source, after_cursor, names):\\n    \"\"\"\\n        Find definitions specified by `after_cursor` and check what found\\n\\n        For example, for the following configuration, you can pass\\n        ``after_cursor = \\'y)\\'``.::\\n\\n            function(\\n                x, y)\\n                   \\\\\\\\\\n                    `- You want cursor to be here\\n        \"\"\"\\n    source = textwrap.dedent(source)\\n    for i, line in enumerate(source.splitlines()):\\n        if after_cursor in line:\\n            break\\n    column = len(line) - len(after_cursor)\\n    defs = Script(source, i + 1, column).goto_definitions()\\n    print(defs)\\n    assert [d.name for d in defs] == names\\n', 'check definition by marker self source after cursor names find definitions specified by after cursor and check what found for example for the following configuration you can pass after cursor function you want cursor to be here source textwrap dedent source for line in enumerate source splitlines if after cursor in line break column len line len after cursor defs script source 1 column goto definitions print defs assert name for in defs names', 'find definitions specified by after cursor and check what found'), ('test_backslash_continuation', 131, 'def test_backslash_continuation(self):\\n    \"\"\"\\n        Test that ModuleWithCursor.get_path_until_cursor handles continuation\\n        \"\"\"\\n    self.check_definition_by_marker(\\n        \"\"\"\\n        x = 0\\n        a = \\\\\\\\\\n          [1, 2, 3, 4, 5, 6, 7, 8, 9, x]  # <-- here\\n        \"\"\"\\n        , \\']  # <-- here\\', [\\'int\\'])\\n    s = \"\"\"asdfxyxxxxxxxx sds\\\\\\\\\\n    hello\"\"\"\\n    assert Script(s, 2, 4).goto_assignments() == []\\n', 'test backslash continuation self self check definition by marker 0 1 2 3 4 5 6 7 8 9 here here int asdfxyxxxxxxxx sds hello assert script 2 4 goto assignments', 'test that modulewithcursor get path until cursor handles continuation'), ('test_backslash_continuation_and_bracket', 145, 'def test_backslash_continuation_and_bracket(self):\\n    self.check_definition_by_marker(\\n        \"\"\"\\n        x = 0\\n        a = \\\\\\\\\\n          [1, 2, 3, 4, 5, 6, 7, 8, 9, (x)]  # <-- here\\n        \"\"\"\\n        , \\'(x)]  # <-- here\\', [\\'int\\'])\\n', 'test backslash continuation and bracket self self check definition by marker 0 1 2 3 4 5 6 7 8 9 here here int', ''), ('test_generator', 152, \"def test_generator(self):\\n    s = 'def abc():\\\\n    yield 1\\\\nabc().'\\n    assert Script(s).completions()\\n\", 'test generator self def abc yield 1 nabc assert script completions', ''), ('test_fake_subnodes', 160, 'def test_fake_subnodes(self):\\n    \"\"\"\\n        Test the number of subnodes of a fake object.\\n\\n        There was a bug where the number of child nodes would grow on every\\n        call to :func:``jedi.evaluate.compiled.fake.get_faked``.\\n\\n        See Github PR#649 and isseu #591.\\n        \"\"\"\\n\\n    def get_str_completion(values):\\n        for c in values:\\n            if c.name == \\'str\\':\\n                return c\\n    limit = None\\n    for i in range(2):\\n        completions = Script(\\'\\').completions()\\n        c = get_str_completion(completions)\\n        n = len(c._definition.subscopes[0].children[-1].children)\\n        if i == 0:\\n            limit = n\\n        else:\\n            assert n == limit\\n', 'test fake subnodes self def get str completion values for in values if name str return limit none for in range 2 completions script completions get str completion completions len definition subscopes 0 children 1 children if 0 limit else assert limit', 'test the number of subnodes of a fake object'), ('test_source_to_unicode_unicode_text', 183, \"def test_source_to_unicode_unicode_text(self):\\n    source = (\\n        b'# vim: fileencoding=utf-8\\\\n# \\\\xe3\\\\x81\\\\x82\\\\xe3\\\\x81\\\\x84\\\\xe3\\\\x81\\\\x86\\\\xe3\\\\x81\\\\x88\\\\xe3\\\\x81\\\\x8a\\\\n'\\n        )\\n    actual = common.source_to_unicode(source)\\n    expected = source.decode('utf-8')\\n    assert actual == expected\\n\", 'test source to unicode unicode text self source vim fileencoding utf 8 actual common source to unicode source expected source decode utf 8 assert actual expected', '')] [('oauth_callback', 13, \"@mod.route('/oauth_callback', methods=['GET'])\\ndef oauth_callback():\\n    code = request.args['code']\\n    res = github.get_access_token(code)\\n    access_token = res.json()['access_token']\\n    res = github.get_user_info(access_token)\\n    data = res.json()\\n    user = upsert_user(github_access_token=access_token, avatar_url=data[\\n        'avatar_url'], username=data['login'])\\n    print('user={}'.format(user))\\n    sid = sessions.create({'user_id': str(user.id)})\\n    return redirect(config.FRONTEND_URL + '#/?code=' + sid)\\n\", 'oauth callback code request args code res github get access token code access token res json access token res github get user info access token data res json user upsert user github access token access token avatar url data avatar url username data login print user format user sid sessions create user id str user id return redirect config frontend url code sid', '')] [] [('shutdown', 68, 'def shutdown(ioloop, server):\\n    \"\"\" 关闭server\\n\\n    :param server: tornado.httpserver.HTTPServer\\n    \"\"\"\\n    logging.info(\\'HTTP interpreter service will shutdown in %ss...\\', 1)\\n    server.stop()\\n    deadline = time.time() + 1\\n\\n    def stop_loop():\\n        \"\"\" 尝试关闭loop\\n        \"\"\"\\n        now = time.time()\\n        if now < deadline and (ioloop._callbacks or ioloop._timeouts):\\n            ioloop.add_timeout(now + 1, stop_loop)\\n        else:\\n            ioloop.stop()\\n            logging.info(\\'Shutdown!\\')\\n    stop_loop()\\n', 'shutdown ioloop server logging info http interpreter service will shutdown in ss 1 server stop deadline time time 1 def stop loop loop now time time if now deadline and ioloop callbacks or ioloop timeouts ioloop add timeout now 1 stop loop else ioloop stop logging info shutdown stop loop', 'server'), ('main', 93, 'def main():\\n    \"\"\" main 函数\\n    \"\"\"\\n    ioloop = tornado.ioloop.IOLoop.instance()\\n    server = tornado.httpserver.HTTPServer(Application(), xheaders=True)\\n    server.listen(options.port)\\n\\n    def sig_handler(sig, _):\\n        \"\"\" 信号接收函数\\n        \"\"\"\\n        logging.warn(\\'Caught signal: %s\\', sig)\\n        shutdown(ioloop, server)\\n    signal.signal(signal.SIGTERM, sig_handler)\\n    signal.signal(signal.SIGINT, sig_handler)\\n    ioloop.start()\\n', 'main ioloop tornado ioloop ioloop instance server tornado httpserver httpserver application xheaders true server listen options port def sig handler sig logging warn caught signal sig shutdown ioloop server signal signal signal sigterm sig handler signal signal signal sigint sig handler ioloop start', 'main'), ('__init__', 35, 'def __init__(self):\\n    \"\"\"\\n        应用初始化\\n        \"\"\"\\n    settings = {\\'xsrf_cookies\\': False, \\'site_title\\': \\'demo\\', \\'debug\\':\\n        options.debug, \\'static_path\\': os.path.join(options.project_path,\\n        \\'static\\'), \\'template_path\\': os.path.join(options.project_path, \\'tpl\\')}\\n    handlers = auto_route_handlers\\n    logging.info(\\'----> %s\\', handlers)\\n    tornado.web.Application.__init__(self, handlers, **settings)\\n', 'init self settings xsrf cookies false site title demo debug options debug static path os path join options project path static template path os path join options project path tpl handlers auto route handlers logging info handlers tornado web application init self handlers settings', ''), ('log_request', 50, 'def log_request(self, handler):\\n    \"\"\"定制如何记录日志\\n\\n        @handler: request handler\\n        \"\"\"\\n    status = handler.get_status()\\n    request_time = 1000.0 * handler.request.request_time()\\n    msg = \\'%d %s %.2f\\' % (status, handler._request_summary(), request_time)\\n    if status < 400:\\n        log_method = logging.info\\n    elif status < 500:\\n        log_method = logging.warning\\n    else:\\n        log_method = logging.error\\n    log_method(msg)\\n', 'log request self handler status handler get status request time 1000 0 handler request request time msg status handler request summary request time if status 400 log method logging info elif status 500 log method logging warning else log method logging error log method msg', '')] [('input_fn', 38, 'def input_fn(self, batch_size=None, points=None):\\n    batch_size = batch_size or self.batch_size\\n    points = points if points is not None else self.points\\n    num_points = points.shape[0]\\n\\n    def _fn():\\n        x = constant_op.constant(points)\\n        if batch_size == num_points:\\n            return x, None\\n        indices = random_ops.random_uniform(constant_op.constant([\\n            batch_size]), minval=0, maxval=num_points - 1, dtype=dtypes.\\n            int32, seed=10)\\n        return array_ops.gather(x, indices), None\\n    return _fn\\n', 'input fn self batch size none points none batch size batch size or self batch size points points if points is not none else self points num points points shape 0 def fn constant op constant points if batch size num points return none indices random ops random uniform constant op constant batch size minval 0 maxval num points 1 dtype dtypes seed 10 return array ops gather indices none return fn', ''), ('setUp', 54, 'def setUp(self):\\n    np.random.seed(3)\\n    random_seed_lib.set_random_seed(2)\\n    self.num_centers = 2\\n    self.num_dims = 2\\n    self.num_points = 4000\\n    self.batch_size = self.num_points\\n    self.true_centers = self.make_random_centers(self.num_centers, self.\\n        num_dims)\\n    self.points, self.assignments = self.make_random_points(self.\\n        true_centers, self.num_points)\\n    clusterer = kmeans.KMeansClustering(num_clusters=self.num_centers)\\n    clusterer.fit(input_fn=lambda : (constant_op.constant(self.points),\\n        None), steps=30)\\n    self.initial_means = clusterer.clusters()\\n', 'setup self np random seed 3 random seed lib set random seed 2 self num centers 2 self num dims 2 self num points 4000 self batch size self num points self true centers self make random centers self num centers self num dims self points self assignments self make random points self true centers self num points clusterer kmeans kmeansclustering num clusters self num centers clusterer fit input fn lambda constant op constant self points none steps 30 self initial means clusterer clusters', ''), ('make_random_centers', 72, '@staticmethod\\ndef make_random_centers(num_centers, num_dims):\\n    return np.round(np.random.rand(num_centers, num_dims).astype(np.float32\\n        ) * 500)\\n', 'make random centers num centers num dims return np round np random rand num centers num dims astype np 500', ''), ('make_random_points', 77, '@staticmethod\\ndef make_random_points(centers, num_points):\\n    num_centers, num_dims = centers.shape\\n    assignments = np.random.choice(num_centers, num_points)\\n    offsets = np.round(np.random.randn(num_points, num_dims).astype(np.\\n        float32) * 20)\\n    points = centers[assignments] + offsets\\n    return points, assignments\\n', 'make random points centers num points num centers num dims centers shape assignments np random choice num centers num points offsets np round np random randn num points num dims astype np 20 points centers assignments offsets return points assignments', ''), ('test_weights', 86, 'def test_weights(self):\\n    \"\"\"Tests the shape of the weights.\"\"\"\\n    gmm = gmm_lib.GMM(self.num_centers, initial_clusters=self.initial_means,\\n        random_seed=4, config=run_config.RunConfig(tf_random_seed=2))\\n    gmm.fit(input_fn=self.input_fn(), steps=0)\\n    weights = gmm.weights()\\n    self.assertAllEqual(list(weights.shape), [self.num_centers])\\n', 'test weights self gmm gmm lib gmm self num centers initial clusters self initial means random seed 4 config run config runconfig tf random seed 2 gmm fit input fn self input fn steps 0 weights gmm weights self assertallequal list weights shape self num centers', 'tests the shape of the weights'), ('test_clusters', 96, 'def test_clusters(self):\\n    \"\"\"Tests the shape of the clusters.\"\"\"\\n    gmm = gmm_lib.GMM(self.num_centers, initial_clusters=self.initial_means,\\n        random_seed=4, config=run_config.RunConfig(tf_random_seed=2))\\n    gmm.fit(input_fn=self.input_fn(), steps=0)\\n    clusters = gmm.clusters()\\n    self.assertAllEqual(list(clusters.shape), [self.num_centers, self.num_dims]\\n        )\\n', 'test clusters self gmm gmm lib gmm self num centers initial clusters self initial means random seed 4 config run config runconfig tf random seed 2 gmm fit input fn self input fn steps 0 clusters gmm clusters self assertallequal list clusters shape self num centers self num dims', 'tests the shape of the clusters'), ('test_fit', 106, \"def test_fit(self):\\n    gmm = gmm_lib.GMM(self.num_centers, initial_clusters='random',\\n        random_seed=4, config=run_config.RunConfig(tf_random_seed=2))\\n    gmm.fit(input_fn=self.input_fn(), steps=1)\\n    score1 = gmm.score(input_fn=self.input_fn(batch_size=self.num_points),\\n        steps=1)\\n    gmm.fit(input_fn=self.input_fn(), steps=10)\\n    score2 = gmm.score(input_fn=self.input_fn(batch_size=self.num_points),\\n        steps=1)\\n    self.assertLess(score1, score2)\\n\", 'test fit self gmm gmm lib gmm self num centers initial clusters random random seed 4 config run config runconfig tf random seed 2 gmm fit input fn self input fn steps 1 gmm score input fn self input fn batch size self num points steps 1 gmm fit input fn self input fn steps 10 gmm score input fn self input fn batch size self num points steps 1 self assertless', ''), ('test_infer', 119, 'def test_infer(self):\\n    gmm = gmm_lib.GMM(self.num_centers, initial_clusters=self.initial_means,\\n        random_seed=4, config=run_config.RunConfig(tf_random_seed=2))\\n    gmm.fit(input_fn=self.input_fn(), steps=60)\\n    clusters = gmm.clusters()\\n    num_points = 40\\n    points, true_assignments = self.make_random_points(clusters, num_points)\\n    assignments = []\\n    for item in gmm.predict_assignments(input_fn=self.input_fn(points=\\n        points, batch_size=num_points)):\\n        assignments.append(item)\\n    assignments = np.ravel(assignments)\\n    self.assertAllEqual(true_assignments, assignments)\\n', 'test infer self gmm gmm lib gmm self num centers initial clusters self initial means random seed 4 config run config runconfig tf random seed 2 gmm fit input fn self input fn steps 60 clusters gmm clusters num points 40 points true assignments self make random points clusters num points assignments for item in gmm predict assignments input fn self input fn points points batch size num points assignments append item assignments np ravel assignments self assertallequal true assignments assignments', ''), ('_compare_with_sklearn', 138, \"def _compare_with_sklearn(self, cov_type):\\n    iterations = 40\\n    np.random.seed(5)\\n    sklearn_assignments = np.asarray([0, 0, 1, 0, 0, 0, 1, 0, 0, 1])\\n    sklearn_means = np.asarray([[144.83417719, 254.20130341], [274.38754816,\\n        353.16074346]])\\n    sklearn_covs = np.asarray([[[395.0081194, -4.50389512], [-4.50389512, \\n        408.27543989]], [[385.17484203, -31.27834935], [-31.27834935, \\n        391.74249925]]])\\n    gmm = gmm_lib.GMM(self.num_centers, initial_clusters=self.initial_means,\\n        covariance_type=cov_type, config=run_config.RunConfig(tf_random_seed=2)\\n        )\\n    gmm.fit(input_fn=self.input_fn(), steps=iterations)\\n    points = self.points[:10, :]\\n    skflow_assignments = []\\n    for item in gmm.predict_assignments(input_fn=self.input_fn(points=\\n        points, batch_size=10)):\\n        skflow_assignments.append(item)\\n    self.assertAllClose(sklearn_assignments, np.ravel(skflow_assignments).\\n        astype(int))\\n    self.assertAllClose(sklearn_means, gmm.clusters())\\n    if cov_type == 'full':\\n        self.assertAllClose(sklearn_covs, gmm.covariances(), rtol=0.01)\\n    else:\\n        for d in [0, 1]:\\n            self.assertAllClose(np.diag(sklearn_covs[d]), gmm.covariances()\\n                [(d), :], rtol=0.01)\\n\", 'compare with sklearn self cov type iterations 40 np random seed 5 sklearn assignments np asarray 0 0 1 0 0 0 1 0 0 1 sklearn means np asarray 144 83417719 254 20130341 274 38754816 353 16074346 sklearn covs np asarray 395 0081194 4 50389512 4 50389512 408 27543989 385 17484203 31 27834935 31 27834935 391 74249925 gmm gmm lib gmm self num centers initial clusters self initial means covariance type cov type config run config runconfig tf random seed 2 gmm fit input fn self input fn steps iterations points self points 10 skflow assignments for item in gmm predict assignments input fn self input fn points points batch size 10 skflow assignments append item self assertallclose sklearn assignments np ravel skflow assignments astype int self assertallclose sklearn means gmm clusters if cov type full self assertallclose sklearn covs gmm covariances rtol 0 01 else for in 0 1 self assertallclose np diag sklearn covs gmm covariances rtol 0 01', ''), ('test_compare_full', 171, \"def test_compare_full(self):\\n    self._compare_with_sklearn('full')\\n\", 'test compare full self self compare with sklearn full', ''), ('test_compare_diag', 174, \"def test_compare_diag(self):\\n    self._compare_with_sklearn('diag')\\n\", 'test compare diag self self compare with sklearn diag', ''), ('test_random_input_large', 177, \"def test_random_input_large(self):\\n    iterations = 5\\n    np.random.seed(5)\\n    num_classes = 20\\n    x = np.array([[np.random.random() for _ in range(100)] for _ in range(\\n        num_classes)], dtype=np.float32)\\n    gmm = gmm_lib.GMM(num_classes, covariance_type='full', config=\\n        run_config.RunConfig(tf_random_seed=2))\\n\\n    def get_input_fn(x):\\n\\n        def input_fn():\\n            return constant_op.constant(x.astype(np.float32)), None\\n        return input_fn\\n    gmm.fit(input_fn=get_input_fn(x), steps=iterations)\\n    self.assertFalse(np.isnan(gmm.clusters()).any())\\n\", 'test random input large self iterations 5 np random seed 5 num classes 20 np array np random random for in range 100 for in range num classes dtype np gmm gmm lib gmm num classes covariance type full config run config runconfig tf random seed 2 def get input fn def input fn return constant op constant astype np none return input fn gmm fit input fn get input fn steps iterations self assertfalse np isnan gmm clusters any', ''), ('input_fn', 201, 'def input_fn(self):\\n\\n    def _fn():\\n        queue = data_flow_ops.FIFOQueue(capacity=10, dtypes=dtypes.float32,\\n            shapes=[10, 3])\\n        enqueue_op = queue.enqueue(array_ops.zeros([10, 3], dtype=dtypes.\\n            float32))\\n        queue_runner.add_queue_runner(queue_runner.QueueRunner(queue, [\\n            enqueue_op]))\\n        return queue.dequeue(), None\\n    return _fn\\n', 'input fn self def fn queue data flow ops fifoqueue capacity 10 dtypes dtypes shapes 10 3 enqueue op queue enqueue array ops zeros 10 3 dtype dtypes queue runner add queue runner queue runner queuerunner queue enqueue op return queue dequeue none return fn', ''), ('test_queues', 216, \"def test_queues(self):\\n    gmm = gmm_lib.GMM(2, covariance_type='diag')\\n    gmm.fit(input_fn=self.input_fn(), steps=1)\\n\", 'test queues self gmm gmm lib gmm 2 covariance type diag gmm fit input fn self input fn steps 1', '')] [('__init__', 25, \"def __init__(self):\\n    self.xsi_type = None\\n    self.tag = None\\n    self.name = None\\n    self.display_name = ''\\n    self.permission = None\\n\", 'init self self xsi type none self tag none self name none self display name self permission none', ''), ('__repr__', 32, 'def __repr__(self):\\n    return repr(\\'Grantee(\"%(tag)s\", \"%(name)s\", \"%(permission)s\")\\' % {\\'tag\\':\\n        self.tag, \\'name\\': self.name, \\'permission\\': self.permission})\\n', 'repr self return repr grantee tag name permission tag self tag name self name permission self permission', ''), ('isAllUsers', 39, \"def isAllUsers(self):\\n    return self.tag == 'URI' and self.name == Grantee.ALL_USERS_URI\\n\", 'isallusers self return self tag uri and self name grantee all users uri', ''), ('isAnonRead', 42, \"def isAnonRead(self):\\n    return self.isAllUsers() and (self.permission == 'READ' or self.\\n        permission == 'FULL_CONTROL')\\n\", 'isanonread self return self isallusers and self permission read or self permission full control', ''), ('getElement', 45, \"def getElement(self):\\n    el = ET.Element('Grant')\\n    grantee = ET.SubElement(el, 'Grantee', {'xmlns:xsi':\\n        'http://www.w3.org/2001/XMLSchema-instance', 'xsi:type': self.xsi_type}\\n        )\\n    name = ET.SubElement(grantee, self.tag)\\n    name.text = self.name\\n    permission = ET.SubElement(el, 'Permission')\\n    permission.text = self.permission\\n    return el\\n\", 'getelement self el et element grant grantee et subelement el grantee xmlns xsi http www org 2001 xmlschema instance xsi type self xsi type name et subelement grantee self tag name text self name permission et subelement el permission permission text self permission return el', ''), ('__init__', 58, \"def __init__(self):\\n    Grantee.__init__(self)\\n    self.xsi_type = 'Group'\\n    self.tag = 'URI'\\n    self.name = Grantee.ALL_USERS_URI\\n    self.permission = 'READ'\\n\", 'init self grantee init self self xsi type group self tag uri self name grantee all users uri self permission read', ''), ('__init__', 66, 'def __init__(self, permission):\\n    \"\"\"\\n        permission must be either READ_ACP or WRITE\\n        \"\"\"\\n    Grantee.__init__(self)\\n    self.xsi_type = \\'Group\\'\\n    self.tag = \\'URI\\'\\n    self.name = Grantee.LOG_DELIVERY_URI\\n    self.permission = permission\\n', 'init self permission grantee init self self xsi type group self tag uri self name grantee log delivery uri self permission permission', 'permission must be either read acp or write'), ('__init__', 79, \"def __init__(self, xml=None):\\n    if not xml:\\n        xml = ACL.EMPTY_ACL\\n    self.grantees = []\\n    self.owner_id = ''\\n    self.owner_nick = ''\\n    tree = getTreeFromXml(encode_to_s3(xml))\\n    self.parseOwner(tree)\\n    self.parseGrants(tree)\\n\", 'init self xml none if not xml xml acl empty acl self grantees self owner id self owner nick tree gettreefromxml encode to xml self parseowner tree self parsegrants tree', ''), ('parseOwner', 91, \"def parseOwner(self, tree):\\n    self.owner_id = tree.findtext('.//Owner//ID')\\n    self.owner_nick = tree.findtext('.//Owner//DisplayName')\\n\", 'parseowner self tree self owner id tree findtext owner id self owner nick tree findtext owner displayname', ''), ('parseGrants', 95, \"def parseGrants(self, tree):\\n    for grant in tree.findall('.//Grant'):\\n        grantee = Grantee()\\n        g = grant.find('.//Grantee')\\n        grantee.xsi_type = g.attrib[\\n            '{http://www.w3.org/2001/XMLSchema-instance}type']\\n        grantee.permission = grant.find('Permission').text\\n        for el in g:\\n            if el.tag == 'DisplayName':\\n                grantee.display_name = el.text\\n            else:\\n                grantee.tag = el.tag\\n                grantee.name = el.text\\n        self.grantees.append(grantee)\\n\", 'parsegrants self tree for grant in tree findall grant grantee grantee grant find grantee grantee xsi type attrib http www org 2001 xmlschema instance type grantee permission grant find permission text for el in if el tag displayname grantee display name el text else grantee tag el tag grantee name el text self grantees append grantee', ''), ('getGrantList', 109, \"def getGrantList(self):\\n    acl = []\\n    for grantee in self.grantees:\\n        if grantee.display_name:\\n            user = grantee.display_name\\n        elif grantee.isAllUsers():\\n            user = '*anon*'\\n        else:\\n            user = grantee.name\\n        acl.append({'grantee': user, 'permission': grantee.permission})\\n    return acl\\n\", 'getgrantlist self acl for grantee in self grantees if grantee display name user grantee display name elif grantee isallusers user anon else user grantee name acl append grantee user permission grantee permission return acl', ''), ('getOwner', 121, \"def getOwner(self):\\n    return {'id': self.owner_id, 'nick': self.owner_nick}\\n\", 'getowner self return id self owner id nick self owner nick', ''), ('isAnonRead', 124, 'def isAnonRead(self):\\n    for grantee in self.grantees:\\n        if grantee.isAnonRead():\\n            return True\\n    return False\\n', 'isanonread self for grantee in self grantees if grantee isanonread return true return false', ''), ('grantAnonRead', 130, 'def grantAnonRead(self):\\n    if not self.isAnonRead():\\n        self.appendGrantee(GranteeAnonRead())\\n', 'grantanonread self if not self isanonread self appendgrantee granteeanonread', ''), ('revokeAnonRead', 134, 'def revokeAnonRead(self):\\n    self.grantees = [g for g in self.grantees if not g.isAnonRead()]\\n', 'revokeanonread self self grantees for in self grantees if not isanonread', ''), ('appendGrantee', 137, 'def appendGrantee(self, grantee):\\n    self.grantees.append(grantee)\\n', 'appendgrantee self grantee self grantees append grantee', ''), ('hasGrant', 140, \"def hasGrant(self, name, permission):\\n    name = name.lower()\\n    permission = permission.upper()\\n    for grantee in self.grantees:\\n        if grantee.name.lower() == name:\\n            if grantee.permission == 'FULL_CONTROL':\\n                return True\\n            elif grantee.permission.upper() == permission:\\n                return True\\n    return False\\n\", 'hasgrant self name permission name name lower permission permission upper for grantee in self grantees if grantee name lower name if grantee permission full control return true elif grantee permission upper permission return true return false', ''), ('grant', 153, \"def grant(self, name, permission):\\n    if self.hasGrant(name, permission):\\n        return\\n    permission = permission.upper()\\n    if 'ALL' == permission:\\n        permission = 'FULL_CONTROL'\\n    if 'FULL_CONTROL' == permission:\\n        self.revoke(name, 'ALL')\\n    grantee = Grantee()\\n    grantee.name = name\\n    grantee.permission = permission\\n    if '@' in name:\\n        grantee.name = grantee.name.lower()\\n        grantee.xsi_type = 'AmazonCustomerByEmail'\\n        grantee.tag = 'EmailAddress'\\n    elif 'http://acs.amazonaws.com/groups/' in name:\\n        grantee.xsi_type = 'Group'\\n        grantee.tag = 'URI'\\n    else:\\n        grantee.name = grantee.name.lower()\\n        grantee.xsi_type = 'CanonicalUser'\\n        grantee.tag = 'ID'\\n    self.appendGrantee(grantee)\\n\", 'grant self name permission if self hasgrant name permission return permission permission upper if all permission permission full control if full control permission self revoke name all grantee grantee grantee name name grantee permission permission if in name grantee name grantee name lower grantee xsi type amazoncustomerbyemail grantee tag emailaddress elif http acs amazonaws com groups in name grantee xsi type group grantee tag uri else grantee name grantee name lower grantee xsi type canonicaluser grantee tag id self appendgrantee grantee', ''), ('revoke', 184, \"def revoke(self, name, permission):\\n    name = name.lower()\\n    permission = permission.upper()\\n    if 'ALL' == permission:\\n        self.grantees = [g for g in self.grantees if not (g.name.lower() ==\\n            name or g.display_name.lower() == name)]\\n    else:\\n        self.grantees = [g for g in self.grantees if not ((g.display_name.\\n            lower() == name or g.name.lower() == name) and g.permission.\\n            upper() == permission)]\\n\", 'revoke self name permission name name lower permission permission upper if all permission self grantees for in self grantees if not name lower name or display name lower name else self grantees for in self grantees if not display name lower name or name lower name and permission upper permission', ''), ('get_printable_tree', 194, \"def get_printable_tree(self):\\n    tree = getTreeFromXml(ACL.EMPTY_ACL)\\n    tree.attrib['xmlns'] = 'http://s3.amazonaws.com/doc/2006-03-01/'\\n    owner = tree.find('.//Owner//ID')\\n    owner.text = self.owner_id\\n    acl = tree.find('.//AccessControlList')\\n    for grantee in self.grantees:\\n        acl.append(grantee.getElement())\\n    return tree\\n\", 'get printable tree self tree gettreefromxml acl empty acl tree attrib xmlns http amazonaws com doc 2006 03 01 owner tree find owner id owner text self owner id acl tree find accesscontrollist for grantee in self grantees acl append grantee getelement return tree', ''), ('__unicode__', 204, 'def __unicode__(self):\\n    return decode_from_s3(ET.tostring(self.get_printable_tree()))\\n', 'unicode self return decode from et tostring self get printable tree', ''), ('__str__', 207, \"def __str__(self):\\n    if PY3:\\n        return ET.tostring(self.get_printable_tree(), encoding='unicode')\\n    else:\\n        return ET.tostring(self.get_printable_tree())\\n\", 'str self if return et tostring self get printable tree encoding unicode else return et tostring self get printable tree', '')] [('test_phone_numbers', 6, \"@patch('twilio.rest.resources.base.make_twilio_request')\\ndef test_phone_numbers(mock):\\n    client = TwilioLookupsClient('ACCOUNT_SID', 'AUTH_TOKEN')\\n    resp = create_mock_json(\\n        'tests/resources/lookups/phone_number_instance.json')\\n    mock.return_value = resp\\n    client.phone_numbers.get('+15108675309')\\n    uri = 'https://lookups.twilio.com/v1/PhoneNumbers/+15108675309'\\n    mock.assert_called_with('GET', uri, params={}, auth=('ACCOUNT_SID',\\n        'AUTH_TOKEN'), use_json_extension=False)\\n\", 'test phone numbers mock client twiliolookupsclient account sid auth token resp create mock json tests resources lookups phone number instance json mock return value resp client phone numbers get 15108675309 uri https lookups twilio com phonenumbers 15108675309 mock assert called with get uri params auth account sid auth token use json extension false', '')] [('reduce_execution_results', 57, 'def reduce_execution_results(execution_results: List[ExecutionResult]\\n    ) ->ExecutionResult:\\n    \"\"\"Aggregate execution results into one.\"\"\"\\n    outputs = []\\n    losses_sum = [(0.0) for _ in execution_results[0].losses]\\n    for result in execution_results:\\n        outputs.extend(result.outputs)\\n        for i, loss in enumerate(result.losses):\\n            losses_sum[i] += loss\\n    if outputs and isinstance(outputs[0], np.ndarray):\\n        outputs = np.array(outputs)\\n    losses = [(l / max(len(outputs), 1)) for l in losses_sum]\\n    return ExecutionResult(outputs, losses, execution_results[0].\\n        scalar_summaries, execution_results[0].histogram_summaries,\\n        execution_results[0].image_summaries)\\n', 'reduce execution results execution results list executionresult executionresult outputs losses sum 0 0 for in execution results 0 losses for result in execution results outputs extend result outputs for loss in enumerate result losses losses sum loss if outputs and isinstance outputs 0 np ndarray outputs np array outputs losses max len outputs 1 for in losses sum return executionresult outputs losses execution results 0 scalar summaries execution results 0 histogram summaries execution results 0 image summaries', 'aggregate execution results into one'), ('next_to_execute', 23, 'def next_to_execute(self) ->NextExecute:\\n    raise NotImplementedError()\\n', 'next to execute self nextexecute raise notimplementederror', ''), ('collect_results', 26, 'def collect_results(self, results: List[Dict]) ->None:\\n    raise NotImplementedError()\\n', 'collect results self results list dict none raise notimplementederror', ''), ('__init__', 31, 'def __init__(self, output_series: str, decoder: MP) ->None:\\n    self.output_series = output_series\\n    self._decoder = decoder\\n    self.all_coders = decoder.get_dependencies()\\n    if not hasattr(decoder, \\'data_id\\'):\\n        notice(\"Top-level decoder {} does not have the \\'data_id\\' attribute\"\\n            .format(decoder.name))\\n', 'init self output series str decoder mp none self output series output series self decoder decoder self all coders decoder get dependencies if not hasattr decoder data id notice top level decoder does not have the data id attribute format decoder name', ''), ('get_executable', 42, 'def get_executable(self, compute_losses: bool, summaries: bool,\\n    num_sessions: int) ->Executable:\\n    raise NotImplementedError()\\n', 'get executable self compute losses bool summaries bool num sessions int executable raise notimplementederror', ''), ('decoder_data_id', 48, \"@property\\ndef decoder_data_id(self) ->str:\\n    return getattr(self._decoder, 'data_id', None)\\n\", 'decoder data id self str return getattr self decoder data id none', ''), ('loss_names', 52, '@property\\ndef loss_names(self) ->List[str]:\\n    raise NotImplementedError()\\n', 'loss names self list str raise notimplementederror', '')] [] [] [('setUpModule', 192, 'def setUpModule():\\n    setup_logging()\\n', 'setupmodule setup logging', ''), ('handle_authorized_event', 71, '@event_handler(AuthorizedEvent)\\ndef handle_authorized_event(self, event):\\n    event.stream.close()\\n    return True\\n', 'handle authorized event self event event stream close return true', ''), ('test_bind_no_resource', 78, 'def test_bind_no_resource(self):\\n    handler = AuthorizedEventHandler()\\n    handlers = [ResourceBindingHandler(), handler]\\n    processor = StanzaProcessor()\\n    processor.setup_stanza_handlers(handlers, \\'post-auth\\')\\n    self.stream = StreamBase(\\'jabber:client\\', processor, handlers)\\n    processor.uplink = self.stream\\n    self.stream.me = JID(\\'test@127.0.0.1\\')\\n    self.start_transport([handler])\\n    self.stream.initiate(self.transport)\\n    self.connect_transport()\\n    self.server.write(C2S_SERVER_STREAM_HEAD)\\n    self.wait_short(1)\\n    self.server.write(BIND_FEATURES)\\n    req_id = self.wait(1, expect=re.compile(\\n        b\\'.*<iq[^>]*id=[\\\\\\\\\"\\\\\\']([^\\\\\\\\\"\\\\\\']*)[\\\\\\\\\"\\\\\\']\\'))\\n    self.assertIsNotNone(req_id)\\n    req_id = req_id.decode(\\'utf-8\\')\\n    self.server.write(BIND_GENERATED_RESPONSE.format(req_id).encode(\\'utf-8\\'))\\n    self.wait()\\n    self.assertFalse(self.stream.is_connected())\\n    event_classes = [e.__class__ for e in handler.events_received]\\n    self.assertEqual(event_classes, [ConnectingEvent, ConnectedEvent,\\n        StreamConnectedEvent, GotFeaturesEvent, BindingResourceEvent,\\n        AuthorizedEvent, DisconnectedEvent])\\n', 'test bind no resource self handler authorizedeventhandler handlers resourcebindinghandler handler processor stanzaprocessor processor setup stanza handlers handlers post auth self stream streambase jabber client processor handlers processor uplink self stream self stream me jid test 127 0 0 1 self start transport handler self stream initiate self transport self connect transport self server write server stream head self wait short 1 self server write bind features req id self wait 1 expect re compile iq id self assertisnotnone req id req id req id decode utf 8 self server write bind generated response format req id encode utf 8 self wait self assertfalse self stream is connected event classes class for in handler events received self assertequal event classes connectingevent connectedevent streamconnectedevent gotfeaturesevent bindingresourceevent authorizedevent disconnectedevent', ''), ('test_bind', 105, 'def test_bind(self):\\n    handler = AuthorizedEventHandler()\\n    handlers = [ResourceBindingHandler(), handler]\\n    processor = StanzaProcessor()\\n    processor.setup_stanza_handlers(handlers, \\'post-auth\\')\\n    self.stream = StreamBase(\\'jabber:client\\', processor, handlers,\\n        XMPPSettings({\\'resource\\': \\'Provided\\'}))\\n    processor.uplink = self.stream\\n    self.stream.me = JID(\\'test@127.0.0.1\\')\\n    self.start_transport([handler])\\n    self.stream.initiate(self.transport)\\n    self.connect_transport()\\n    self.server.write(C2S_SERVER_STREAM_HEAD)\\n    self.wait_short(1)\\n    self.server.write(BIND_FEATURES)\\n    req_id = self.wait(1, expect=re.compile(\\n        b\\'.*<iq[^>]*id=[\\\\\\\\\"\\\\\\']([^\\\\\\\\\"\\\\\\']*)[\\\\\\\\\"\\\\\\'].*<resource>Provided</resource>\\'\\n        ))\\n    self.assertIsNotNone(req_id)\\n    req_id = req_id.decode(\\'utf-8\\')\\n    self.server.write(BIND_PROVIDED_RESPONSE.format(req_id).encode(\\'utf-8\\'))\\n    self.wait()\\n    event_classes = [e.__class__ for e in handler.events_received]\\n    self.assertEqual(event_classes, [ConnectingEvent, ConnectedEvent,\\n        StreamConnectedEvent, GotFeaturesEvent, BindingResourceEvent,\\n        AuthorizedEvent, DisconnectedEvent])\\n', 'test bind self handler authorizedeventhandler handlers resourcebindinghandler handler processor stanzaprocessor processor setup stanza handlers handlers post auth self stream streambase jabber client processor handlers xmppsettings resource provided processor uplink self stream self stream me jid test 127 0 0 1 self start transport handler self stream initiate self transport self connect transport self server write server stream head self wait short 1 self server write bind features req id self wait 1 expect re compile iq id resource provided resource self assertisnotnone req id req id req id decode utf 8 self server write bind provided response format req id encode utf 8 self wait event classes class for in handler events received self assertequal event classes connectingevent connectedevent streamconnectedevent gotfeaturesevent bindingresourceevent authorizedevent disconnectedevent', ''), ('test_bind_no_resource', 133, 'def test_bind_no_resource(self):\\n    handler = EventRecorder()\\n    handlers = [ResourceBindingHandler(), handler]\\n    processor = StanzaProcessor()\\n    self.start_transport(handlers)\\n    self.stream = StreamBase(\\'jabber:client\\', processor, handlers)\\n    processor.uplink = self.stream\\n    self.stream.receive(self.transport, self.addr[0])\\n    self.stream.set_peer_authenticated(JID(\\'test@127.0.0.1\\'))\\n    processor.setup_stanza_handlers(handlers, \\'post-auth\\')\\n    self.client.write(C2S_CLIENT_STREAM_HEAD)\\n    features = self.wait(expect=re.compile(\\n        b\\'.*<stream:features>(.*<bind.*urn:ietf:params:xml:ns:xmpp-bind.*)</stream:features>\\'\\n        ))\\n    self.assertIsNotNone(features)\\n    self.client.write(BIND_GENERATED_REQUEST)\\n    resource = self.wait(expect=re.compile(\\n        b\\'.*<iq.*id=(?:\\\\\\\\\"42\\\\\\\\\"|\\\\\\'42\\\\\\').*><bind.*<jid>test@127.0.0.1/(.*)</jid>.*</bind>\\'\\n        ))\\n    self.assertTrue(resource)\\n    self.client.write(STREAM_TAIL)\\n    self.client.disconnect()\\n    self.wait()\\n    event_classes = [e.__class__ for e in handler.events_received]\\n    self.assertEqual(event_classes, [AuthenticatedEvent,\\n        StreamConnectedEvent, AuthorizedEvent, DisconnectedEvent])\\n', 'test bind no resource self handler eventrecorder handlers resourcebindinghandler handler processor stanzaprocessor self start transport handlers self stream streambase jabber client processor handlers processor uplink self stream self stream receive self transport self addr 0 self stream set peer authenticated jid test 127 0 0 1 processor setup stanza handlers handlers post auth self client write client stream head features self wait expect re compile stream features bind urn ietf params xml ns xmpp bind stream features self assertisnotnone features self client write bind generated request resource self wait expect re compile iq id 42 42 bind jid test 127 0 0 1 jid bind self asserttrue resource self client write stream tail self client disconnect self wait event classes class for in handler events received self assertequal event classes authenticatedevent streamconnectedevent authorizedevent disconnectedevent', ''), ('test_bind_resource', 161, 'def test_bind_resource(self):\\n    handler = EventRecorder()\\n    handlers = [ResourceBindingHandler(), handler]\\n    processor = StanzaProcessor()\\n    self.start_transport(handlers)\\n    self.stream = StreamBase(\\'jabber:client\\', processor, handlers)\\n    processor.uplink = self.stream\\n    self.stream.receive(self.transport, self.addr[0])\\n    self.stream.set_peer_authenticated(JID(\\'test@127.0.0.1\\'))\\n    processor.setup_stanza_handlers(handlers, \\'post-auth\\')\\n    self.client.write(C2S_CLIENT_STREAM_HEAD)\\n    features = self.wait(expect=re.compile(\\n        b\\'.*<stream:features>(.*<bind.*urn:ietf:params:xml:ns:xmpp-bind.*)</stream:features>\\'\\n        ))\\n    self.assertIsNotNone(features)\\n    self.client.write(BIND_PROVIDED_REQUEST)\\n    resource = self.wait(expect=re.compile(\\n        b\\'.*<iq.*id=(?:\\\\\\\\\"42\\\\\\\\\"|\\\\\\'42\\\\\\').*><bind.*<jid>test@127.0.0.1/(.*)</jid>.*</bind>\\'\\n        ))\\n    self.assertEqual(resource, b\\'Provided\\')\\n    self.client.write(STREAM_TAIL)\\n    self.client.disconnect()\\n    self.wait()\\n    event_classes = [e.__class__ for e in handler.events_received]\\n    self.assertEqual(event_classes, [AuthenticatedEvent,\\n        StreamConnectedEvent, AuthorizedEvent, DisconnectedEvent])\\n', 'test bind resource self handler eventrecorder handlers resourcebindinghandler handler processor stanzaprocessor self start transport handlers self stream streambase jabber client processor handlers processor uplink self stream self stream receive self transport self addr 0 self stream set peer authenticated jid test 127 0 0 1 processor setup stanza handlers handlers post auth self client write client stream head features self wait expect re compile stream features bind urn ietf params xml ns xmpp bind stream features self assertisnotnone features self client write bind provided request resource self wait expect re compile iq id 42 42 bind jid test 127 0 0 1 jid bind self assertequal resource provided self client write stream tail self client disconnect self wait event classes class for in handler events received self assertequal event classes authenticatedevent streamconnectedevent authorizedevent disconnectedevent', '')] [('ror', 83, 'def ror(self, param1, param2):\\n    _loc3_ = 0\\n    while _loc3_ < param2:\\n        param1 = urshift(param1, 1) + ((param1 & 1) << 31)\\n        _loc3_ += 1\\n    return param1\\n', 'ror self 0 while urshift 1 1 31 1 return', ''), ('calc_time_key', 90, 'def calc_time_key(self, param1):\\n    _loc2_ = 185025305\\n    return self.ror(param1, _loc2_ % 17) ^ _loc2_\\n', 'calc time key self 185025305 return self ror 17', ''), ('decrypt_m3u8', 95, \"@staticmethod\\ndef decrypt_m3u8(encrypted_data):\\n    if encrypted_data[:5].decode('utf-8').lower() != 'vc_01':\\n        return encrypted_data\\n    encrypted_data = encrypted_data[5:]\\n    _loc4_ = bytearray(2 * len(encrypted_data))\\n    for idx, val in enumerate(encrypted_data):\\n        b = compat_ord(val)\\n        _loc4_[2 * idx] = b // 16\\n        _loc4_[2 * idx + 1] = b % 16\\n    idx = len(_loc4_) - 11\\n    _loc4_ = _loc4_[idx:] + _loc4_[:idx]\\n    _loc7_ = bytearray(len(encrypted_data))\\n    for i in range(len(encrypted_data)):\\n        _loc7_[i] = _loc4_[2 * i] * 16 + _loc4_[2 * i + 1]\\n    return bytes(_loc7_)\\n\", 'decrypt encrypted data if encrypted data 5 decode utf 8 lower vc 01 return encrypted data encrypted data encrypted data 5 bytearray 2 len encrypted data for idx val in enumerate encrypted data compat ord val 2 idx 16 2 idx 1 16 idx len 11 idx idx bytearray len encrypted data for in range len encrypted data 2 16 2 1 return bytes', ''), ('_check_errors', 114, \"def _check_errors(self, play_json):\\n    playstatus = play_json['msgs']['playstatus']\\n    if playstatus['status'] == 0:\\n        flag = playstatus['flag']\\n        if flag == 1:\\n            self.raise_geo_restricted()\\n        else:\\n            raise ExtractorError('Generic error. flag = %d' % flag,\\n                expected=True)\\n\", 'check errors self play json playstatus play json msgs playstatus if playstatus status 0 flag playstatus flag if flag 1 self raise geo restricted else raise extractorerror generic error flag flag expected true', ''), ('_real_extract', 124, \"def _real_extract(self, url):\\n    media_id = self._match_id(url)\\n    page = self._download_webpage(url, media_id)\\n    play_json_flash = self._download_json(\\n        'http://player-pc.le.com/mms/out/video/playJson', media_id,\\n        'Downloading flash playJson data', query={'id': media_id, 'platid':\\n        1, 'splatid': 101, 'format': 1, 'source': 1000, 'tkey': self.\\n        calc_time_key(int(time.time())), 'domain': 'www.le.com', 'region':\\n        'cn'}, headers=self.geo_verification_headers())\\n    self._check_errors(play_json_flash)\\n\\n    def get_flash_urls(media_url, format_id):\\n        nodes_data = self._download_json(media_url, media_id, \\n            'Download JSON metadata for format %s' % format_id, query={\\n            'm3v': 1, 'format': 1, 'expect': 3, 'tss': 'ios'})\\n        req = self._request_webpage(nodes_data['nodelist'][0]['location'],\\n            media_id, note='Downloading m3u8 information for format %s' %\\n            format_id)\\n        m3u8_data = self.decrypt_m3u8(req.read())\\n        return {'hls': encode_data_uri(m3u8_data,\\n            'application/vnd.apple.mpegurl')}\\n    extracted_formats = []\\n    formats = []\\n    playurl = play_json_flash['msgs']['playurl']\\n    play_domain = playurl['domain'][0]\\n    for format_id, format_data in playurl.get('dispatch', []).items():\\n        if format_id in extracted_formats:\\n            continue\\n        extracted_formats.append(format_id)\\n        media_url = play_domain + format_data[0]\\n        for protocol, format_url in get_flash_urls(media_url, format_id).items(\\n            ):\\n            f = {'url': format_url, 'ext': determine_ext(format_data[1]),\\n                'format_id': '%s-%s' % (protocol, format_id), 'protocol': \\n                'm3u8_native' if protocol == 'hls' else 'http', 'quality':\\n                int_or_none(format_id)}\\n            if format_id[-1:] == 'p':\\n                f['height'] = int_or_none(format_id[:-1])\\n            formats.append(f)\\n    self._sort_formats(formats, ('height', 'quality', 'format_id'))\\n    publish_time = parse_iso8601(self._html_search_regex(\\n        '发布时间&nbsp;([^<>]+) ', page, 'publish time', default=None),\\n        delimiter=' ', timezone=datetime.timedelta(hours=8))\\n    description = self._html_search_meta('description', page, fatal=False)\\n    return {'id': media_id, 'formats': formats, 'title': playurl['title'],\\n        'thumbnail': playurl['pic'], 'description': description,\\n        'timestamp': publish_time}\\n\", 'real extract self url media id self match id url page self download webpage url media id play json flash self download json http player pc le com mms out video playjson media id downloading flash playjson data query id media id platid 1 splatid 101 format 1 source 1000 tkey self calc time key int time time domain www le com region cn headers self geo verification headers self check errors play json flash def get flash urls media url format id nodes data self download json media url media id download json metadata for format format id query 1 format 1 expect 3 tss ios req self request webpage nodes data nodelist 0 location media id note downloading information for format format id data self decrypt req read return hls encode data uri data application vnd apple mpegurl extracted formats formats playurl play json flash msgs playurl play domain playurl domain 0 for format id format data in playurl get dispatch items if format id in extracted formats continue extracted formats append format id media url play domain format data 0 for protocol format url in get flash urls media url format id items url format url ext determine ext format data 1 format id protocol format id protocol native if protocol hls else http quality int or none format id if format id 1 height int or none format id 1 formats append self sort formats formats height quality format id publish time parse self html search regex nbsp page publish time default none delimiter timezone datetime timedelta hours 8 description self html search meta description page fatal false return id media id formats formats title playurl title thumbnail playurl pic description description timestamp publish time', ''), ('suitable', 237, '@classmethod\\ndef suitable(cls, url):\\n    return False if LeIE.suitable(url) else super(LePlaylistIE, cls).suitable(\\n        url)\\n', 'suitable cls url return false if leie suitable url else super leplaylistie cls suitable url', ''), ('_real_extract', 241, 'def _real_extract(self, url):\\n    playlist_id = self._match_id(url)\\n    page = self._download_webpage(url, playlist_id)\\n    media_ids = orderedSet(re.findall(\\n        \\'<a[^>]+href=\"http://www\\\\\\\\.letv\\\\\\\\.com/ptv/vplay/(\\\\\\\\d+)\\\\\\\\.html\\', page))\\n    entries = [self.url_result(LeIE._URL_TEMPLATE % media_id, ie=\\'Le\\') for\\n        media_id in media_ids]\\n    title = self._html_search_meta(\\'keywords\\', page, fatal=False).split(\\'，\\')[0]\\n    description = self._html_search_meta(\\'description\\', page, fatal=False)\\n    return self.playlist_result(entries, playlist_id, playlist_title=title,\\n        playlist_description=description)\\n', 'real extract self url playlist id self match id url page self download webpage url playlist id media ids orderedset re findall href http www letv com ptv vplay html page entries self url result leie url template media id ie le for media id in media ids title self html search meta keywords page fatal false split 0 description self html search meta description page fatal false return self playlist result entries playlist id playlist title title playlist description description', ''), ('sign_data', 291, \"@staticmethod\\ndef sign_data(obj):\\n    if obj['cf'] == 'flash':\\n        salt = '2f9d6924b33a165a6d8b5d3d42f4f987'\\n        items = ['cf', 'format', 'ran', 'uu', 'ver', 'vu']\\n    elif obj['cf'] == 'html5':\\n        salt = 'fbeh5player12c43eccf2bec3300344'\\n        items = ['cf', 'ran', 'uu', 'bver', 'vu']\\n    input_data = ''.join([(item + obj[item]) for item in items]) + salt\\n    obj['sign'] = hashlib.md5(input_data.encode('utf-8')).hexdigest()\\n\", 'sign data obj if obj cf flash salt items cf format ran uu ver vu elif obj cf salt items cf ran uu bver vu input data join item obj item for item in items salt obj sign hashlib input data encode utf 8 hexdigest', ''), ('_get_formats', 302, \"def _get_formats(self, cf, uu, vu, media_id):\\n\\n    def get_play_json(cf, timestamp):\\n        data = {'cf': cf, 'ver': '2.2', 'bver': 'firefox44.0', 'format':\\n            'json', 'uu': uu, 'vu': vu, 'ran': compat_str(timestamp)}\\n        self.sign_data(data)\\n        return self._download_json('http://api.letvcloud.com/gpc.php?' +\\n            compat_urllib_parse_urlencode(data), media_id, \\n            'Downloading playJson data for type %s' % cf)\\n    play_json = get_play_json(cf, time.time())\\n    if play_json.get('code') == 10071:\\n        play_json = get_play_json(cf, play_json['timestamp'])\\n    if not play_json.get('data'):\\n        if play_json.get('message'):\\n            raise ExtractorError('Letv cloud said: %s' % play_json[\\n                'message'], expected=True)\\n        elif play_json.get('code'):\\n            raise ExtractorError('Letv cloud returned error %d' % play_json\\n                ['code'], expected=True)\\n        else:\\n            raise ExtractorError('Letv cloud returned an unknwon error')\\n\\n    def b64decode(s):\\n        return base64.b64decode(s.encode('utf-8')).decode('utf-8')\\n    formats = []\\n    for media in play_json['data']['video_info']['media'].values():\\n        play_url = media['play_url']\\n        url = b64decode(play_url['main_url'])\\n        decoded_url = b64decode(url_basename(url))\\n        formats.append({'url': url, 'ext': determine_ext(decoded_url),\\n            'format_id': str_or_none(play_url.get('vtype')), 'format_note':\\n            str_or_none(play_url.get('definition')), 'width': int_or_none(\\n            play_url.get('vwidth')), 'height': int_or_none(play_url.get(\\n            'vheight'))})\\n    return formats\\n\", 'get formats self cf uu vu media id def get play json cf timestamp data cf cf ver 2 2 bver 0 format json uu uu vu vu ran compat str timestamp self sign data data return self download json http api letvcloud com gpc php compat urllib parse urlencode data media id downloading playjson data for type cf play json get play json cf time time if play json get code 10071 play json get play json cf play json timestamp if not play json get data if play json get message raise extractorerror letv cloud said play json message expected true elif play json get code raise extractorerror letv cloud returned error play json code expected true else raise extractorerror letv cloud returned an unknwon error def return encode utf 8 decode utf 8 formats for media in play json data video info media values play url media play url url play url main url decoded url url basename url formats append url url ext determine ext decoded url format id str or none play url get vtype format note str or none play url get definition width int or none play url get vwidth height int or none play url get vheight return formats', ''), ('_real_extract', 350, \"def _real_extract(self, url):\\n    uu_mobj = re.search('uu=([\\\\\\\\w]+)', url)\\n    vu_mobj = re.search('vu=([\\\\\\\\w]+)', url)\\n    if not uu_mobj or not vu_mobj:\\n        raise ExtractorError('Invalid URL: %s' % url, expected=True)\\n    uu = uu_mobj.group(1)\\n    vu = vu_mobj.group(1)\\n    media_id = uu + '_' + vu\\n    formats = self._get_formats('flash', uu, vu, media_id) + self._get_formats(\\n        'html5', uu, vu, media_id)\\n    self._sort_formats(formats)\\n    return {'id': media_id, 'title': 'Video %s' % media_id, 'formats': formats}\\n\", 'real extract self url uu mobj re search uu url vu mobj re search vu url if not uu mobj or not vu mobj raise extractorerror invalid url url expected true uu uu mobj group 1 vu vu mobj group 1 media id uu vu formats self get formats flash uu vu media id self get formats uu vu media id self sort formats formats return id media id title video media id formats formats', '')] [('generate_j2c_identifiers', 21, \"def generate_j2c_identifiers(element, class_ref_name, object_ref_name):\\n    identifiers = []\\n    for field in element.fields:\\n        field_type = field.type\\n        identifiers.append(_REQUEST_FIELD_IDENTIFIER_TEMPLATE.substitute(\\n            java_name=field.java_name, class_ref_name=class_ref_name,\\n            jni_signature=field_type.jni_signature, jni_type=field_type.\\n            jni_type, jni_accessor=field_type.jni_accessor, object_ref_name\\n            =object_ref_name))\\n    return ''.join(identifiers)\\n\", 'generate identifiers element class ref name object ref name identifiers for field in element fields field type field type identifiers append request field identifier template substitute java name field java name class ref name class ref name jni signature field type jni signature jni type field type jni type jni accessor field type jni accessor object ref name object ref name return join identifiers', ''), ('generate_j2c_swap', 42, \"def generate_j2c_swap(element, struct_ref_name):\\n    initialization = []\\n    for field in element.fields:\\n        field_type = field.type\\n        if is_array(field):\\n            template = _ARRAY_J2C_NO_SWAP_TEMPLATE\\n            field_reference_name = field.java_name\\n            c_name = field.name\\n            swap_elements = None\\n            jni_name = None\\n            if field_type.is_swap_needed:\\n                template = _ARRAY_J2C_SWAP_TEMPLATE\\n                host = '%sArrayElements[_i]' % field_reference_name\\n                net = '%s->%s[_i]' % (struct_ref_name, c_name)\\n                swap_elements = field_type.get_host_to_net_function(host, net)\\n            if isinstance(field_type.base_type, Class):\\n                jni_name = field_type.base_type.jni_name\\n                host = '%sArrayElement' % field_reference_name\\n                net = '%s->%s[_i]' % (struct_ref_name, c_name)\\n                swap_elements = field_type.get_host_to_net_function(host, net)\\n                template = _CLASS_ARRAY_J2C_TEMPLATE\\n            initialization.append(template.substitute(field_reference_name=\\n                field_reference_name, field_length_check=\\n                _generate_field_length_check(field), base_type=field_type.\\n                base_type.jni_accessor, jni_base_type=field_type.base_type.\\n                jni_type, struct_reference_name=struct_ref_name, jni_name=\\n                jni_name, c_name=c_name, swap_elements=swap_elements))\\n        elif field_type.is_swap_needed:\\n            host = field.java_name\\n            net = '%s->%s' % (struct_ref_name, field.name)\\n            initialization.append('    %s;' % field_type.\\n                get_host_to_net_function(host, net))\\n        else:\\n            initialization.append('    %s->%s = %s;' % (struct_ref_name,\\n                field.name, field.java_name))\\n    return '\\\\n'.join(initialization)\\n\", 'generate swap element struct ref name initialization for field in element fields field type field type if is array field template array no swap template field reference name field java name name field name swap elements none jni name none if field type is swap needed template array swap template host sarrayelements field reference name net struct ref name name swap elements field type get host to net function host net if isinstance field type base type class jni name field type base type jni name host sarrayelement field reference name net struct ref name name swap elements field type get host to net function host net template class array template initialization append template substitute field reference name field reference name field length check generate field length check field base type field type base type jni accessor jni base type field type base type jni type struct reference name struct ref name jni name jni name name name swap elements swap elements elif field type is swap needed host field java name net struct ref name field name initialization append field type get host to net function host net else initialization append struct ref name field name field java name return join initialization', ''), ('_generate_field_length_check', 120, \"def _generate_field_length_check(field):\\n    field_length = str(field.array_len)\\n    if field.array_len_field:\\n        field_length = field.array_len_field.java_name\\n    if field_length != '0':\\n        return _FIELD_LENGTH_CHECK.substitute(field_length=field_length)\\n    else:\\n        return ''\\n\", 'generate field length check field field length str field array len if field array len field field length field array len field java name if field length 0 return field length check substitute field length field length else return', ''), ('generate_c2j_swap', 138, \"def generate_c2j_swap(element, object_ref_name, struct_ref_name):\\n    msg_java_name = element.java_name_lower\\n    setters = []\\n    for field in element.fields:\\n        field_type = field.type\\n        if is_retval(field):\\n            continue\\n        elif is_array(field):\\n            jni_name = ''\\n            template = (_ARRAY_C2J_SWAP_TEMPLATE if field_type.\\n                is_swap_needed else _ARRAY_C2J_NO_SWAP_TEMPLATE)\\n            if isinstance(field_type.base_type, Class):\\n                template = _ARRAY_C2J_CLASS_SWAP_TEMPLATE\\n                jni_name = field_type.base_type.jni_name\\n            setters.append(template.substitute(field_reference_name=field.\\n                java_name, class_ref_name=msg_java_name, jni_signature=\\n                field_type.jni_signature, jni_type=field_type.jni_type,\\n                jni_name=jni_name, base_type=field_type.base_type.\\n                jni_accessor, field_length=_generate_array_length(field,\\n                struct_ref_name), jni_base_type=field_type.base_type.\\n                jni_type, object_ref_name=object_ref_name, struct_ref_name=\\n                struct_ref_name, net_to_host_function=field_type.\\n                net_to_host_function, c_name=field.name))\\n        elif field_type.is_swap_needed:\\n            template = _SIMPLE_TYPE_FIELD_SETTER_TEMPLATE\\n            jni_name = ''\\n            if isinstance(field_type, Class):\\n                template = _STRUCT_SETTER_TEMPLATE\\n                jni_name = field_type.jni_name\\n            setters.append(template.substitute(java_name=field.java_name,\\n                class_ref_name=msg_java_name, jni_signature=field_type.\\n                jni_signature, jni_name=jni_name, jni_accessor=field_type.\\n                jni_accessor, object_ref_name=object_ref_name,\\n                struct_ref_name=struct_ref_name, net_to_host_function=\\n                field_type.net_to_host_function, c_name=field.name))\\n        else:\\n            setters.append(_SIMPLE_TYPE_NO_SWAP_FIELD_SETTER_TEMPLATE.\\n                substitute(java_name=field.java_name, class_ref_name=\\n                msg_java_name, jni_signature=field_type.jni_signature,\\n                jni_accessor=field_type.jni_accessor, object_ref_name=\\n                object_ref_name, struct_ref_name=struct_ref_name, c_name=\\n                field.name))\\n    return ''.join(setters)\\n\", 'generate swap element object ref name struct ref name msg java name element java name lower setters for field in element fields field type field type if is retval field continue elif is array field jni name template array swap template if field type is swap needed else array no swap template if isinstance field type base type class template array class swap template jni name field type base type jni name setters append template substitute field reference name field java name class ref name msg java name jni signature field type jni signature jni type field type jni type jni name jni name base type field type base type jni accessor field length generate array length field struct ref name jni base type field type base type jni type object ref name object ref name struct ref name struct ref name net to host function field type net to host function name field name elif field type is swap needed template simple type field setter template jni name if isinstance field type class template struct setter template jni name field type jni name setters append template substitute java name field java name class ref name msg java name jni signature field type jni signature jni name jni name jni accessor field type jni accessor object ref name object ref name struct ref name struct ref name net to host function field type net to host function name field name else setters append simple type no swap field setter template substitute java name field java name class ref name msg java name jni signature field type jni signature jni accessor field type jni accessor object ref name object ref name struct ref name struct ref name name field name return join setters', ''), ('_generate_array_length', 260, \"def _generate_array_length(field, struct_ref_name):\\n    if field.array_len_field:\\n        len_field = field.array_len_field\\n        if len_field.type.is_swap_needed:\\n            return '%s(%s->%s)' % (len_field.type.host_to_net_function,\\n                struct_ref_name, len_field.name)\\n        else:\\n            return '%s->%s' % (struct_ref_name, len_field.name)\\n    return field.array_len\\n\", 'generate array length field struct ref name if field array len field len field field array len field if len field type is swap needed return len field type host to net function struct ref name len field name else return struct ref name len field name return field array len', '')] [] [('upgrade', 31, \"def upgrade():\\n    op.alter_column('amphora', 'host_id', new_column_name='compute_id',\\n        existing_type=sa.String(36), nullable=True)\\n\", 'upgrade op alter column amphora host id new column name compute id existing type sa string 36 nullable true', '')] [('test_do_and_ignore', 24, \"def test_do_and_ignore(self):\\n    self.__did_call_function = False\\n\\n    def erroneous_func():\\n        self.__did_call_function = True\\n        raise RuntimeError('IGNORE ME')\\n    do_and_ignore(lambda : erroneous_func())\\n    self.assertTrue(self.__did_call_function)\\n\", 'test do and ignore self self did call function false def erroneous func self did call function true raise runtimeerror ignore me do and ignore lambda erroneous func self asserttrue self did call function', ''), ('test_do_if_match', 34, 'def test_do_if_match(self):\\n    self.__magic_number = None\\n    numbers = [1, 3, 4, 5, 6]\\n    matcher = lambda num: num % 2 == 0\\n\\n    def target_action(item):\\n        self.__magic_number = item\\n    do_if_match(numbers, matcher, target_action)\\n    self.assertEqual(4, self.__magic_number)\\n', 'test do if match self self magic number none numbers 1 3 4 5 6 matcher lambda num num 2 0 def target action item self magic number item do if match numbers matcher target action self assertequal 4 self magic number', ''), ('test_find_dictonary_in_item_in_haystack', 46, \"def test_find_dictonary_in_item_in_haystack(self):\\n    targets = [{'first': 'Sarah', 'last': 'Connor', 'gender': 'female'}, {\\n        'first': 'John', 'last': 'Connor', 'gender': 'male'}, {'first':\\n        'Waldo', 'last': 'Smith', 'gender': 'male'}]\\n    look_for = {'first': 'John', 'last': 'Connor'}\\n    self.assertEqual({'first': 'John', 'last': 'Connor', 'gender': 'male'},\\n        find_dictonary_in(look_for, targets))\\n\", 'test find dictonary in item in haystack self targets first sarah last connor gender female first john last connor gender male first waldo last smith gender male look for first john last connor self assertequal first john last connor gender male find dictonary in look for targets', ''), ('test_find_dictonary_in_item_not_in_haystack', 56, \"def test_find_dictonary_in_item_not_in_haystack(self):\\n    targets = [{'first': 'Sarah', 'last': 'Connor', 'gender': 'female'}, {\\n        'first': 'Waldo', 'last': 'Smith', 'gender': 'male'}]\\n    look_for = {'first': 'John', 'last': 'Connor'}\\n    self.assertEqual(None, find_dictonary_in(look_for, targets))\\n\", 'test find dictonary in item not in haystack self targets first sarah last connor gender female first waldo last smith gender male look for first john last connor self assertequal none find dictonary in look for targets', '')] [('main', 6, \"def main():\\n    global pos\\n    pos += 1\\n    scene = bge.logic.getCurrentScene()\\n    object = scene.objects['Spinal_Cord']\\n    rotation = [0.0, 0.1, 0.0]\\n    object.applyRotation(rotation, False)\\n    object = scene.objects['Skull.Left']\\n    rotation = [0.0, -0.01, 0.0]\\n    object.applyRotation(rotation, False)\\n    object = scene.objects['Servo_Jaw_Drive_shaft']\\n    rotation = [0.0, -0.01, 0.0]\\n    object.applyRotation(rotation, False)\\n    object = scene.objects['MRL_logo']\\n    rotation = [0.01, 0.01, 0.0]\\n    object.applyRotation(rotation, False)\\n    object = scene.objects['Ear_speaker_organic']\\n    rotation = [0.01, 0.01, 0.0]\\n    object.applyRotation(rotation, False)\\n    object = scene.objects['Skull.Right']\\n    rotation = [0.01, 0.01, 0.0]\\n    object.applyRotation(rotation, True)\\n\", 'main global pos pos 1 scene bge logic getcurrentscene object scene objects spinal cord rotation 0 0 0 1 0 0 object applyrotation rotation false object scene objects skull left rotation 0 0 0 01 0 0 object applyrotation rotation false object scene objects servo jaw drive shaft rotation 0 0 0 01 0 0 object applyrotation rotation false object scene objects mrl logo rotation 0 01 0 01 0 0 object applyrotation rotation false object scene objects ear speaker organic rotation 0 01 0 01 0 0 object applyrotation rotation false object scene objects skull right rotation 0 01 0 01 0 0 object applyrotation rotation true', '')] [('test', 13, \"def test():\\n    year = 2013\\n    reform = landais_piketty_saez(tax_benefit_system)\\n    scenario = reform.new_scenario().init_single_entity(axes=[dict(count=3,\\n        max=30000, min=0, name='salaire_de_base')], period=periods.period(\\n        year), parent1=dict(date_naissance=datetime.date(year - 40, 1, 1)))\\n    reform_simulation = scenario.new_simulation()\\n    reform_assiette_csg = reform_simulation.calculate('assiette_csg',\\n        period=year)\\n    reform_impot_revenu_lps = reform_simulation.calculate('impot_revenu_lps',\\n        period=year)\\n    assert_near(-reform_impot_revenu_lps, ((reform_assiette_csg - 10000) * \\n        0.25 / 30000 + 0.25) * reform_assiette_csg, absolute_error_margin=0.01)\\n    reform_simulation.calculate('revenu_disponible', period=year)\\n\", 'test year 2013 reform landais piketty saez tax benefit system scenario reform new scenario init single entity axes dict count 3 max 30000 min 0 name salaire de base period periods period year dict date naissance datetime date year 40 1 1 reform simulation scenario new simulation reform assiette csg reform simulation calculate assiette csg period year reform impot revenu lps reform simulation calculate impot revenu lps period year assert near reform impot revenu lps reform assiette csg 10000 0 25 30000 0 25 reform assiette csg absolute error margin 0 01 reform simulation calculate revenu disponible period year', '')] [('test_agreement_hamming', 8, \"def test_agreement_hamming():\\n    X = np.array([[0.1, 0.3, 0.4, 0.1, 0.3, 0.4], [0.09, 0.28, 0.45, 0.4, \\n        0.3, 0.1]])\\n    e = rafpc.agreement_hamming(3, X, 0, 1)\\n    print('e', e)\\n    assert e[0] > 0.9\\n    assert e[1] < 0.9\\n\", 'test agreement hamming np array 0 1 0 3 0 4 0 1 0 3 0 4 0 09 0 28 0 45 0 4 0 3 0 1 rafpc agreement hamming 3 0 1 print assert 0 0 9 assert 1 0 9', ''), ('test_classifier', 22, 'def test_classifier():\\n    l = rafpc.RandomAgreementFuzzyPatternClassifier(n_protos=1, n_features=2)\\n    X = np.array([[0.1, 0.2, 0.4], [0.15, 0.18, 0.43], [0.2, 0.4, 0.8], [\\n        0.25, 0.42, 0.78]])\\n    y = np.array([0, 0, 1, 1])\\n    l.fit(X, y)\\n    assert [0] == l.predict([[0.9, 1.7, 4.5]])\\n', 'test classifier rafpc randomagreementfuzzypatternclassifier protos 1 features 2 np array 0 1 0 2 0 4 0 15 0 18 0 43 0 2 0 4 0 8 0 25 0 42 0 78 np array 0 0 1 1 fit assert 0 predict 0 9 1 7 4 5', ''), ('test_classifier_single', 44, 'def test_classifier_single():\\n    l = rafpc.RandomAgreementFuzzyPatternClassifier(n_protos=1, n_features=2)\\n    X = np.array([[0.1, 0.2, 0.4], [0.15, 0.18, 0.43], [0.2, 0.4, 0.8], [\\n        0.25, 0.42, 0.78]])\\n    y = np.array([0, 0, 1, 1])\\n    l.fit(X, y)\\n    assert 0 == l.predict([0.9, 1.7, 4.5])\\n', 'test classifier single rafpc randomagreementfuzzypatternclassifier protos 1 features 2 np array 0 1 0 2 0 4 0 15 0 18 0 43 0 2 0 4 0 8 0 25 0 42 0 78 np array 0 0 1 1 fit assert 0 predict 0 9 1 7 4 5', '')] [('install', 31, \"@classmethod\\ndef install(cls):\\n    if cls._replaced_summary_writer:\\n        raise ValueError('FakeSummaryWriter already installed.')\\n    cls._replaced_summary_writer = writer.FileWriter\\n    writer.FileWriter = FakeSummaryWriter\\n    writer_cache.FileWriter = FakeSummaryWriter\\n\", 'install cls if cls replaced summary writer raise valueerror fakesummarywriter already installed cls replaced summary writer writer filewriter writer filewriter fakesummarywriter writer cache filewriter fakesummarywriter', ''), ('uninstall', 39, \"@classmethod\\ndef uninstall(cls):\\n    if not cls._replaced_summary_writer:\\n        raise ValueError('FakeSummaryWriter not installed.')\\n    writer.FileWriter = cls._replaced_summary_writer\\n    writer_cache.FileWriter = cls._replaced_summary_writer\\n    cls._replaced_summary_writer = None\\n\", 'uninstall cls if not cls replaced summary writer raise valueerror fakesummarywriter not installed writer filewriter cls replaced summary writer writer cache filewriter cls replaced summary writer cls replaced summary writer none', ''), ('__init__', 47, 'def __init__(self, logdir, graph=None):\\n    self._logdir = logdir\\n    self._graph = graph\\n    self._summaries = {}\\n    self._added_graphs = []\\n    self._added_meta_graphs = []\\n    self._added_session_logs = []\\n', 'init self logdir graph none self logdir logdir self graph graph self summaries self added graphs self added meta graphs self added session logs', ''), ('summaries', 55, '@property\\ndef summaries(self):\\n    return self._summaries\\n', 'summaries self return self summaries', ''), ('assert_summaries', 59, 'def assert_summaries(self, test_case, expected_logdir=None, expected_graph=\\n    None, expected_summaries=None, expected_added_graphs=None,\\n    expected_added_meta_graphs=None, expected_session_logs=None):\\n    \"\"\"Assert expected items have been added to summary writer.\"\"\"\\n    if expected_logdir is not None:\\n        test_case.assertEqual(expected_logdir, self._logdir)\\n    if expected_graph is not None:\\n        test_case.assertTrue(expected_graph is self._graph)\\n    expected_summaries = expected_summaries or {}\\n    for step in expected_summaries:\\n        test_case.assertTrue(step in self._summaries, msg=\\n            \\'Missing step %s from %s.\\' % (step, self._summaries.keys()))\\n        actual_simple_values = {}\\n        for step_summary in self._summaries[step]:\\n            for v in step_summary.value:\\n                if \\'global_step/sec\\' != v.tag:\\n                    actual_simple_values[v.tag] = v.simple_value\\n        test_case.assertEqual(expected_summaries[step], actual_simple_values)\\n    if expected_added_graphs is not None:\\n        test_case.assertEqual(expected_added_graphs, self._added_graphs)\\n    if expected_added_meta_graphs is not None:\\n        test_case.assertEqual(expected_added_meta_graphs, self.\\n            _added_meta_graphs)\\n    if expected_session_logs is not None:\\n        test_case.assertEqual(expected_session_logs, self._added_session_logs)\\n', 'assert summaries self test case expected logdir none expected graph none expected summaries none expected added graphs none expected added meta graphs none expected session logs none if expected logdir is not none test case assertequal expected logdir self logdir if expected graph is not none test case asserttrue expected graph is self graph expected summaries expected summaries or for step in expected summaries test case asserttrue step in self summaries msg missing step from step self summaries keys actual simple values for step summary in self summaries step for in step summary value if global step sec tag actual simple values tag simple value test case assertequal expected summaries step actual simple values if expected added graphs is not none test case assertequal expected added graphs self added graphs if expected added meta graphs is not none test case assertequal expected added meta graphs self added meta graphs if expected session logs is not none test case assertequal expected session logs self added session logs', 'assert expected items have been added to summary writer'), ('add_summary', 92, 'def add_summary(self, summ, current_global_step):\\n    \"\"\"Add summary.\"\"\"\\n    if isinstance(summ, bytes):\\n        summary_proto = summary_pb2.Summary()\\n        summary_proto.ParseFromString(summ)\\n        summ = summary_proto\\n    if current_global_step in self._summaries:\\n        step_summaries = self._summaries[current_global_step]\\n    else:\\n        step_summaries = []\\n        self._summaries[current_global_step] = step_summaries\\n    step_summaries.append(summ)\\n', 'add summary self summ current global step if isinstance summ bytes summary proto summary summary summary proto parsefromstring summ summ summary proto if current global step in self summaries step summaries self summaries current global step else step summaries self summaries current global step step summaries step summaries append summ', 'add summary'), ('add_graph', 106, 'def add_graph(self, graph, global_step=None, graph_def=None):\\n    \"\"\"Add graph.\"\"\"\\n    if global_step is not None and global_step < 0:\\n        raise ValueError(\\'Invalid global_step %s.\\' % global_step)\\n    if graph_def is not None:\\n        raise ValueError(\\'Unexpected graph_def %s.\\' % graph_def)\\n    self._added_graphs.append(graph)\\n', 'add graph self graph global step none graph def none if global step is not none and global step 0 raise valueerror invalid global step global step if graph def is not none raise valueerror unexpected graph def graph def self added graphs append graph', 'add graph'), ('add_meta_graph', 114, 'def add_meta_graph(self, meta_graph_def, global_step=None):\\n    \"\"\"Add metagraph.\"\"\"\\n    if global_step is not None and global_step < 0:\\n        raise ValueError(\\'Invalid global_step %s.\\' % global_step)\\n    self._added_meta_graphs.append(meta_graph_def)\\n', 'add meta graph self meta graph def global step none if global step is not none and global step 0 raise valueerror invalid global step global step self added meta graphs append meta graph def', 'add metagraph'), ('add_session_log', 121, 'def add_session_log(self, session_log, global_step=None):\\n    self._added_session_logs.append(session_log)\\n', 'add session log self session log global step none self added session logs append session log', ''), ('flush', 125, 'def flush(self):\\n    pass\\n', 'flush self pass', ''), ('reopen', 128, 'def reopen(self):\\n    pass\\n', 'reopen self pass', ''), ('close', 131, 'def close(self):\\n    pass\\n', 'close self pass', '')] [('setup_logging', 37, 'def setup_logging():\\n    \"\"\"\\n    Setup python logging module (destinations like terminal output and file, formatting, etc.)\\n    \"\"\"\\n    logging_default_level = logging.DEBUG\\n    logging_console_enable = True\\n    logging_format = \\'%(levelname)s: %(message)s\\'\\n    logging_file_enable = True\\n    logging_file_dir = \\'./\\'\\n    logging_file_name = SELF + time.strftime(\\'_%Y%b%d-%H%M%S\\') + \\'.log\\'\\n    logging.STAT = logging.DEBUG + 1\\n    logging.addLevelName(logging.STAT, \\'STAT\\')\\n    root_logger = logging.getLogger()\\n    formatter = logging.Formatter(logging_format)\\n    if logging_console_enable:\\n        console = logging.StreamHandler()\\n        console.setFormatter(formatter)\\n        root_logger.addHandler(console)\\n    if logging_file_enable:\\n        filename = os.path.join(logging_file_dir, logging_file_name)\\n        logfile = logging.FileHandler(filename)\\n        logfile.setFormatter(formatter)\\n        root_logger.addHandler(logfile)\\n    atexit.register(logging.shutdown)\\n    root_logger.setLevel(logging_default_level)\\n', 'setup logging logging default level logging debug logging console enable true logging format levelname message logging file enable true logging file dir logging file name self time strftime log logging stat logging debug 1 logging addlevelname logging stat stat root logger logging getlogger formatter logging formatter logging format if logging console enable console logging streamhandler console setformatter formatter root logger addhandler console if logging file enable filename os path join logging file dir logging file name logfile logging filehandler filename logfile setformatter formatter root logger addhandler logfile atexit register logging shutdown root logger setlevel logging default level', 'setup python logging module destinations like terminal output and file formatting etc'), ('random_key', 69, 'def random_key():\\n    \"\"\"Generate random key from pre-defined alphabet \"\"\"\\n    length = random.randint(KEY_MINLEN, KEY_MAXLEN)\\n    return \\'\\'.join(random.choice(KEY_ALPHABET) for _ in range(length))\\n', 'random key length random randint key minlen key maxlen return join random choice key alphabet for in range length', 'generate random key from pre defined alphabet'), ('random_value', 75, 'def random_value(length):\\n    \"\"\"Generate value payload (we care only about length, not contents)\"\"\"\\n    return \\'X\\' * length\\n', 'random value length return length', 'generate value payload we care only about length not contents'), ('log_stats', 80, 'def log_stats(mc):\\n    \"\"\"Print memory related stats of cachelot / memcached to the log\"\"\"\\n    FILTER_STATS = frozenset([\\'cmd_get\\', \\'get_hits\\', \\'get_misses\\',\\n        \\'used_memory\\', \\'total_requested\\', \\'total_served\\', \\'evictions\\',\\n        \\'curr_items\\', \\'total_items\\', \\'evicted_unfetched\\'])\\n    for stat, value in mc.stats():\\n        if stat in FILTER_STATS:\\n            log.log(logging.STAT, \\'%30s:  %s\\' % (stat, value))\\n', 'log stats mc filter stats frozenset cmd get get hits get misses used memory total requested total served evictions curr items total items evicted unfetched for stat value in mc stats if stat in filter stats log log logging stat stat value', 'print memory related stats of cachelot memcached to the log'), ('shell_exec', 95, \"def shell_exec(command):\\n    devnull = open(os.devnull, 'w')\\n    return subprocess.Popen(args=shlex.split(command), stdout=devnull,\\n        stderr=devnull)\\n\", 'shell exec command devnull open os devnull return subprocess popen args shlex split command stdout devnull stderr devnull', ''), ('create_kv_data', 100, 'def create_kv_data(minval, maxval, memlimit):\\n    \"\"\" Generate array of (key, value_length) pairs\\n        Actual values will be generated randomly every time\\n        Total size of all keys and values will be around memlimit\\n    \"\"\"\\n    data = []\\n    data_size = 0\\n    while data_size < memlimit:\\n        k = random_key()\\n        data_size += len(k)\\n        v_length = random.randint(minval, maxval)\\n        data_size += v_length\\n        data.append((k, v_length))\\n    return data\\n', 'create kv data minval maxval memlimit data data size 0 while data size memlimit random key data size len length random randint minval maxval data size length data append length return data', 'generate array of key value length pairs actual values will be generated randomly every time total size of all keys and values will be around memlimit'), ('execute_test', 117, 'def execute_test(mc, minval, maxval, memlimit):\\n    \"\"\" Main test function.\\n        It takes connected memcached client and fills it with random data.\\n        After that there is an attempt to retrieve data back.\\n    \"\"\"\\n    all_keys = []\\n    local_effective_mem = 0\\n    log.info(\\'Fill-in caching server with random data\\')\\n    start_time = time.time()\\n    while local_effective_mem < memlimit:\\n        key = random_key()\\n        local_effective_mem += len(key)\\n        value = random_value(random.randint(minval, maxval))\\n        local_effective_mem += len(value)\\n        mc.set(key, value)\\n        all_keys.append(key)\\n    log.debug(\\'  Took: %.2f sec\\', time.time() - start_time)\\n    log.debug(\\'Checking caching server ...\\')\\n    cache_effective_mem = 0\\n    stored_items = 0\\n    start_time = time.time()\\n    for k in all_keys:\\n        external_val = mc.get(k)\\n        if external_val:\\n            cache_effective_mem += len(k) + len(external_val)\\n            stored_items += 1\\n    log.debug(\\'  Took: %.2f sec\\', time.time() - start_time)\\n    log_stats(mc)\\n    log.info(\\'External effective memory: %.02f/%.02f Mb. (%d/%d items)\\',\\n        toMb(cache_effective_mem), stored_items, toMb(local_effective_mem),\\n        len(all_keys))\\n    return cache_effective_mem, stored_items\\n', 'execute test mc minval maxval memlimit all keys local effective mem 0 log info fill in caching server with random data start time time time while local effective mem memlimit key random key local effective mem len key value random value random randint minval maxval local effective mem len value mc set key value all keys append key log debug took sec time time start time log debug checking caching server cache effective mem 0 stored items 0 start time time time for in all keys external val mc get if external val cache effective mem len len external val stored items 1 log debug took sec time time start time log stats mc log info external effective memory mb items tomb cache effective mem stored items tomb local effective mem len all keys return cache effective mem stored items', 'main test function it takes connected memcached client and fills it with random data after that there is an attempt to retrieve data back'), ('execute_test_n_times', 152, \"def execute_test_n_times(times, mc, minval, maxval, memlimit):\\n    all_effective_mem = []\\n    all_stored_items = []\\n    for run_no in range(1, times + 1):\\n        log.info('Run #%2d', run_no)\\n        effective_mem, stored_items = execute_test(mc, minval, maxval, memlimit\\n            )\\n        all_effective_mem.append(effective_mem)\\n        all_stored_items.append(stored_items)\\n    return all_effective_mem, all_stored_items\\n\", 'execute test times times mc minval maxval memlimit all effective mem all stored items for run no in range 1 times 1 log info run run no effective mem stored items execute test mc minval maxval memlimit all effective mem append effective mem all stored items append stored items return all effective mem all stored items', ''), ('execute_test_for_values_range', 163, 'def execute_test_for_values_range(range_name, minval, maxval):\\n    log.info(\\'\\')\\n    log.info(\\'*** CACHELOT - range \"%s\" [%d:%d]\\' % (range_name, minval, maxval)\\n        )\\n    cache_process = shell_exec(CACHELOTD + \\' -p 11211\\')\\n    time.sleep(1)\\n    mc = memcached.connect_tcp(\\'localhost\\', 11211)\\n    cachelot_eff_mem, cachelot_items = execute_test_n_times(NUM_RUNS, mc,\\n        minval, maxval, MEMORY_LIMIT)\\n    cache_process.terminate()\\n    log.info(\\'-\\' * 60)\\n    log.info(\\'*** MEMCACHED - range \"%s\" [%d:%d]\\' % (range_name, minval,\\n        maxval))\\n    cache_process = shell_exec(MEMCACHED + \\' -p 11212\\')\\n    time.sleep(1)\\n    mc = memcached.connect_tcp(\\'localhost\\', 11212)\\n    memcached_eff_mem, memcached_items = execute_test_n_times(NUM_RUNS, mc,\\n        minval, maxval, MEMORY_LIMIT)\\n    cache_process.terminate()\\n    log.info(\\'-\\' * 60)\\n    log.info(\\'\\\\n\\\\n\\')\\n    r = \\'cachelot: [%s]\\\\n\\' % \\', \\'.join(\\'%.02f\\' % toMb(mem) for mem in\\n        cachelot_eff_mem)\\n    r += \\'memcached: [%s]\\\\n\\' % \\', \\'.join(\\'%.02f\\' % toMb(mem) for mem in\\n        memcached_eff_mem)\\n    r += \\'memDiff: [%s]\\\\n\\' % \\', \\'.join(\\'%.02f\\' % toMb(abs(m1 - m2)) for m1,\\n        m2 in zip(cachelot_eff_mem, memcached_eff_mem))\\n    r += \\'cachelotItems: [%s]\\\\n\\' % \\', \\'.join(\\'%d\\' % i for i in cachelot_items)\\n    r += \\'memcachedItems: [%s]\\\\n\\' % \\', \\'.join(\\'%d\\' % i for i in memcached_items\\n        )\\n    return r\\n', 'execute test for values range range name minval maxval log info log info cachelot range range name minval maxval cache process shell exec cachelotd 11211 time sleep 1 mc memcached connect tcp localhost 11211 cachelot eff mem cachelot items execute test times num runs mc minval maxval memory limit cache process terminate log info 60 log info memcached range range name minval maxval cache process shell exec memcached 11212 time sleep 1 mc memcached connect tcp localhost 11212 memcached eff mem memcached items execute test times num runs mc minval maxval memory limit cache process terminate log info 60 log info cachelot join tomb mem for mem in cachelot eff mem memcached join tomb mem for mem in memcached eff mem memdiff join tomb abs for in zip cachelot eff mem memcached eff mem cachelotitems join for in cachelot items memcacheditems join for in memcached items return', ''), ('main', 191, 'def main():\\n    benchmark_data = {}\\n    for range, minval, maxval in VALUE_RANGES:\\n        benchmark_data[range] = execute_test_for_values_range(range, minval,\\n            maxval)\\n    print(\\'\\\\n\\\\n\\\\n\\')\\n    for range, minval, maxval in VALUE_RANGES:\\n        print(\\'Range %d ~ %d bytes \"%s\"\\' % (minval, maxval, range))\\n        print(benchmark_data[range])\\n        print(\\'\\\\n\\')\\n', 'main benchmark data for range minval maxval in value ranges benchmark data range execute test for values range range minval maxval print for range minval maxval in value ranges print range bytes minval maxval range print benchmark data range print', '')] [] [('__init__', 6, \"def __init__(self, config):\\n    PathChecker.__init__(self, config)\\n    self._max_stack_size = config.get('max_stack_size')\\n    self._action_must_clear_beam = config.get('action_must_clear_beam')\\n\", 'init self config pathchecker init self config self max stack size config get max stack size self action must clear beam config get action must clear beam', ''), ('__call__', 11, 'def __call__(self, path):\\n    \"\"\"Check whether the path should be added to the beam.\\n\\n        Args:\\n            path (ParsePath)\\n        Returns:\\n            boolean\\n        \"\"\"\\n    if self._max_stack_size and len(path.denotation.execution_stack\\n        ) > self._max_stack_size:\\n        return False\\n    if (self._action_must_clear_beam and path.denotation.execution_stack and\\n        path[-1].decision.name[0] == \\'A\\'):\\n        return False\\n    return True\\n', 'call self path if self max stack size and len path denotation execution stack self max stack size return false if self action must clear beam and path denotation execution stack and path 1 decision name 0 return false return true', 'check whether the path should be added to the beam')] [('parent', 25, '@post_generation\\ndef parent(self, create, extracted, **kwargs):\\n    if not create:\\n        return\\n    if extracted:\\n        self.parent = extracted\\n', 'parent self create extracted kwargs if not create return if extracted self parent extracted', '')] [('analyze', 12, 'def analyze(infile):\\n    \"\"\"\\n    Performs the required analysis on an audio file.\\n    Will try to load an existing analysis based on the file\\'s hash;\\n    otherwise, will save the analysis when it\\'s done.\\n    \"\"\"\\n    data = load(infile)\\n    if data is not None:\\n        bpm, key, scale = data\\n        return bpm, Key(key, scale)\\n    bpm = estimate_bpm(infile)\\n    key = estimate_key(infile)\\n    save(infile, bpm, key)\\n    return bpm, key\\n', 'analyze infile data load infile if data is not none bpm key scale data return bpm key key scale bpm estimate bpm infile key estimate key infile save infile bpm key return bpm key', 'performs the required analysis on an audio file will try to load an existing analysis based on the file s hash otherwise will save the analysis when it s done'), ('estimate_bpm', 30, 'def estimate_bpm(infile):\\n    \"\"\"\\n    Estimates the BPM for an audio file.\\n    \"\"\"\\n    pool = Pool()\\n    loader = streaming.MonoLoader(filename=infile)\\n    bt = streaming.RhythmExtractor2013()\\n    bpm_histogram = streaming.BpmHistogramDescriptors()\\n    centroid = streaming.Centroid(range=250)\\n    loader.audio >> bt.signal\\n    bt.bpm >> (pool, \\'bpm\\')\\n    bt.ticks >> None\\n    bt.confidence >> (pool, \\'confidence\\')\\n    bt.estimates >> None\\n    bt.bpmIntervals >> bpm_histogram.bpmIntervals\\n    bpm_histogram.firstPeakBPM >> (pool, \\'bpm_first_peak\\')\\n    bpm_histogram.firstPeakWeight >> None\\n    bpm_histogram.firstPeakSpread >> None\\n    bpm_histogram.secondPeakBPM >> (pool, \\'bpm_second_peak\\')\\n    bpm_histogram.secondPeakWeight >> None\\n    bpm_histogram.secondPeakSpread >> None\\n    bpm_histogram.histogram >> (pool, \\'bpm_histogram\\')\\n    bpm_histogram.histogram >> centroid.array\\n    centroid.centroid >> (pool, \\'bpm_centroid\\')\\n    run(loader)\\n    return pool[\\'bpm\\']\\n', 'estimate bpm infile pool pool loader streaming monoloader filename infile bt streaming bpm histogram streaming bpmhistogramdescriptors centroid streaming centroid range 250 loader audio bt signal bt bpm pool bpm bt ticks none bt confidence pool confidence bt estimates none bt bpmintervals bpm histogram bpmintervals bpm histogram firstpeakbpm pool bpm first peak bpm histogram firstpeakweight none bpm histogram firstpeakspread none bpm histogram secondpeakbpm pool bpm second peak bpm histogram secondpeakweight none bpm histogram secondpeakspread none bpm histogram histogram pool bpm histogram bpm histogram histogram centroid array centroid centroid pool bpm centroid run loader return pool bpm', 'estimates the bpm for an audio file'), ('estimate_key', 61, 'def estimate_key(infile):\\n    \"\"\"\\n    Estimates the key and scale for an audio file.\\n    \"\"\"\\n    loader = streaming.MonoLoader(filename=infile)\\n    framecutter = streaming.FrameCutter()\\n    windowing = streaming.Windowing(type=\\'blackmanharris62\\')\\n    spectrum = streaming.Spectrum()\\n    spectralpeaks = streaming.SpectralPeaks(orderBy=\\'magnitude\\',\\n        magnitudeThreshold=1e-05, minFrequency=40, maxFrequency=5000,\\n        maxPeaks=10000)\\n    pool = Pool()\\n    hpcp = streaming.HPCP()\\n    key = streaming.Key()\\n    loader.audio >> framecutter.signal\\n    framecutter.frame >> windowing.frame >> spectrum.frame\\n    spectrum.spectrum >> spectralpeaks.spectrum\\n    spectralpeaks.magnitudes >> hpcp.magnitudes\\n    spectralpeaks.frequencies >> hpcp.frequencies\\n    hpcp.hpcp >> key.pcp\\n    key.key >> (pool, \\'tonal.key_key\\')\\n    key.scale >> (pool, \\'tonal.key_scale\\')\\n    key.strength >> (pool, \\'tonal.key_strength\\')\\n    run(loader)\\n    return Key(pool[\\'tonal.key_key\\'], pool[\\'tonal.key_scale\\'])\\n', 'estimate key infile loader streaming monoloader filename infile framecutter streaming framecutter windowing streaming windowing type spectrum streaming spectrum spectralpeaks streaming spectralpeaks orderby magnitude magnitudethreshold 05 minfrequency 40 maxfrequency 5000 maxpeaks 10000 pool pool hpcp streaming hpcp key streaming key loader audio framecutter signal framecutter frame windowing frame spectrum frame spectrum spectrum spectralpeaks spectrum spectralpeaks magnitudes hpcp magnitudes spectralpeaks frequencies hpcp frequencies hpcp hpcp key pcp key key pool tonal key key key scale pool tonal key scale key strength pool tonal key strength run loader return key pool tonal key key pool tonal key scale', 'estimates the key and scale for an audio file'), ('estimate_beats', 93, 'def estimate_beats(infile):\\n    \"\"\"\\n    Return the estimated beat onsets in seconds for an audio file.\\n    \"\"\"\\n    audio = standard.MonoLoader(filename=infile)()\\n    bt = standard.BeatTrackerMultiFeature()\\n    beats, confidence = bt(audio)\\n    return beats\\n', 'estimate beats infile audio standard monoloader filename infile bt standard beattrackermultifeature beats confidence bt audio return beats', 'return the estimated beat onsets in seconds for an audio file'), ('estimate_main_band', 103, 'def estimate_main_band(infile):\\n    \"\"\"\\n    Estimate if this is a low, mid, or high track.\\n\\n    Not _really_ sure if this does what I need it to,\\n    but some quick tests looked right.\\n    \"\"\"\\n    loader = streaming.MonoLoader(filename=infile)\\n    framecutter = streaming.FrameCutter()\\n    windowing = streaming.Windowing(type=\\'blackmanharris62\\')\\n    spectrum = streaming.Spectrum()\\n    freqbands = streaming.FrequencyBands(frequencyBands=[0, 250, 750, 4000])\\n    pool = Pool()\\n    loader.audio >> framecutter.signal\\n    framecutter.frame >> windowing.frame >> spectrum.frame\\n    spectrum.spectrum >> freqbands.spectrum\\n    freqbands.bands >> (pool, \\'bands\\')\\n    run(loader)\\n    sums = np.sum(pool[\\'bands\\'], axis=0)\\n    band = np.argmax(sums)\\n    if band == 0:\\n        return \\'low\\'\\n    elif band == 1:\\n        return \\'mid\\'\\n    elif band == 2:\\n        return \\'high\\'\\n', 'estimate main band infile loader streaming monoloader filename infile framecutter streaming framecutter windowing streaming windowing type spectrum streaming spectrum freqbands streaming frequencybands frequencybands 0 250 750 4000 pool pool loader audio framecutter signal framecutter frame windowing frame spectrum frame spectrum spectrum freqbands spectrum freqbands bands pool bands run loader sums np sum pool bands axis 0 band np argmax sums if band 0 return low elif band 1 return mid elif band 2 return high', 'estimate if this is a low mid or high track'), ('estimate_danceability', 134, \"def estimate_danceability(infile):\\n    loader = streaming.MonoLoader(filename=infile)\\n    dance = streaming.Danceability()\\n    pool = Pool()\\n    loader.audio >> dance.signal\\n    dance.danceability >> (pool, 'danceability')\\n    run(loader)\\n    return pool['danceability']\\n\", 'estimate danceability infile loader streaming monoloader filename infile dance streaming danceability pool pool loader audio dance signal dance danceability pool danceability run loader return pool danceability', ''), ('duration', 147, 'def duration(infile):\\n    \"\"\"\\n    Returns the duration of a song in seconds.\\n    \"\"\"\\n    dur = standard.Duration()\\n    audio = standard.MonoLoader(filename=infile)()\\n    duration = dur(audio)\\n    return duration\\n', 'duration infile dur standard duration audio standard monoloader filename infile duration dur audio return duration', 'returns the duration of a song in seconds')] [] [('setUp', 10, \"def setUp(self):\\n    self.source = models.Source.objects.create(name='source')\\n    models.Metric.objects.create(source=self.source, name='a', latest_value\\n        =1.0, last_updated=now())\\n\", 'setup self self source models source objects create name source models metric objects create source self source name latest value 1 0 last updated now', ''), ('test_dashboard_get', 15, \"def test_dashboard_get(self):\\n    url = reverse('dashboard')\\n    response = self.client.get(url)\\n    self.assertEqual(response.status_code, 200)\\n\", 'test dashboard get self url reverse dashboard response self client get url self assertequal response status code 200', ''), ('test_history_get', 20, \"def test_history_get(self):\\n    url = self.source.get_absolute_url()\\n    response = self.client.get(url)\\n    self.assertEqual(response.status_code, 200)\\n    self.assertEqual(len(response.context['graphs']), 1)\\n\", 'test history get self url self source get absolute url response self client get url self assertequal response status code 200 self assertequal len response context graphs 1', '')] [('start_discovery', 73, 'def start_discovery(callback=None):\\n    \"\"\"\\n    Start discovering chromecasts on the network.\\n\\n    This method will start discovering chromecasts on a separate thread. When\\n    a chromecast is discovered, the callback will be called with the\\n    discovered chromecast\\'s zeroconf name. This is the dictionary key to find\\n    the chromecast metadata in listener.services.\\n\\n    This method returns the CastListener object and the zeroconf ServiceBrowser\\n    object. The CastListener object will contain information for the discovered\\n    chromecasts. To stop discovery, call the stop_discovery method with the\\n    ServiceBrowser object.\\n    \"\"\"\\n    listener = CastListener(callback)\\n    service_browser = False\\n    try:\\n        service_browser = zeroconf.ServiceBrowser(zeroconf.Zeroconf(),\\n            \\'_googlecast._tcp.local.\\', listener)\\n    except (zeroconf.BadTypeInNameException, NotImplementedError, OSError,\\n        socket.error, zeroconf.NonUniqueNameException):\\n        pass\\n    return listener, service_browser\\n', 'start discovery callback none listener castlistener callback service browser false try service browser zeroconf servicebrowser zeroconf zeroconf googlecast tcp local listener except zeroconf badtypeinnameexception notimplementederror oserror socket error zeroconf nonuniquenameexception pass return listener service browser', 'start discovering chromecasts on the network'), ('stop_discovery', 103, 'def stop_discovery(browser):\\n    \"\"\"Stop the chromecast discovery thread.\"\"\"\\n    browser.zc.close()\\n', 'stop discovery browser browser zc close', 'stop the chromecast discovery thread'), ('discover_chromecasts', 108, 'def discover_chromecasts(max_devices=None, timeout=DISCOVER_TIMEOUT):\\n    \"\"\" Discover chromecasts on the network. \"\"\"\\n    from threading import Event\\n    browser = False\\n    try:\\n\\n        def callback(name):\\n            \"\"\"Called when zeroconf has discovered a new chromecast.\"\"\"\\n            if max_devices is not None and listener.count >= max_devices:\\n                discover_complete.set()\\n        discover_complete = Event()\\n        listener, browser = start_discovery(callback)\\n        discover_complete.wait(timeout)\\n        return listener.devices\\n    except Exception:\\n        raise\\n    finally:\\n        if browser is not False:\\n            stop_discovery(browser)\\n', 'discover chromecasts max devices none timeout discover timeout from threading import event browser false try def callback name called when zeroconf has discovered new chromecast if max devices is not none and listener count max devices discover complete set discover complete event listener browser start discovery callback discover complete wait timeout return listener devices except exception raise finally if browser is not false stop discovery browser', 'discover chromecasts on the network'), ('__init__', 13, 'def __init__(self, callback=None):\\n    self.services = {}\\n    self.callback = callback\\n', 'init self callback none self services self callback callback', ''), ('count', 17, '@property\\ndef count(self):\\n    \"\"\"Number of discovered cast services.\"\"\"\\n    return len(self.services)\\n', 'count self return len self services', 'number of discovered cast services'), ('devices', 22, '@property\\ndef devices(self):\\n    \"\"\"List of tuples (ip, host) for each discovered device.\"\"\"\\n    return list(self.services.values())\\n', 'devices self return list self services values', 'list of tuples ip host for each discovered device'), ('remove_service', 28, 'def remove_service(self, zconf, typ, name):\\n    \"\"\" Remove a service from the collection. \"\"\"\\n    self.services.pop(name, None)\\n', 'remove service self zconf typ name self services pop name none', 'remove a service from the collection'), ('add_service', 32, 'def add_service(self, zconf, typ, name):\\n    \"\"\" Add a service to the collection. \"\"\"\\n    service = None\\n    tries = 0\\n    while service is None and tries < 4:\\n        try:\\n            service = zconf.get_service_info(typ, name)\\n        except IOError:\\n            break\\n        tries += 1\\n    if not service:\\n        return\\n\\n    def get_value(key):\\n        \"\"\"Retrieve value and decode to UTF-8.\"\"\"\\n        value = service.properties.get(key.encode(\\'utf-8\\'))\\n        if value is None or isinstance(value, str):\\n            return value\\n        return value.decode(\\'utf-8\\')\\n    ips = zconf.cache.entries_with_name(service.server.lower())\\n    host = repr(ips[0]) if ips else service.server\\n    model_name = get_value(\\'md\\')\\n    uuid = get_value(\\'id\\')\\n    friendly_name = get_value(\\'fn\\')\\n    if uuid:\\n        uuid = UUID(uuid)\\n    self.services[name] = host, service.port, uuid, model_name, friendly_name\\n    if self.callback:\\n        self.callback(name)\\n', 'add service self zconf typ name service none tries 0 while service is none and tries 4 try service zconf get service info typ name except ioerror break tries 1 if not service return def get value key retrieve value and decode to utf 8 value service properties get key encode utf 8 if value is none or isinstance value str return value return value decode utf 8 ips zconf cache entries with name service server lower host repr ips 0 if ips else service server model name get value md uuid get value id friendly name get value fn if uuid uuid uuid uuid self services name host service port uuid model name friendly name if self callback self callback name', 'add a service to the collection')] [('__init__', 18, 'def __init__(self):\\n    self.weights = {}\\n    super(AbigaleMod, self).__init__()\\n', 'init self self weights super abigalemod self init', ''), ('get_benchmark', 22, 'def get_benchmark(self):\\n    \"\"\"获取基准标的的净值\"\"\"\\n    benchmark = wind.get_wind_data(\\'AIndexEODPrices\\', \\'s_dq_close\\')[CONFIG.\\n        get(\\'BENCHMARK\\', \\'000905.SH\\')].dropna().truncate(self.strategy.\\n        start_date, self.strategy.end_date)\\n    benchmark /= benchmark.iloc[0]\\n    return benchmark\\n', 'get benchmark self benchmark wind get wind data aindexeodprices dq close config get benchmark 000905 sh dropna truncate self strategy start date self strategy end date benchmark benchmark iloc 0 return benchmark', ''), ('get_exposure', 29, 'def get_exposure(self, position, factor):\\n    \"\"\"计算持仓的因子暴露\"\"\"\\n    factor_name = factor.name\\n    factor_value = factor.get_exposures()\\n    exposure = get_factor_exposure(position, factor_value, benchmark=CONFIG\\n        .BENCHMARK).resample(\\'1m\\').mean().rename(factor_name)\\n    return exposure\\n', 'get exposure self position factor factor name factor name factor value factor get exposures exposure get factor exposure position factor value benchmark config benchmark resample mean rename factor name return exposure', ''), ('on_change_position', 40, 'def on_change_position(self, weight):\\n    self.weights[self.strategy.today] = pd.Series(weight)\\n', 'on change position self weight self weights self strategy today pd series weight', ''), ('on_backtest_finish', 43, \"def on_backtest_finish(self, fund):\\n    weights = pd.DataFrame(self.weights).T\\n    weights.index = pd.to_datetime(weights.index)\\n    benchmark = self.get_benchmark()\\n    relative_net_value = (fund.sheet['net_value'] / benchmark).dropna().rename(\\n        'netValue')\\n    data = {'strategyName': self.strategy.name, 'basic': self.\\n        generate_basic_info(relative_net_value, weights), 'netValues': self\\n        .generate_net_values(relative_net_value), 'styleRisks': self.\\n        generate_style_risks(weights), 'industryRisks': self.\\n        generate_industry_risks(weights), 'factorYields': self.\\n        generate_factor_exposure_yields()}\\n    with open(f'{self.strategy.name}.json', 'w') as f:\\n        json.dump(data, f)\\n    Logger.info(f'回测结果输出到{self.strategy.name}.json')\\n\", 'on backtest finish self fund weights pd dataframe self weights weights index pd to datetime weights index benchmark self get benchmark relative net value fund sheet net value benchmark dropna rename netvalue data strategyname self strategy name basic self generate basic info relative net value weights netvalues self generate net values relative net value stylerisks self generate style risks weights industryrisks self generate industry risks weights factoryields self generate factor exposure yields with open self strategy name json as json dump data logger info self strategy name json', ''), ('generate_basic_info', 60, 'def generate_basic_info(self, net_values, weights):\\n    \"\"\"\\n        生成基本信息，包括：每年的平均收益率、波动率、夏普率、换手率、回撤\\n        \"\"\"\\n    years = sorted(net_values.index.year.unique())\\n    data = []\\n    for year in years:\\n        try:\\n            idx = net_values.index[net_values.index.year == year]\\n            nv = net_values.loc[idx]\\n            idx = weights.index[weights.index.year == year]\\n            w = weights.loc[idx]\\n            data.append(self._basic_info_item(str(year), nv, w))\\n        except IndexError:\\n            continue\\n    data.append(self._basic_info_item(\\'Total\\', net_values, weights))\\n    return data\\n', 'generate basic info self net values weights years sorted net values index year unique data for year in years try idx net values index net values index year year nv net values loc idx idx weights index weights index year year weights loc idx data append self basic info item str year nv except indexerror continue data append self basic info item total net values weights return data', ''), ('generate_net_values', 78, 'def generate_net_values(self, net_values):\\n    \"\"\"\\n        生成净值序列\\n        \"\"\"\\n    return self._series_to_list(net_values)\\n', 'generate net values self net values return self series to list net values', ''), ('generate_factor_exposure_yields', 84, 'def generate_factor_exposure_yields(self):\\n    \"\"\"\\n        因子归因\\n        \"\"\"\\n    position = self.strategy.fund.position.apply(lambda x: x / x.sum(), axis=1)\\n    factor_yields = get_factor_yields()\\n    factor_exposure_yields = {}\\n    for factor in factor_yields.columns:\\n        factor_value = getattr(Factor, factor).get_exposures()\\n        exposure = get_factor_exposure(position, factor_value, benchmark=\\n            CONFIG.BENCHMARK)\\n        yields = exposure * factor_yields[factor]\\n        factor_exposure_yields[factor] = yields.dropna()\\n    industry_exposure_yields = sum(value for key, value in\\n        factor_exposure_yields.items() if key.startswith(\\'Industry\\'))\\n    for key in list(factor_exposure_yields.keys()):\\n        if key.startswith(\\'Industry\\'):\\n            del factor_exposure_yields[key]\\n    factor_exposure_yields[\\'Industry\\'] = industry_exposure_yields\\n    return {key: self._series_to_list(series.cumsum()) for key, series in\\n        factor_exposure_yields.items()}\\n', 'generate factor exposure yields self position self strategy fund position apply lambda sum axis 1 factor yields get factor yields factor exposure yields for factor in factor yields columns factor value getattr factor factor get exposures exposure get factor exposure position factor value benchmark config benchmark yields exposure factor yields factor factor exposure yields factor yields dropna industry exposure yields sum value for key value in factor exposure yields items if key startswith industry for key in list factor exposure yields keys if key startswith industry del factor exposure yields key factor exposure yields industry industry exposure yields return key self series to list series cumsum for key series in factor exposure yields items', ''), ('generate_style_risks', 116, 'def generate_style_risks(self, weights):\\n    \"\"\"\\n        风格暴露\\n        \"\"\"\\n    style_risks = {}\\n    for factor in Factor.get_factors().values():\\n        if factor.name.startswith(\\'Industry\\'):\\n            continue\\n        series = self.get_exposure(weights, factor)\\n        style_risks[factor.name] = self._series_to_list(series)\\n    return style_risks\\n', 'generate style risks self weights style risks for factor in factor get factors values if factor name startswith industry continue series self get exposure weights factor style risks factor name self series to list series return style risks', ''), ('generate_industry_risks', 128, 'def generate_industry_risks(self, weights):\\n    \"\"\"\\n        行业暴露\\n        \"\"\"\\n    industry_risks = {}\\n    for factor in Factor.get_factors().values():\\n        if not factor.name.startswith(\\'Industry\\'):\\n            continue\\n        series = self.get_exposure(weights, factor)\\n        industry_risks[factor.name[8:]] = self._series_to_list(series)\\n    return industry_risks\\n', 'generate industry risks self weights industry risks for factor in factor get factors values if not factor name startswith industry continue series self get exposure weights factor industry risks factor name 8 self series to list series return industry risks', ''), ('_series_to_list', 140, '@staticmethod\\ndef _series_to_list(series):\\n    \"\"\"\\n        把DatetimeIndex的Series转换成Highstocks的序列格式\\n        \"\"\"\\n    return [[int(idx.timestamp() * 1000), value] for idx, value in series.\\n        iteritems()]\\n', 'series to list series return int idx timestamp 1000 value for idx value in series iteritems', 'datetimeindex series highstocks'), ('_basic_info_item', 150, '@staticmethod\\ndef _basic_info_item(name, nv, weights):\\n    \"\"\"\\n        基本统计信息\\n        \"\"\"\\n    rtns = nv.pct_change()\\n    rtn = rtns.mean() * 252\\n    std = rtns.std() * 252 ** 0.5\\n    sharpe = rtn / std\\n    mdd = (1 - nv / nv.cummax()).max()\\n    turnover = weights.fillna(0).diff().abs().sum(1).sum() / (weights.index\\n        [-1] - weights.index[0]).days * 252\\n    return {\\'period\\': name, \\'rtn\\': f\\'{rtn * 100:0.2f}%\\', \\'volatility\\':\\n        f\\'{std * 100:0.2f}%\\', \\'sharpe\\': f\\'{sharpe:0.2f}\\', \\'mdd\\':\\n        f\\'{mdd * 100:0.1f}%\\', \\'turnover\\': f\\'{turnover * 100:0.1f}%\\'}\\n', 'basic info item name nv weights rtns nv pct change rtn rtns mean 252 std rtns std 252 0 5 sharpe rtn std mdd 1 nv nv cummax max turnover weights fillna 0 diff abs sum 1 sum weights index 1 weights index 0 days 252 return period name rtn rtn 100 0 volatility std 100 0 sharpe sharpe 0 mdd mdd 100 0 turnover turnover 100 0', '')] [('testTemplate', 8, \"def testTemplate(self):\\n    logging.debug('testTemplates')\\n    with HoverPy() as hoverpy:\\n        with open('hoverpy/tests/templates/template.json', 'r') as f:\\n            data = f.read()\\n            hoverpy.records(data=data)\\n            records = hoverpy.records()\\n            self.assertTrue('data' in records)\\n            self.assertTrue(isinstance(records['data'], list))\\n\", 'testtemplate self logging debug testtemplates with hoverpy as hoverpy with open hoverpy tests templates template json as data read hoverpy records data data records hoverpy records self asserttrue data in records self asserttrue isinstance records data list', '')] [('get_chart_labels', 36, \"def get_chart_labels(indicators, themes):\\n    chart_labels = {}\\n    for indicator in indicators:\\n        chart_labels[indicator['code']] = _(indicator['title'])\\n        for subindicator in indicator['children']:\\n            chart_labels[subindicator['code']] = _(subindicator['title'])\\n    for theme in themes:\\n        for subtheme in theme['children']:\\n            chart_labels['t' + subtheme['code']] = _(subtheme['title'])\\n    return chart_labels\\n\", 'get chart labels indicators themes chart labels for indicator in indicators chart labels indicator code indicator title for subindicator in indicator children chart labels subindicator code subindicator title for theme in themes for subtheme in theme children chart labels subtheme code subtheme title return chart labels', ''), ('get_map_context', 48, 'def get_map_context(context, year):\\n    \"\"\"\\n    In the map page we pass the level 1 and 2 indicators to the\\n    template with the `indicators` variable. It has the following form:\\n\\n    [\\n        {\\n            \\'code\\': \\'1\\',\\n            \\'title\\': \\'Governance\\',\\n            \\'level\\': 1,\\n            \\'indicators\\': [\\n                {\\n                    \\'code\\': \\'1.1\\',\\n                    \\'title\\': \\'International Framework\\',\\n                    \\'level\\': 2,\\n                },\\n                {\\n                    \\'code\\': \\'1.2\\',\\n                    \\'title\\': \\'National Law\\',\\n                    \\'level\\': 2,\\n                },\\n                ...\\n            ]\\n        },\\n        {\\n            \\'code\\': \\'2\\',\\n            \\'title\\': \\'Availability\\',\\n            \\'level\\': 1,\\n            \\'indicators\\': [\\n                ...\\n            ]\\n        },\\n        ...\\n    ]\\n\\n    We also pass the `themes` variable to build the second menu, with\\n    identical format.\\n\\n    \"\"\"\\n    context[\\'indicators\\'] = data.get_indicators(year)\\n    context[\\'themes\\'] = data.get_themes(year)\\n', 'get map context context year context indicators data get indicators year context themes data get themes year', 'in the map page we pass the level 1 and 2 indicators to the template with the indicators variable it has the following form'), ('get_country_context', 91, 'def get_country_context(context, country_code, year):\\n    \"\"\"In the RTEI by Country page we pass the following variables:\\n\\n    * `available_countries`: a dict with the countries where data is avaiable,\\n        ordered alphabetically by name:\\n\\n        {\\n            \\'CL\\': \\'Chile\\',\\n            \\'NG: \\'Nigeria\\',\\n            ...\\n        }\\n\\n    Only if there is a country selected (via `id` param):\\n\\n    * `country_code`\\n    * `country_name`\\n    * `country_indicators`: full indicators data for the country, in the form:\\n\\n        {\\n            \\'1\\': 74.34,\\n            \\'1.1\\': 23.43,\\n            \\'1.1.1\\': None,\\n            ...\\n        }\\n\\n    * `chart_data`: data necessary to build the C3 chart for just this\\n        particular country, eg:\\n\\n            [{\\n                \\'name\\': \\'Tanzania\\',\\n                \\'index\\': 64.063,\\n                \\'1\\': 16.91,\\n                \\'3\\': 15.92,\\n                \\'2\\': 8.68,\\n                \\'5\\': 8.32,\\n                \\'4\\': 14.24\\n            }]\\n\\n    \"\"\"\\n    if country_code:\\n        country_data = data.get_indicators_for_country(country_code, year)\\n        if not country_data:\\n            raise Http404(_(\\'No data available for this country\\'))\\n        context[\\'country_code\\'] = country_code\\n        context[\\'country_name\\'] = data.get_country_name(country_code)\\n        context[\\'country_indicators\\'] = country_data\\n        for country in data.get_c3_scores_per_country(year):\\n            if country[\\'name\\'] == context[\\'country_name\\']:\\n                chart_data = country\\n                break\\n        context[\\'chart_data\\'] = json.dumps([chart_data])\\n        context[\\'indicators\\'] = data.get_indicators(year)\\n        context[\\'themes\\'] = data.get_themes(year)\\n        context[\\'chart_labels\\'] = json.dumps(get_chart_labels(context[\\n            \\'indicators\\'], context[\\'themes\\']))\\n    context[\\'available_countries\\'] = OrderedDict(sorted({code: data.\\n        get_country_name(code) for code, c in data.get_scores_per_country(\\n        year).iteritems()}.items(), key=lambda t: t[1]))\\n', 'get country context context country code year if country code country data data get indicators for country country code year if not country data raise no data available for this country context country code country code context country name data get country name country code context country indicators country data for country in data get scores per country year if country name context country name chart data country break context chart data json dumps chart data context indicators data get indicators year context themes data get themes year context chart labels json dumps get chart labels context indicators context themes context available countries ordereddict sorted code data get country name code for code in data get scores per country year iteritems items key lambda 1', 'in the rtei by country page we pass the following variables'), ('get_theme_context', 160, \"def get_theme_context(context, year):\\n    context['indicators'] = data.get_indicators(year)\\n    context['themes'] = data.get_themes(year)\\n    context['chart_labels'] = json.dumps(get_chart_labels(context[\\n        'indicators'], context['themes']))\\n\", 'get theme context context year context indicators data get indicators year context themes data get themes year context chart labels json dumps get chart labels context indicators context themes', ''), ('get_context', 175, \"def get_context(self, request):\\n    context = super(RTEIPage, self).get_context(request)\\n    year = request.GET.get('year')\\n    if not year or year not in settings.YEARS:\\n        year = settings.YEARS[-1]\\n    context['year'] = year\\n    context['all_years'] = settings.YEARS\\n    if self.slug == 'map':\\n        get_map_context(context, year)\\n    elif self.slug == 'rtei-country':\\n        get_country_context(context, request.GET.get('id'), year)\\n    elif self.slug == 'rtei-theme':\\n        get_theme_context(context, year)\\n    return context\\n\", 'get context self request context super rteipage self get context request year request get get year if not year or year not in settings years year settings years 1 context year year context all years settings years if self slug map get map context context year elif self slug rtei country get country context context request get get id year elif self slug rtei theme get theme context context year return context', ''), ('get_template', 194, \"def get_template(self, request, *args, **kwargs):\\n    return '%s/%s.html' % (self._meta.app_label, self.slug.replace('-', '_'))\\n\", 'get template self request args kwargs return html self meta app label self slug replace', ''), ('get_context', 214, \"def get_context(self, request):\\n    context = super(RTEIAncillaryPage, self).get_context(request)\\n    if self.slug == 'contact-us':\\n        if request.method == 'POST':\\n            contact_form = ContactForm(request.POST)\\n        else:\\n            contact_form = ContactForm()\\n        context['contact_form'] = contact_form\\n    elif self.slug in ['supporters', 'civil-society-partners']:\\n        context['listing'] = True\\n    return context\\n\", 'get context self request context super rteiancillarypage self get context request if self slug contact us if request method post contact form contactform request post else contact form contactform context contact form contact form elif self slug in supporters civil society partners context listing true return context', ''), ('serve', 226, \"def serve(self, request):\\n    if self.slug == 'contact-us' and request.method == 'POST':\\n        contact_form = ContactForm(request.POST)\\n        if contact_form.is_valid():\\n            try:\\n                contact_form.save()\\n            except SMTPException:\\n                messages.error(request, _(\\n                    'There was a problem submitting your contact details.'))\\n                log.error('Internal Server Error: %s', request.path,\\n                    exc_info=sys.exc_info(), extra={'status_code': 500,\\n                    'request': request})\\n            else:\\n                messages.success(request, _(\\n                    'Your contact details have been submitted.'))\\n            redirect(self.url)\\n    return super(RTEIAncillaryPage, self).serve(request)\\n\", 'serve self request if self slug contact us and request method post contact form contactform request post if contact form is valid try contact form save except smtpexception messages error request there was problem submitting your contact details log error internal server error request path exc info sys exc info extra status code 500 request request else messages success request your contact details have been submitted redirect self url return super rteiancillarypage self serve request', ''), ('get_template', 251, \"def get_template(self, request, *args, **kwargs):\\n    if self.slug == 'contact-us':\\n        return 'rtei/contact_us.html'\\n    elif self.get_parent().slug == 'partners':\\n        return 'rtei/partners.html'\\n    else:\\n        return 'rtei/about.html'\\n\", 'get template self request args kwargs if self slug contact us return rtei contact us html elif self get parent slug partners return rtei partners html else return rtei about html', ''), ('resources', 263, 'def resources(self):\\n    \"\"\"Return a queryset of resource documents, filtered based on query\\n        string args in the passed request.\"\"\"\\n    resources = RteiDocument.objects.filter(is_resource=True)\\n    resources = resources.exclude(collection__id=get_root_collection_id())\\n    return resources\\n', 'resources self resources rteidocument objects filter is resource true resources resources exclude collection id get root collection id return resources', 'return a queryset of resource documents filtered based on query string args in the passed request'), ('years', 273, 'def years(self, resources):\\n    \"\"\"Return a list of years used as values for the `year` property of\\n        document model.\"\"\"\\n    years = resources.order_by(\\'-year\\').exclude(year__exact=\\'\\').values_list(\\n        \\'year\\', flat=True).distinct()\\n    return years\\n', 'years self resources years resources order by year exclude year exact values list year flat true distinct return years', 'return a list of years used as values for the year property of document model'), ('countries', 282, 'def countries(self, resources):\\n    \"\"\"Return a list of counties used as values for the `country` property\\n        of document models.\"\"\"\\n    countries = resources.order_by(\\'country\\').exclude(country__exact=\\'\\'\\n        ).values_list(\\'country\\', flat=True).distinct()\\n    return countries\\n', 'countries self resources countries resources order by country exclude country exact values list country flat true distinct return countries', 'return a list of counties used as values for the country property of document models'), ('collections', 291, 'def collections(self, resources):\\n    \"\"\"Return a list of collections used as values for the `collection`\\n        property of document models.\"\"\"\\n    collections = resources.order_by(\\'collection\\').exclude(collection__id=\\n        get_root_collection_id()).values_list(\\'collection__id\\',\\n        \\'collection__name\\').distinct()\\n    return collections\\n', 'collections self resources collections resources order by collection exclude collection id get root collection id values list collection id collection name distinct return collections', 'return a list of collections used as values for the collection property of document models'), ('get_context', 300, \"def get_context(self, request):\\n    context = super(ResourceIndexPage, self).get_context(request)\\n    resources = self.resources()\\n    context['years'] = self.years(resources)\\n    context['countries'] = self.countries(resources)\\n    context['collections'] = self.collections(resources)\\n    resources_to_display = resources.order_by('-created_at')\\n    has_filter = False\\n    for filter_name in ['year', 'country', 'collection']:\\n        filter_value = request.GET.get(filter_name)\\n        if filter_value:\\n            if filter_value != 'all':\\n                kwargs = {'{0}'.format(filter_name): filter_value}\\n                resources_to_display = resources_to_display.filter(**kwargs)\\n            has_filter = True\\n    page = request.GET.get('page')\\n    paginator = Paginator(resources_to_display, 10)\\n    try:\\n        resources_to_display = paginator.page(page)\\n    except PageNotAnInteger:\\n        resources_to_display = paginator.page(1)\\n    except EmptyPage:\\n        resources_to_display = paginator.page(paginator.num_pages)\\n    context['documents'] = resources_to_display\\n    context['has_filter'] = has_filter\\n    return context\\n\", 'get context self request context super resourceindexpage self get context request resources self resources context years self years resources context countries self countries resources context collections self collections resources resources to display resources order by created at has filter false for filter name in year country collection filter value request get get filter name if filter value if filter value all kwargs 0 format filter name filter value resources to display resources to display filter kwargs has filter true page request get get page paginator paginator resources to display 10 try resources to display paginator page page except pagenotaninteger resources to display paginator page 1 except emptypage resources to display paginator page paginator num pages context documents resources to display context has filter has filter return context', ''), ('blogs', 394, \"@property\\ndef blogs(self):\\n    blogs = BlogPage.objects.live().descendant_of(self)\\n    blogs = blogs.order_by('-date')\\n    return blogs\\n\", 'blogs self blogs blogpage objects live descendant of self blogs blogs order by date return blogs', ''), ('get_context', 404, \"def get_context(self, request):\\n    blogs = self.blogs\\n    tag = request.GET.get('tag')\\n    if tag:\\n        blogs = blogs.filter(tags__name=tag)\\n    page = request.GET.get('page')\\n    paginator = Paginator(blogs, 10)\\n    try:\\n        blogs = paginator.page(page)\\n    except PageNotAnInteger:\\n        blogs = paginator.page(1)\\n    except EmptyPage:\\n        blogs = paginator.page(paginator.num_pages)\\n    context = super(BlogIndexPage, self).get_context(request)\\n    context['blogs'] = blogs\\n    return context\\n\", 'get context self request blogs self blogs tag request get get tag if tag blogs blogs filter tags name tag page request get get page paginator paginator blogs 10 try blogs paginator page page except pagenotaninteger blogs paginator page 1 except emptypage blogs paginator page paginator num pages context super blogindexpage self get context request context blogs blogs return context', ''), ('blog_index', 460, '@property\\ndef blog_index(self):\\n    return self.get_ancestors().type(BlogIndexPage).last()\\n', 'blog index self return self get ancestors type blogindexpage last', '')] [('controlIsOnView', 8, \"def controlIsOnView(self, controlID):\\n    return not xbmc.getCondVisibility('ControlGroup(9000).HasFocus(0)')\\n\", 'controlisonview self controlid return not xbmc getcondvisibility controlgroup 9000 hasfocus 0', ''), ('init', 11, 'def init(self):\\n    self.mode = False\\n', 'init self self mode false', ''), ('updateMode', 14, \"def updateMode(self, controlID):\\n    if self.controlIsOnView(controlID):\\n        self.mode = 'VIEW'\\n    else:\\n        self.mode = None\\n    return self.mode\\n\", 'updatemode self controlid if self controlisonview controlid self mode view else self mode none return self mode', ''), ('getControlDescription', 21, \"def getControlDescription(self, controlID):\\n    old = self.mode\\n    new = self.updateMode(controlID)\\n    if new == None and old != None:\\n        return 'View Options'\\n\", 'getcontroldescription self controlid old self mode new self updatemode controlid if new none and old none return view options', ''), ('getControlText', 45, \"def getControlText(self, controlID):\\n    if not controlID:\\n        return '', ''\\n    if self.slideoutHasFocus():\\n        return self.getSlideoutText(controlID)\\n    text = xbmc.getInfoLabel('System.CurrentControl')\\n    if not text:\\n        return '', ''\\n    compare = text + xbmc.getInfoLabel('ListItem.StartTime'\\n        ) + xbmc.getInfoLabel('ListItem.EndTime')\\n    return text.decode('utf-8'), compare\\n\", 'getcontroltext self controlid if not controlid return if self slideouthasfocus return self getslideouttext controlid text xbmc getinfolabel system currentcontrol if not text return compare text xbmc getinfolabel listitem starttime xbmc getinfolabel listitem endtime return text decode utf 8 compare', ''), ('getItemExtraTexts', 53, \"def getItemExtraTexts(self, controlID):\\n    text = None\\n    if self.controlIsOnView(controlID):\\n        if controlID == 10:\\n            text = guitables.convertTexts(self.winID, self.timelineInfo)\\n        elif controlID == 11 or controlID == 12 or controlID == 13:\\n            info = list(self.nowNextInfo)\\n            if xbmc.getCondVisibility('ListItem.IsRecording'):\\n                info.append(19043)\\n            elif xbmc.getCondVisibility('ListItem.HasTimer'):\\n                info.append(31510)\\n            text = guitables.convertTexts(self.winID, info)\\n    return text\\n\", 'getitemextratexts self controlid text none if self controlisonview controlid if controlid 10 text guitables converttexts self winid self timelineinfo elif controlid 11 or controlid 12 or controlid 13 info list self nownextinfo if xbmc getcondvisibility listitem isrecording info append 19043 elif xbmc getcondvisibility listitem hastimer info append 31510 text guitables converttexts self winid info return text', ''), ('getControlText', 76, \"def getControlText(self, controlID):\\n    if not controlID:\\n        return '', ''\\n    if self.slideoutHasFocus():\\n        return self.getSlideoutText(controlID)\\n    text = '{0}... {1}... {2}'.format(xbmc.getInfoLabel(\\n        'ListItem.ChannelNumber'), xbmc.getInfoLabel('ListItem.Label'),\\n        xbmc.getInfoLabel('ListItem.Title'))\\n    if not text:\\n        return '', ''\\n    compare = text + xbmc.getInfoLabel('ListItem.StartTime'\\n        ) + xbmc.getInfoLabel('ListItem.EndTime')\\n    return text.decode('utf-8'), compare\\n\", 'getcontroltext self controlid if not controlid return if self slideouthasfocus return self getslideouttext controlid text 0 1 2 format xbmc getinfolabel listitem channelnumber xbmc getinfolabel listitem label xbmc getinfolabel listitem title if not text return compare text xbmc getinfolabel listitem starttime xbmc getinfolabel listitem endtime return text decode utf 8 compare', ''), ('getItemExtraTexts', 84, \"def getItemExtraTexts(self, controlID):\\n    text = None\\n    if self.controlIsOnView(controlID):\\n        if controlID == 50:\\n            info = list(self.channelInfo)\\n            if xbmc.getCondVisibility('ListItem.IsRecording'):\\n                info.insert(0, 19043)\\n            text = guitables.convertTexts(self.winID, info)\\n    return text\\n\", 'getitemextratexts self controlid text none if self controlisonview controlid if controlid 50 info list self channelinfo if xbmc getcondvisibility listitem isrecording info insert 0 19043 text guitables converttexts self winid info return text', ''), ('getControlText', 97, \"def getControlText(self, controlID):\\n    if not controlID:\\n        return '', ''\\n    if self.slideoutHasFocus():\\n        return self.getSlideoutText(controlID)\\n    text = xbmc.getInfoLabel('System.CurrentControl')\\n    if not text:\\n        return '', ''\\n    return text.decode('utf-8'), text\\n\", 'getcontroltext self controlid if not controlid return if self slideouthasfocus return self getslideouttext controlid text xbmc getinfolabel system currentcontrol if not text return return text decode utf 8 text', ''), ('getItemExtraTexts', 104, \"def getItemExtraTexts(self, controlID):\\n    text = None\\n    if self.controlIsOnView(controlID):\\n        text = text = guitables.convertTexts(self.winID, (\\n            '$INFO[ListItem.Plot]',))\\n    return text\\n\", 'getitemextratexts self controlid text none if self controlisonview controlid text text guitables converttexts self winid info listitem plot return text', ''), ('getControlText', 119, \"def getControlText(self, controlID):\\n    if not controlID:\\n        return '', ''\\n    if self.slideoutHasFocus():\\n        return self.getSlideoutText(controlID)\\n    text = xbmc.getInfoLabel('System.CurrentControl')\\n    if not text:\\n        return '', ''\\n    compare = text + xbmc.getInfoLabel('ListItem.StartTime'\\n        ) + xbmc.getInfoLabel('ListItem.EndTime')\\n    return text.decode('utf-8'), compare\\n\", 'getcontroltext self controlid if not controlid return if self slideouthasfocus return self getslideouttext controlid text xbmc getinfolabel system currentcontrol if not text return compare text xbmc getinfolabel listitem starttime xbmc getinfolabel listitem endtime return text decode utf 8 compare', ''), ('getItemExtraTexts', 127, 'def getItemExtraTexts(self, controlID):\\n    text = None\\n    if self.controlIsOnView(controlID):\\n        text = guitables.convertTexts(self.winID, self.timerInfo)\\n    return text\\n', 'getitemextratexts self controlid text none if self controlisonview controlid text guitables converttexts self winid self timerinfo return text', ''), ('getControlText', 141, \"def getControlText(self, controlID):\\n    if not controlID:\\n        return '', ''\\n    if self.slideoutHasFocus():\\n        return self.getSlideoutText(controlID)\\n    text = xbmc.getInfoLabel('System.CurrentControl')\\n    if not text:\\n        return '', ''\\n    compare = text + xbmc.getInfoLabel('ListItem.Date')\\n    return text.decode('utf-8'), compare\\n\", 'getcontroltext self controlid if not controlid return if self slideouthasfocus return self getslideouttext controlid text xbmc getinfolabel system currentcontrol if not text return compare text xbmc getinfolabel listitem date return text decode utf 8 compare', ''), ('getItemExtraTexts', 149, \"def getItemExtraTexts(self, controlID):\\n    text = None\\n    if self.controlIsOnView(controlID):\\n        info = list(self.searchInfo)\\n        if xbmc.getCondVisibility('ListItem.IsRecording'):\\n            info.append(19043)\\n        elif xbmc.getCondVisibility('ListItem.HasTimer'):\\n            info.append(31510)\\n        text = guitables.convertTexts(self.winID, info)\\n    return text\\n\", 'getitemextratexts self controlid text none if self controlisonview controlid info list self searchinfo if xbmc getcondvisibility listitem isrecording info append 19043 elif xbmc getcondvisibility listitem hastimer info append 31510 text guitables converttexts self winid info return text', ''), ('controlIsOnView', 184, 'def controlIsOnView(self, controlID):\\n    return controlID > 9 and controlID < 18\\n', 'controlisonview self controlid return controlid 9 and controlid 18', ''), ('getControlText', 187, \"def getControlText(self, controlID):\\n    if not controlID:\\n        return '', ''\\n    text = None\\n    if controlID == 11 or controlID == 12:\\n        text = '{0}... {1}... {2}'.format(xbmc.getInfoLabel(\\n            'ListItem.ChannelNumber'), xbmc.getInfoLabel('ListItem.Label'),\\n            xbmc.getInfoLabel('ListItem.Title'))\\n    else:\\n        text = xbmc.getInfoLabel('System.CurrentControl')\\n    if not text:\\n        return '', ''\\n    compare = text + xbmc.getInfoLabel('ListItem.StartTime'\\n        ) + xbmc.getInfoLabel('ListItem.EndTime')\\n    return text.decode('utf-8'), compare\\n\", 'getcontroltext self controlid if not controlid return text none if controlid 11 or controlid 12 text 0 1 2 format xbmc getinfolabel listitem channelnumber xbmc getinfolabel listitem label xbmc getinfolabel listitem title else text xbmc getinfolabel system currentcontrol if not text return compare text xbmc getinfolabel listitem starttime xbmc getinfolabel listitem endtime return text decode utf 8 compare', ''), ('getItemExtraTexts', 198, 'def getItemExtraTexts(self, controlID):\\n    text = None\\n    if self.controlIsOnView(controlID):\\n        if controlID == 10:\\n            text = guitables.convertTexts(self.winID, self.timelineInfo)\\n        elif controlID == 11 or controlID == 12:\\n            text = guitables.convertTexts(self.winID, self.channelInfo)\\n        elif controlID == 16:\\n            text = guitables.convertTexts(self.winID, self.nowNextInfo)\\n    return text\\n', 'getitemextratexts self controlid text none if self controlisonview controlid if controlid 10 text guitables converttexts self winid self timelineinfo elif controlid 11 or controlid 12 text guitables converttexts self winid self channelinfo elif controlid 16 text guitables converttexts self winid self nownextinfo return text', '')] [('post', 17, '@require_POST\\ndef post(request, template_name=\\'comments/comments.html\\'):\\n    \"\"\"Post a comment\"\"\"\\n    data = request.POST.copy()\\n    if request.user.is_authenticated():\\n        if not data.get(\\'name\\', \\'\\'):\\n            data[\\'name\\'] = request.user.get_full_name(\\n                ) or request.user.username\\n        if not data.get(\\'email\\', \\'\\'):\\n            data[\\'email\\'] = request.user.email\\n    ctype = data.get(\\'content_type\\')\\n    object_pk = data.get(\\'object_pk\\')\\n    model = utils.get_model(ctype)\\n    target = model._default_manager.get(pk=object_pk)\\n    form = comments.get_form()(target, data=data)\\n    if not form.is_valid():\\n        context_data = {\\'object\\': target, \\'form\\': form}\\n        return render(request, template_name, context=context_data)\\n    comment = form.get_comment_object()\\n    comment.ip_address = request.META.get(\\'REMOTE_ADDR\\', None)\\n    if request.user.is_authenticated():\\n        comment.user = request.user\\n    comments.signals.comment_will_be_posted.send(sender=comment.__class__,\\n        comment=comment, request=request)\\n    comment.is_removed = False\\n    comment.save()\\n    comments.signals.comment_was_posted.send(sender=comment.__class__,\\n        comment=comment, request=request)\\n    context_data = {\\'object\\': target, \\'form\\': form}\\n    return render(request, template_name, context=context_data)\\n', 'post request template name comments comments html data request post copy if request user is authenticated if not data get name data name request user get full name or request user username if not data get email data email request user email ctype data get content type object pk data get object pk model utils get model ctype target model default manager get pk object pk form comments get form target data data if not form is valid context data object target form form return render request template name context context data comment form get comment object comment ip address request meta get remote addr none if request user is authenticated comment user request user comments signals comment will be posted send sender comment class comment comment request request comment is removed false comment save comments signals comment was posted send sender comment class comment comment request request context data object target form form return render request template name context context data', 'post a comment'), ('delete', 76, '@require_POST\\ndef delete(request):\\n    \"\"\"Deletes a comment\"\"\"\\n    ajax_response = {\\'rc\\': 0, \\'response\\': \\'ok\\'}\\n    comments_s = comments.get_model().objects.filter(pk__in=request.POST.\\n        getlist(\\'comment_id\\'), site__pk=settings.SITE_ID, is_removed=False,\\n        user_id=request.user.id)\\n    if not comments_s:\\n        if request.is_ajax():\\n            ajax_response = {\\'rc\\': 1, \\'response\\': \\'Object does not exist.\\'}\\n            return HttpResponse(json.dumps(ajax_response))\\n        raise ObjectDoesNotExist()\\n    for comment in comments_s:\\n        if comment.user == request.user:\\n            flag, created = comments.models.CommentFlag.objects.get_or_create(\\n                comment=comment, user=request.user, flag=comments.models.\\n                CommentFlag.MODERATOR_DELETION)\\n            comment.is_removed = True\\n            comment.save()\\n            comments.signals.comment_was_flagged.send(sender=comment.\\n                __class__, comment=comment, flag=flag, created=created,\\n                request=request)\\n    return HttpResponse(json.dumps(ajax_response))\\n', 'delete request ajax response rc 0 response ok comments comments get model objects filter pk in request post getlist comment id site pk settings site id is removed false user id request user id if not comments if request is ajax ajax response rc 1 response object does not exist return httpresponse json dumps ajax response raise objectdoesnotexist for comment in comments if comment user request user flag created comments models commentflag objects get or create comment comment user request user flag comments models commentflag moderator deletion comment is removed true comment save comments signals comment was flagged send sender comment class comment comment flag flag created created request request return httpresponse json dumps ajax response', 'deletes a comment')] [('export_doc', 10, 'def export_doc(doc):\\n    export_to_files([[doc.doctype, doc.name]])\\n', 'export doc doc export to files doc doctype doc name', ''), ('export_to_files', 13, 'def export_to_files(record_list=None, record_module=None, verbose=0,\\n    create_init=None):\\n    \"\"\"\\n\\t\\tExport record_list to files. record_list is a list of lists ([doctype],[docname] )  ,\\n\\t\"\"\"\\n    if frappe.flags.in_import:\\n        return\\n    if record_list:\\n        for record in record_list:\\n            write_document_file(frappe.get_doc(record[0], record[1]),\\n                record_module, create_init=create_init)\\n', 'export to files record list none record module none verbose 0 create init none if frappe flags in import return if record list for record in record list write document file frappe get doc record 0 record 1 record module create init create init', 'export record list to files record list is a list of lists doctype docname'), ('write_document_file', 24, \"def write_document_file(doc, record_module=None, create_init=True):\\n    newdoc = doc.as_dict(no_nulls=True)\\n    for df in doc.meta.get_table_fields():\\n        for d in newdoc.get(df.fieldname):\\n            for fieldname in frappe.model.default_fields:\\n                if fieldname in d:\\n                    del d[fieldname]\\n    module = record_module or get_module_name(doc)\\n    folder = create_folder(module, doc.doctype, doc.name, create_init)\\n    fname = scrub(doc.name)\\n    with open(os.path.join(folder, fname + '.json'), 'w+') as txtfile:\\n        txtfile.write(frappe.as_json(newdoc))\\n\", 'write document file doc record module none create init true newdoc doc as dict no nulls true for df in doc meta get table fields for in newdoc get df fieldname for fieldname in frappe model default fields if fieldname in del fieldname module record module or get module name doc folder create folder module doc doctype doc name create init fname scrub doc name with open os path join folder fname json as txtfile txtfile write frappe as json newdoc', ''), ('get_module_name', 44, \"def get_module_name(doc):\\n    if doc.doctype == 'Module Def':\\n        module = doc.name\\n    elif doc.doctype == 'Workflow':\\n        module = frappe.db.get_value('DocType', doc.document_type, 'module')\\n    elif hasattr(doc, 'module'):\\n        module = doc.module\\n    else:\\n        module = frappe.db.get_value('DocType', doc.doctype, 'module')\\n    return module\\n\", 'get module name doc if doc doctype module def module doc name elif doc doctype workflow module frappe db get value doctype doc document type module elif hasattr doc module module doc module else module frappe db get value doctype doc doctype module return module', ''), ('create_folder', 56, 'def create_folder(module, dt, dn, create_init):\\n    module_path = get_module_path(module)\\n    dt, dn = scrub_dt_dn(dt, dn)\\n    folder = os.path.join(module_path, dt, dn)\\n    frappe.create_folder(folder)\\n    if create_init:\\n        create_init_py(module_path, dt, dn)\\n    return folder\\n', 'create folder module dt dn create init module path get module path module dt dn scrub dt dn dt dn folder os path join module path dt dn frappe create folder folder if create init create init py module path dt dn return folder', ''), ('create_init_py', 72, \"def create_init_py(module_path, dt, dn):\\n\\n    def create_if_not_exists(path):\\n        initpy = os.path.join(path, '__init__.py')\\n        if not os.path.exists(initpy):\\n            open(initpy, 'w').close()\\n    create_if_not_exists(os.path.join(module_path))\\n    create_if_not_exists(os.path.join(module_path, dt))\\n    create_if_not_exists(os.path.join(module_path, dt, dn))\\n\", 'create init py module path dt dn def create if not exists path initpy os path join path init py if not os path exists initpy open initpy close create if not exists os path join module path create if not exists os path join module path dt create if not exists os path join module path dt dn', '')] [('generate_key', 9, 'def generate_key(path):\\n    chars = \\'abcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*(-_=+)\\'\\n    key = get_random_string(50, chars)\\n    with open(key_path, \\'w\\') as file:\\n        file.write(\"SECRET_KEY = \\'{0}\\'\\\\n\".format(key))\\n', 'generate key path chars key get random string 50 chars with open key path as file file write secret key 0 format key', '')] [('suite', 109, 'def suite():\\n    suite1 = unittest.makeSuite(testcase_getAttrib)\\n    return unittest.TestSuite(suite1)\\n', 'suite unittest makesuite testcase getattrib return unittest testsuite', ''), ('setUp', 51, 'def setUp(self):\\n    hresult, self.hcontext = SCardEstablishContext(SCARD_SCOPE_USER)\\n    self.assertEqual(hresult, 0)\\n    hresult, self.readers = SCardListReaders(self.hcontext, [])\\n    self.assertEqual(hresult, 0)\\n', 'setup self hresult self hcontext scardestablishcontext scard scope user self assertequal hresult 0 hresult self readers scardlistreaders self hcontext self assertequal hresult 0', ''), ('tearDown', 57, 'def tearDown(self):\\n    hresult = SCardReleaseContext(self.hcontext)\\n    self.assertEqual(hresult, 0)\\n', 'teardown self hresult scardreleasecontext self hcontext self assertequal hresult 0', ''), ('_getAttrib', 61, \"def _getAttrib(self, r):\\n    if r < len(expectedATRs) and [] != expectedATRs[r]:\\n        hresult, hcard, dwActiveProtocol = SCardConnect(self.hcontext, self\\n            .readers[r], SCARD_SHARE_SHARED, SCARD_PROTOCOL_T0 |\\n            SCARD_PROTOCOL_T1)\\n        self.assertEqual(hresult, 0)\\n        try:\\n            hresult, reader, state, protocol, atr = SCardStatus(hcard)\\n            self.assertEqual(hresult, 0)\\n            self.assertEqual(reader, expectedReaders[r])\\n            self.assertEqual(atr, expectedATRs[r])\\n            if 'SCARD_ATTR_ATR_STRING' in scard.__dict__:\\n                hresult, attrib = SCardGetAttrib(hcard, SCARD_ATTR_ATR_STRING)\\n                self.assertEqual(hresult, 0)\\n                self.assertEqual(expectedATRs[r], attrib)\\n            if 'winscard' == resourceManager:\\n                hresult, attrib = SCardGetAttrib(hcard,\\n                    SCARD_ATTR_DEVICE_SYSTEM_NAME_A)\\n                self.assertEqual(hresult, 0)\\n                trimmedAttrib = attrib[:-1]\\n                self.assertEqual(expectedReaders[r], apply(struct.pack, [\\n                    '<' + 'B' * len(trimmedAttrib)] + trimmedAttrib))\\n        finally:\\n            hresult = SCardDisconnect(hcard, SCARD_UNPOWER_CARD)\\n            self.assertEqual(hresult, 0)\\n\", 'getattrib self if len expectedatrs and expectedatrs hresult hcard dwactiveprotocol scardconnect self hcontext self readers scard share shared scard protocol scard protocol self assertequal hresult 0 try hresult reader state protocol atr scardstatus hcard self assertequal hresult 0 self assertequal reader expectedreaders self assertequal atr expectedatrs if scard attr atr string in scard dict hresult attrib scardgetattrib hcard scard attr atr string self assertequal hresult 0 self assertequal expectedatrs attrib if winscard resourcemanager hresult attrib scardgetattrib hcard scard attr device system name self assertequal hresult 0 trimmedattrib attrib 1 self assertequal expectedreaders apply struct pack len trimmedattrib trimmedattrib finally hresult scarddisconnect hcard scard unpower card self assertequal hresult 0', ''), ('test_getATR0', 96, 'def test_getATR0(self):\\n    testcase_getAttrib._getAttrib(self, 0)\\n', 'test self testcase getattrib getattrib self 0', ''), ('test_getATR1', 99, 'def test_getATR1(self):\\n    testcase_getAttrib._getAttrib(self, 1)\\n', 'test self testcase getattrib getattrib self 1', ''), ('test_getATR3', 102, 'def test_getATR3(self):\\n    testcase_getAttrib._getAttrib(self, 2)\\n', 'test self testcase getattrib getattrib self 2', ''), ('test_getATR4', 105, 'def test_getATR4(self):\\n    testcase_getAttrib._getAttrib(self, 3)\\n', 'test self testcase getattrib getattrib self 3', '')] [('forest_aad_loss_linear', 5, 'def forest_aad_loss_linear(w, xi, yi, qval, in_constr_set=None, x_tau=None,\\n    Ca=1.0, Cn=1.0, Cx=1.0, withprior=False, w_prior=None, sigma2=1.0):\\n    \"\"\"\\n    Computes AAD loss:\\n        for square_slack:\\n            ( score_loss + 1/(2*sigma2) * (w - w_prior)^2 )\\n        else:\\n            ( score_loss + 1/(2*sigma2) * (w - w_prior)^2 )\\n\\n    :param w: numpy.array\\n        parameter vector with both weights and slack variables\\n    :param xi: csr_matrix\\n    :param yi: numpy.array\\n    :param qval: float\\n        tau-th quantile value\\n    :param Ca: float\\n    :param Cn: float\\n    :param Cx: float\\n    :param withprior: boolean\\n    :param w_prior: numpy.array\\n    :param w_old: numpy.array\\n    :param sigma2: float\\n    :param square_slack: boolean\\n    :return:\\n    \"\"\"\\n    s = xi.dot(w)\\n    loss_a = 0\\n    loss_n = 0\\n    n_anom = 0\\n    n_noml = 0\\n    tau_rel_loss = None\\n    if x_tau is not None:\\n        tau_rel_loss = x_tau.dot(w)\\n    for i in range(len(yi)):\\n        lbl = yi[i]\\n        if lbl == 1:\\n            n_anom += 1\\n        else:\\n            n_noml += 1\\n        if lbl == 1 and s[i] < qval:\\n            loss_a += Ca * (qval - s[i])\\n        elif lbl == 0 and s[i] >= qval:\\n            loss_n += Cn * (s[i] - qval)\\n        else:\\n            pass\\n        if tau_rel_loss is not None and (in_constr_set is None or \\n            in_constr_set[i] == 1):\\n            tau_val = tau_rel_loss[0]\\n            if lbl == 1 and s[i] < tau_val:\\n                loss_a += Cx * (tau_val - s[i])\\n            elif lbl == 0 and s[i] >= tau_val:\\n                loss_n += Cx * (s[i] - tau_val)\\n            else:\\n                pass\\n    loss = loss_a / max(1, n_anom) + loss_n / max(1, n_noml)\\n    if withprior and w_prior is not None:\\n        w_diff = w - w_prior\\n        loss += 1 / (2 * sigma2) * w_diff.dot(w_diff)\\n    return loss\\n', 'forest aad loss linear xi yi qval in constr set none tau none ca 1 0 cn 1 0 cx 1 0 withprior false prior none 1 0 xi dot loss 0 loss 0 anom 0 noml 0 tau rel loss none if tau is not none tau rel loss tau dot for in range len yi lbl yi if lbl 1 anom 1 else noml 1 if lbl 1 and qval loss ca qval elif lbl 0 and qval loss cn qval else pass if tau rel loss is not none and in constr set is none or in constr set 1 tau val tau rel loss 0 if lbl 1 and tau val loss cx tau val elif lbl 0 and tau val loss cx tau val else pass loss loss max 1 anom loss max 1 noml if withprior and prior is not none diff prior loss 1 2 diff dot diff return loss', 'computes aad loss'), ('forest_aad_loss_gradient_linear', 79, 'def forest_aad_loss_gradient_linear(w, xi, yi, qval, in_constr_set=None,\\n    x_tau=None, Ca=1.0, Cn=1.0, Cx=1.0, withprior=False, w_prior=None,\\n    sigma2=1.0):\\n    \"\"\"\\n    Computes jacobian of AAD loss:\\n        for square_slack:\\n            jacobian( score_loss + 1/(2*sigma2) * (w - w_prior)^2 )\\n        else:\\n            jacobian( score_loss + 1/(2*sigma2) * (w - w_prior)^2 )\\n    \"\"\"\\n    m = ncol(xi)\\n    grad = np.zeros(m, dtype=float)\\n    s = xi.dot(w)\\n    loss_a = rep(0, m)\\n    loss_n = rep(0, m)\\n    n_anom = 0\\n    n_noml = 0\\n    anom_idxs = list()\\n    noml_idxs = list()\\n    anom_tau_idxs = list()\\n    noml_tau_idxs = list()\\n    tau_score = None\\n    if x_tau is not None:\\n        tau_score = x_tau.dot(w)\\n    for i in range(len(yi)):\\n        lbl = yi[i]\\n        if lbl == 1:\\n            n_anom += 1\\n        else:\\n            n_noml += 1\\n        if lbl == 1 and s[i] < qval:\\n            anom_idxs.append(i)\\n        elif lbl == 0 and s[i] >= qval:\\n            noml_idxs.append(i)\\n        else:\\n            pass\\n        if x_tau is not None and (in_constr_set is None or in_constr_set[i] ==\\n            1):\\n            tau_val = tau_score[0]\\n            if lbl == 1 and s[i] < tau_val:\\n                anom_tau_idxs.append(i)\\n            elif lbl == 0 and s[i] >= tau_val:\\n                noml_tau_idxs.append(i)\\n            else:\\n                pass\\n    anom_idxs = np.array(anom_idxs, dtype=int)\\n    noml_idxs = np.array(noml_idxs, dtype=int)\\n    anom_tau_idxs = np.array(anom_tau_idxs, dtype=int)\\n    noml_tau_idxs = np.array(noml_tau_idxs, dtype=int)\\n    if len(anom_idxs) > 0:\\n        loss_a[:] = -Ca * np.sum(xi[anom_idxs], axis=0)\\n    if len(anom_tau_idxs) > 0:\\n        loss_a[:] = loss_a + Cx * (len(anom_tau_idxs) * x_tau - np.sum(xi[\\n            anom_tau_idxs], axis=0))\\n    if len(noml_idxs) > 0:\\n        loss_n[:] = Cn * np.sum(xi[noml_idxs], axis=0)\\n    if len(noml_tau_idxs) > 0:\\n        loss_n[:] = loss_n + Cx * (np.sum(xi[noml_tau_idxs], axis=0) - len(\\n            noml_tau_idxs) * x_tau)\\n    grad[0:m] = loss_a / max(1, n_anom) + loss_n / max(1, n_noml)\\n    if withprior and w_prior is not None:\\n        w_diff = w - w_prior\\n        grad[0:m] += 1 / sigma2 * w_diff\\n    return grad\\n', 'forest aad loss gradient linear xi yi qval in constr set none tau none ca 1 0 cn 1 0 cx 1 0 withprior false prior none 1 0 ncol xi grad np zeros dtype float xi dot loss rep 0 loss rep 0 anom 0 noml 0 anom idxs list noml idxs list anom tau idxs list noml tau idxs list tau score none if tau is not none tau score tau dot for in range len yi lbl yi if lbl 1 anom 1 else noml 1 if lbl 1 and qval anom idxs append elif lbl 0 and qval noml idxs append else pass if tau is not none and in constr set is none or in constr set 1 tau val tau score 0 if lbl 1 and tau val anom tau idxs append elif lbl 0 and tau val noml tau idxs append else pass anom idxs np array anom idxs dtype int noml idxs np array noml idxs dtype int anom tau idxs np array anom tau idxs dtype int noml tau idxs np array noml tau idxs dtype int if len anom idxs 0 loss ca np sum xi anom idxs axis 0 if len anom tau idxs 0 loss loss cx len anom tau idxs tau np sum xi anom tau idxs axis 0 if len noml idxs 0 loss cn np sum xi noml idxs axis 0 if len noml tau idxs 0 loss loss cx np sum xi noml tau idxs axis 0 len noml tau idxs tau grad 0 loss max 1 anom loss max 1 noml if withprior and prior is not none diff prior grad 0 1 diff return grad', 'computes jacobian of aad loss'), ('forest_aad_loss_exp', 170, 'def forest_aad_loss_exp(w, xi, yi, qval, in_constr_set=None, x_tau=None, Ca\\n    =1.0, Cn=1.0, Cx=1.0, withprior=False, w_prior=None, sigma2=1.0):\\n    \"\"\"\\n    Computes AAD loss:\\n        for square_slack:\\n            ( score_loss + 1/(2*sigma2) * (w - w_prior)^2 )\\n        else:\\n            ( score_loss + 1/(2*sigma2) * (w - w_prior)^2 )\\n\\n    :param w: numpy.array\\n        parameter vector with both weights and slack variables\\n    :param xi: csr_matrix\\n    :param yi: numpy.array\\n    :param qval: float\\n        tau-th quantile value\\n    :param in_constr_set: list of int\\n        indicators 0/1 whether to include in constraint set or not \\n    :param Ca: float\\n    :param Cn: float\\n    :param Cx: float\\n    :param withprior: boolean\\n    :param w_prior: numpy.array\\n    :param w_old: numpy.array\\n    :param sigma2: float\\n    :param square_slack: boolean\\n    :return:\\n    \"\"\"\\n    loss_a = 0\\n    loss_n = 0\\n    n_anom = 0\\n    n_noml = 0\\n    vals = xi.dot(w)\\n    tau_rel_loss = None\\n    if x_tau is not None:\\n        tau_rel_loss = x_tau.dot(w)\\n    for i in range(len(yi)):\\n        if yi[i] == 1:\\n            n_anom += 1\\n        else:\\n            n_noml += 1\\n        if yi[i] == 1 and vals[i] < qval:\\n            loss_a = loss_a + Ca * np.exp(qval - vals[i])\\n        elif yi[i] == 0 and vals[i] >= qval:\\n            loss_n = loss_n + Cn * np.exp(vals[i] - qval)\\n        else:\\n            pass\\n        if tau_rel_loss is not None and (in_constr_set is None or \\n            in_constr_set[i] == 1):\\n            tau_val = tau_rel_loss[0]\\n            if yi[i] == 1 and vals[i] < tau_val:\\n                loss_a += Cx * (tau_val - vals[i])\\n            elif yi[i] == 0 and vals[i] >= tau_val:\\n                loss_n += Cx * (vals[i] - tau_val)\\n            else:\\n                pass\\n    loss = loss_a / max(1, n_anom) + loss_n / max(1, n_noml)\\n    if withprior and w_prior is not None:\\n        w_diff = w - w_prior\\n        loss += 1 / (2 * sigma2) * w_diff.dot(w_diff)\\n    return loss\\n', 'forest aad loss exp xi yi qval in constr set none tau none ca 1 0 cn 1 0 cx 1 0 withprior false prior none 1 0 loss 0 loss 0 anom 0 noml 0 vals xi dot tau rel loss none if tau is not none tau rel loss tau dot for in range len yi if yi 1 anom 1 else noml 1 if yi 1 and vals qval loss loss ca np exp qval vals elif yi 0 and vals qval loss loss cn np exp vals qval else pass if tau rel loss is not none and in constr set is none or in constr set 1 tau val tau rel loss 0 if yi 1 and vals tau val loss cx tau val vals elif yi 0 and vals tau val loss cx vals tau val else pass loss loss max 1 anom loss max 1 noml if withprior and prior is not none diff prior loss 1 2 diff dot diff return loss', 'computes aad loss'), ('forest_aad_loss_gradient_exp', 243, 'def forest_aad_loss_gradient_exp(w, xi, yi, qval, in_constr_set=None, x_tau\\n    =None, Ca=1.0, Cn=1.0, Cx=1.0, withprior=False, w_prior=None, sigma2=1.0):\\n    \"\"\"\\n    Computes jacobian of AAD loss:\\n        for square_slack:\\n            jacobian( score_loss + 1/(2*sigma2) * (w - w_prior)^2 )\\n        else:\\n            jacobian( score_loss + 1/(2*sigma2) * (w - w_prior)^2 )\\n    \"\"\"\\n    vals = xi.dot(w)\\n    m = ncol(xi)\\n    loss_a = rep(0, m)\\n    loss_n = rep(0, m)\\n    n_anom = 0\\n    n_noml = 0\\n    tau_score = None\\n    if x_tau is not None:\\n        tau_score = x_tau.dot(w)\\n    for i in range(len(yi)):\\n        lbl = yi[i]\\n        if lbl == 1:\\n            n_anom += 1\\n        else:\\n            n_noml += 1\\n        if lbl == 1 and vals[i] < qval:\\n            exp_diff = np.minimum(np.exp(qval - vals[i]), 1000)\\n            loss_a[:] = loss_a - Ca * exp_diff * xi[(i), :]\\n        elif lbl == 0 and vals[i] >= qval:\\n            exp_diff = np.minimum(np.exp(vals[i] - qval), 1000)\\n            loss_n[:] = loss_n + Cn * exp_diff * xi[(i), :]\\n        else:\\n            pass\\n        if x_tau is not None and (in_constr_set is None or in_constr_set[i] ==\\n            1):\\n            tau_val = tau_score[0]\\n            if lbl == 1 and vals[i] < tau_val:\\n                loss_a[:] = loss_a + Cx * (x_tau - xi[(i), :])\\n            elif lbl == 0 and vals[i] >= tau_val:\\n                loss_n[:] = loss_n + Cx * (xi[(i), :] - x_tau)\\n            else:\\n                pass\\n    dl_dw = loss_a / max(1, n_anom) + loss_n / max(1, n_noml)\\n    if withprior and w_prior is not None:\\n        w_diff = w - w_prior\\n        dl_dw[0:m] += 1 / sigma2 * w_diff\\n    return dl_dw\\n', 'forest aad loss gradient exp xi yi qval in constr set none tau none ca 1 0 cn 1 0 cx 1 0 withprior false prior none 1 0 vals xi dot ncol xi loss rep 0 loss rep 0 anom 0 noml 0 tau score none if tau is not none tau score tau dot for in range len yi lbl yi if lbl 1 anom 1 else noml 1 if lbl 1 and vals qval exp diff np minimum np exp qval vals 1000 loss loss ca exp diff xi elif lbl 0 and vals qval exp diff np minimum np exp vals qval 1000 loss loss cn exp diff xi else pass if tau is not none and in constr set is none or in constr set 1 tau val tau score 0 if lbl 1 and vals tau val loss loss cx tau xi elif lbl 0 and vals tau val loss loss cx xi tau else pass dl dw loss max 1 anom loss max 1 noml if withprior and prior is not none diff prior dl dw 0 1 diff return dl dw', 'computes jacobian of aad loss')] [('__init__', 37, \"def __init__(self, name, init_config, agentConfig, instances):\\n    for idx, inst in enumerate(instances):\\n        try:\\n            inst['name'] = inst['name']\\n        except KeyError:\\n            inst['name'] = 'dns-check-%s' % idx\\n    NetworkCheck.__init__(self, name, init_config, agentConfig, instances)\\n    self.default_timeout = init_config.get('default_timeout', self.\\n        DEFAULT_TIMEOUT)\\n\", 'init self name init config agentconfig instances for idx inst in enumerate instances try inst name inst name except keyerror inst name dns check idx networkcheck init self name init config agentconfig instances self default timeout init config get default timeout self default timeout', ''), ('_load_conf', 50, 'def _load_conf(self, instance):\\n    hostname = instance.get(\\'hostname\\')\\n    if not hostname:\\n        raise BadConfException(\\'A valid \"hostname\" must be specified\\')\\n    resolver = dns.resolver.Resolver()\\n    nameserver = instance.get(\\'nameserver\\')\\n    nameserver_port = instance.get(\\'nameserver_port\\')\\n    if nameserver is not None:\\n        resolver.nameservers = [nameserver]\\n    if nameserver_port is not None:\\n        resolver.port = nameserver_port\\n    timeout = float(instance.get(\\'timeout\\', self.default_timeout))\\n    resolver.lifetime = timeout\\n    record_type = instance.get(\\'record_type\\', \\'A\\')\\n    return hostname, timeout, nameserver, record_type, resolver\\n', 'load conf self instance hostname instance get hostname if not hostname raise badconfexception valid hostname must be specified resolver dns resolver resolver nameserver instance get nameserver nameserver port instance get nameserver port if nameserver is not none resolver nameservers nameserver if nameserver port is not none resolver port nameserver port timeout float instance get timeout self default timeout resolver lifetime timeout record type instance get record type return hostname timeout nameserver record type resolver', ''), ('_check', 72, 'def _check(self, instance):\\n    hostname, timeout, nameserver, record_type, resolver = self._load_conf(\\n        instance)\\n    response_time = 0\\n    t0 = time_func()\\n    try:\\n        self.log.debug(\\'Querying \"{0}\" record for hostname \"{1}\"...\\'.format\\n            (record_type, hostname))\\n        if record_type == \\'NXDOMAIN\\':\\n            try:\\n                resolver.query(hostname)\\n            except dns.resolver.NXDOMAIN:\\n                pass\\n            else:\\n                raise AssertionError(\\'Expected an NXDOMAIN, got a result.\\')\\n        else:\\n            answer = resolver.query(hostname, rdtype=record_type)\\n            assert answer.rrset.items[0].to_text()\\n        response_time = time_func() - t0\\n    except dns.exception.Timeout:\\n        self.log.error(\\'DNS resolution of {0} timed out\\'.format(hostname))\\n        return Status.CRITICAL, \\'DNS resolution of {0} timed out\\'.format(\\n            hostname)\\n    except Exception:\\n        self.log.exception(\\'DNS resolution of {0} has failed.\\'.format(hostname)\\n            )\\n        return Status.CRITICAL, \\'DNS resolution of {0} has failed\\'.format(\\n            hostname)\\n    else:\\n        tags = self._get_tags(instance)\\n        if response_time > 0:\\n            self.gauge(\\'dns.response_time\\', response_time, tags=tags)\\n        self.log.debug(\\'Resolved hostname: {0}\\'.format(hostname))\\n        return Status.UP, \\'UP\\'\\n', 'check self instance hostname timeout nameserver record type resolver self load conf instance response time 0 time func try self log debug querying 0 record for hostname 1 format record type hostname if record type nxdomain try resolver query hostname except dns resolver nxdomain pass else raise assertionerror expected an nxdomain got result else answer resolver query hostname rdtype record type assert answer rrset items 0 to text response time time func except dns exception timeout self log error dns resolution of 0 timed out format hostname return status critical dns resolution of 0 timed out format hostname except exception self log exception dns resolution of 0 has failed format hostname return status critical dns resolution of 0 has failed format hostname else tags self get tags instance if response time 0 self gauge dns response time response time tags tags self log debug resolved hostname 0 format hostname return status up up', ''), ('_get_tags', 109, \"def _get_tags(self, instance):\\n    hostname = instance.get('hostname')\\n    instance_name = instance.get('name', hostname)\\n    record_type = instance.get('record_type', 'A')\\n    custom_tags = instance.get('tags', [])\\n    tags = []\\n    try:\\n        nameserver = instance.get('nameserver') or dns.resolver.Resolver(\\n            ).nameservers[0]\\n        tags.append('nameserver:{0}'.format(nameserver))\\n    except IndexError:\\n        self.log.error('No DNS server was found on this host.')\\n    tags = custom_tags + ['nameserver:{0}'.format(nameserver),\\n        'resolved_hostname:{0}'.format(hostname), 'instance:{0}'.format(\\n        instance_name), 'record_type:{0}'.format(record_type)]\\n    return tags\\n\", 'get tags self instance hostname instance get hostname instance name instance get name hostname record type instance get record type custom tags instance get tags tags try nameserver instance get nameserver or dns resolver resolver nameservers 0 tags append nameserver 0 format nameserver except indexerror self log error no dns server was found on this host tags custom tags nameserver 0 format nameserver resolved hostname 0 format hostname instance 0 format instance name record type 0 format record type return tags', ''), ('report_as_service_check', 128, 'def report_as_service_check(self, sc_name, status, instance, msg=None):\\n    tags = self._get_tags(instance)\\n    if status == Status.UP:\\n        msg = None\\n    self.service_check(self.SERVICE_CHECK_NAME, NetworkCheck.\\n        STATUS_TO_SERVICE_CHECK[status], tags=tags, message=msg)\\n', 'report as service check self sc name status instance msg none tags self get tags instance if status status up msg none self service check self service check name networkcheck status to service check status tags tags message msg', '')] [('setupUi', 13, \"def setupUi(self, Form):\\n    Form.setObjectName('Form')\\n    Form.resize(653, 20)\\n    self.horizontalLayout = QtGui.QHBoxLayout(Form)\\n    self.horizontalLayout.setSpacing(3)\\n    self.horizontalLayout.setMargin(0)\\n    self.horizontalLayout.setObjectName('horizontalLayout')\\n    self.projectSeqShotLayout = QtGui.QHBoxLayout()\\n    self.projectSeqShotLayout.setObjectName('projectSeqShotLayout')\\n    self.horizontalLayout.addLayout(self.projectSeqShotLayout)\\n    spacerItem = QtGui.QSpacerItem(40, 20, QtGui.QSizePolicy.Expanding,\\n        QtGui.QSizePolicy.Minimum)\\n    self.horizontalLayout.addItem(spacerItem)\\n    self.departmentLabel = QtGui.QLabel(Form)\\n    self.departmentLabel.setObjectName('departmentLabel')\\n    self.horizontalLayout.addWidget(self.departmentLabel)\\n    self.departmentComboBox = QtGui.QComboBox(Form)\\n    font = QtGui.QFont()\\n    font.setFamily('Monospace')\\n    self.departmentComboBox.setFont(font)\\n    self.departmentComboBox.setObjectName('departmentComboBox')\\n    self.departmentComboBox.addItem('')\\n    self.departmentComboBox.addItem('')\\n    self.departmentComboBox.addItem('')\\n    self.departmentComboBox.addItem('')\\n    self.departmentComboBox.addItem('')\\n    self.departmentComboBox.addItem('')\\n    self.departmentComboBox.addItem('')\\n    self.horizontalLayout.addWidget(self.departmentComboBox)\\n    spacerItem1 = QtGui.QSpacerItem(40, 20, QtGui.QSizePolicy.Expanding,\\n        QtGui.QSizePolicy.Minimum)\\n    self.horizontalLayout.addItem(spacerItem1)\\n    self.userLabel = QtGui.QLabel(Form)\\n    self.userLabel.setObjectName('userLabel')\\n    self.horizontalLayout.addWidget(self.userLabel)\\n    self.userComboBox = QtGui.QComboBox(Form)\\n    font = QtGui.QFont()\\n    font.setFamily('Monospace')\\n    self.userComboBox.setFont(font)\\n    self.userComboBox.setObjectName('userComboBox')\\n    self.userComboBox.addItem('')\\n    self.userComboBox.addItem('')\\n    self.userComboBox.addItem('')\\n    self.userComboBox.addItem('')\\n    self.userComboBox.addItem('')\\n    self.horizontalLayout.addWidget(self.userComboBox)\\n    spacerItem2 = QtGui.QSpacerItem(40, 20, QtGui.QSizePolicy.Expanding,\\n        QtGui.QSizePolicy.Minimum)\\n    self.horizontalLayout.addItem(spacerItem2)\\n    self.departmentLabel.setBuddy(self.departmentComboBox)\\n    self.userLabel.setBuddy(self.userComboBox)\\n    self.retranslateUi(Form)\\n    QtCore.QMetaObject.connectSlotsByName(Form)\\n    Form.setTabOrder(self.departmentComboBox, self.userComboBox)\\n\", 'setupui self form form setobjectname form form resize 653 20 self horizontallayout qtgui qhboxlayout form self horizontallayout setspacing 3 self horizontallayout setmargin 0 self horizontallayout setobjectname horizontallayout self projectseqshotlayout qtgui qhboxlayout self projectseqshotlayout setobjectname projectseqshotlayout self horizontallayout addlayout self projectseqshotlayout spaceritem qtgui qspaceritem 40 20 qtgui qsizepolicy expanding qtgui qsizepolicy minimum self horizontallayout additem spaceritem self departmentlabel qtgui qlabel form self departmentlabel setobjectname departmentlabel self horizontallayout addwidget self departmentlabel self departmentcombobox qtgui qcombobox form font qtgui qfont font setfamily monospace self departmentcombobox setfont font self departmentcombobox setobjectname departmentcombobox self departmentcombobox additem self departmentcombobox additem self departmentcombobox additem self departmentcombobox additem self departmentcombobox additem self departmentcombobox additem self departmentcombobox additem self horizontallayout addwidget self departmentcombobox qtgui qspaceritem 40 20 qtgui qsizepolicy expanding qtgui qsizepolicy minimum self horizontallayout additem self userlabel qtgui qlabel form self userlabel setobjectname userlabel self horizontallayout addwidget self userlabel self usercombobox qtgui qcombobox form font qtgui qfont font setfamily monospace self usercombobox setfont font self usercombobox setobjectname usercombobox self usercombobox additem self usercombobox additem self usercombobox additem self usercombobox additem self usercombobox additem self horizontallayout addwidget self usercombobox qtgui qspaceritem 40 20 qtgui qsizepolicy expanding qtgui qsizepolicy minimum self horizontallayout additem self departmentlabel setbuddy self departmentcombobox self userlabel setbuddy self usercombobox self retranslateui form qtcore qmetaobject connectslotsbyname form form settaborder self departmentcombobox self usercombobox', ''), ('retranslateUi', 66, \"def retranslateUi(self, Form):\\n    Form.setWindowTitle(QtGui.QApplication.translate('Form', 'Form', None,\\n        QtGui.QApplication.UnicodeUTF8))\\n    self.departmentLabel.setText(QtGui.QApplication.translate('Form',\\n        'Department', None, QtGui.QApplication.UnicodeUTF8))\\n    self.departmentComboBox.setItemText(0, QtGui.QApplication.translate(\\n        'Form', 'dept', None, QtGui.QApplication.UnicodeUTF8))\\n    self.departmentComboBox.setItemText(1, QtGui.QApplication.translate(\\n        'Form', 'all depts', None, QtGui.QApplication.UnicodeUTF8))\\n    self.departmentComboBox.setItemText(2, QtGui.QApplication.translate(\\n        'Form', 'layout', None, QtGui.QApplication.UnicodeUTF8))\\n    self.departmentComboBox.setItemText(3, QtGui.QApplication.translate(\\n        'Form', 'light', None, QtGui.QApplication.UnicodeUTF8))\\n    self.departmentComboBox.setItemText(4, QtGui.QApplication.translate(\\n        'Form', 'model', None, QtGui.QApplication.UnicodeUTF8))\\n    self.departmentComboBox.setItemText(5, QtGui.QApplication.translate(\\n        'Form', 'rig', None, QtGui.QApplication.UnicodeUTF8))\\n    self.departmentComboBox.setItemText(6, QtGui.QApplication.translate(\\n        'Form', 'rnd', None, QtGui.QApplication.UnicodeUTF8))\\n    self.userLabel.setText(QtGui.QApplication.translate('Form', 'User',\\n        None, QtGui.QApplication.UnicodeUTF8))\\n    self.userComboBox.setItemText(0, QtGui.QApplication.translate('Form',\\n        'user', None, QtGui.QApplication.UnicodeUTF8))\\n    self.userComboBox.setItemText(1, QtGui.QApplication.translate('Form',\\n        'current user', None, QtGui.QApplication.UnicodeUTF8))\\n    self.userComboBox.setItemText(2, QtGui.QApplication.translate('Form',\\n        'davidc', None, QtGui.QApplication.UnicodeUTF8))\\n    self.userComboBox.setItemText(3, QtGui.QApplication.translate('Form',\\n        'bob', None, QtGui.QApplication.UnicodeUTF8))\\n    self.userComboBox.setItemText(4, QtGui.QApplication.translate('Form',\\n        'john', None, QtGui.QApplication.UnicodeUTF8))\\n\", 'retranslateui self form form setwindowtitle qtgui qapplication translate form form none qtgui qapplication self departmentlabel settext qtgui qapplication translate form department none qtgui qapplication self departmentcombobox setitemtext 0 qtgui qapplication translate form dept none qtgui qapplication self departmentcombobox setitemtext 1 qtgui qapplication translate form all depts none qtgui qapplication self departmentcombobox setitemtext 2 qtgui qapplication translate form layout none qtgui qapplication self departmentcombobox setitemtext 3 qtgui qapplication translate form light none qtgui qapplication self departmentcombobox setitemtext 4 qtgui qapplication translate form model none qtgui qapplication self departmentcombobox setitemtext 5 qtgui qapplication translate form rig none qtgui qapplication self departmentcombobox setitemtext 6 qtgui qapplication translate form rnd none qtgui qapplication self userlabel settext qtgui qapplication translate form user none qtgui qapplication self usercombobox setitemtext 0 qtgui qapplication translate form user none qtgui qapplication self usercombobox setitemtext 1 qtgui qapplication translate form current user none qtgui qapplication self usercombobox setitemtext 2 qtgui qapplication translate form davidc none qtgui qapplication self usercombobox setitemtext 3 qtgui qapplication translate form bob none qtgui qapplication self usercombobox setitemtext 4 qtgui qapplication translate form john none qtgui qapplication', '')] [('generate_graph', 26, 'def generate_graph(net, lod=\\'np-np\\'):\\n    \"\"\"Generate a networkx graph from a network dictionary.\\n    \\n    Parameters\\n    ----------\\n    net : dict\\n        The network dictionary to be converted.\\n    mode : string\\n        Specifies the level of detail the graph is generated.\\n        \\'np-np\\': All neuron-pools are generated as nodes,\\n            all synapse-pools are generated as edges.\\n        \\'np-sp-np\\': All nps and sps are generated as nodes.\\n        \\'np-f-sp-np\\': All nps, sps and sp\\'s factors are\\n            generated as nodes.\\n    Returns \\n    -------\\n    nx.MultiDiGraph\\n        The converted graph.\\n    \"\"\"\\n    nx_graph = nx.MultiDiGraph()\\n    nx_graph.lod = copy(lod)\\n    for n, N in net[\\'neuron_pools\\'].items():\\n        nx_graph.add_node(n, node_type=\\'np\\')\\n    if lod == \\'np-np\\':\\n        for s, S in net[\\'synapse_pools\\'].items():\\n            for factor in S[\\'source\\']:\\n                for src in factor:\\n                    nx_graph.add_edge(src, S[\\'target\\'], edge_name=s)\\n    elif lod == \\'np-sp-np\\':\\n        for s, S in net[\\'synapse_pools\\'].items():\\n            nx_graph.add_node(s, node_type=\\'sp\\')\\n            nx_graph.add_edge(s, S[\\'target\\'])\\n            for factor in S[\\'source\\']:\\n                for src in factor:\\n                    nx_graph.add_edge(src, s)\\n    elif lod == \\'np-f-sp-np\\':\\n        for s, S in net[\\'synapse_pools\\'].items():\\n            nx_graph.add_node(s, node_type=\\'sp\\')\\n            nx_graph.add_edge(s, S[\\'target\\'])\\n            for f, F in enumerate(S[\\'source\\']):\\n                node_name = s + \\' \\' + str(f)\\n                nx_graph.add_node(node_name, node_type=\\'factor\\')\\n                nx_graph.add_edge(node_name, s)\\n                for src in F:\\n                    nx_graph.add_edge(src, node_name)\\n    return nx_graph\\n', 'generate graph net lod np np nx graph nx multidigraph nx graph lod copy lod for in net neuron pools items nx graph add node node type np if lod np np for in net synapse pools items for factor in source for src in factor nx graph add edge src target edge name elif lod np sp np for in net synapse pools items nx graph add node node type sp nx graph add edge target for factor in source for src in factor nx graph add edge src elif lod np sp np for in net synapse pools items nx graph add node node type sp nx graph add edge target for in enumerate source node name str nx graph add node node name node type factor nx graph add edge node name for src in nx graph add edge src node name return nx graph', 'generate a networkx graph from a network dictionary')] [('delete', 89, 'def delete(self):\\n    \"\"\"Delete this Message record from Twilio.\"\"\"\\n    return self.parent.delete(self.sid)\\n', 'delete self return self parent delete self sid', 'delete this message record from twilio'), ('redact', 93, 'def redact(self):\\n    \"\"\"Redact this Message\\'s `body` field from Twilio while preserving\\n        the record itself and related metadata.\\n        \"\"\"\\n    return self.parent.redact(self.sid)\\n', 'redact self return self parent redact self sid', 'redact this message s body field from twilio while preserving the record itself and related metadata'), ('create', 105, 'def create(self, from_=None, **kwargs):\\n    \"\"\"\\n        Create and send a Message.\\n\\n        :param str to: The destination phone number.\\n        :param str `from_`: The phone number sending this message\\n            (must be a verified Twilio number)\\n        :param str body: The message you want to send,\\n            limited to 1600 characters.\\n        :param list media_url: A list of URLs of images to include in the\\n            message.\\n        :param status_callback: A URL that Twilio will POST to when\\n            your message is processed.\\n        :param str application_sid: The 34 character sid of the application\\n            Twilio should use to handle this message.\\n        \"\"\"\\n    kwargs[\\'from\\'] = from_\\n    return self.create_instance(kwargs)\\n', 'create self from none kwargs kwargs from from return self create instance kwargs', 'create and send a message'), ('list', 124, '@normalize_dates\\ndef list(self, from_=None, before=None, after=None, date_sent=None, **kw):\\n    \"\"\"\\n        Returns a page of :class:`Message` resources as a list. For\\n        paging information see :class:`ListResource`.\\n\\n        :param to: Only show messages to this phone number.\\n        :param from_: Only show messages from this phone number.\\n        :param date after: Only list messages sent after this date.\\n        :param date before: Only list message sent before this date.\\n        :param date date_sent: Only list message sent on this date.\\n        :param `from_`: Only show messages from this phone number.\\n        :param date after: Only list messages logged after this datetime\\n        :param date before: Only list messages logged before this datetime\\n        \"\"\"\\n    kw[\\'From\\'] = from_\\n    kw[\\'DateSent<\\'] = before\\n    kw[\\'DateSent>\\'] = after\\n    kw[\\'DateSent\\'] = parse_date(date_sent)\\n    return self.get_instances(kw)\\n', 'list self from none before none after none date sent none kw kw from from kw datesent before kw datesent after kw datesent parse date date sent return self get instances kw', 'returns a page of'), ('iter', 145, '@normalize_dates\\ndef iter(self, from_=None, to=None, before=None, after=None, date_sent=None,\\n    **kwargs):\\n    \"\"\"\\n        Returns an iterator of :class:`Message` resources.\\n\\n        :param date after: Only list calls started after this datetime\\n        :param date before: Only list calls started before this datetime\\n        \"\"\"\\n    kwargs[\\'From\\'] = from_\\n    kwargs[\\'To\\'] = to\\n    kwargs[\\'DateSent<\\'] = before\\n    kwargs[\\'DateSent>\\'] = after\\n    kwargs[\\'DateSent\\'] = parse_date(date_sent)\\n    return super(Messages, self).iter(**kwargs)\\n', 'iter self from none to none before none after none date sent none kwargs kwargs from from kwargs to to kwargs datesent before kwargs datesent after kwargs datesent parse date date sent return super messages self iter kwargs', 'returns an iterator of'), ('update', 161, 'def update(self, sid, **kwargs):\\n    \"\"\" Updates the message for the given sid\\n        :param sid: The sid of the message to update.\\n        \"\"\"\\n    return self.update_instance(sid, kwargs)\\n', 'update self sid kwargs return self update instance sid kwargs', 'updates the message for the given sid'), ('delete', 167, 'def delete(self, sid):\\n    \"\"\"Delete the specified Message record from Twilio.\"\"\"\\n    return self.delete_instance(sid)\\n', 'delete self sid return self delete instance sid', 'delete the specified message record from twilio'), ('redact', 171, 'def redact(self, sid):\\n    \"\"\"Redact the specified Message record\\'s Body field.\"\"\"\\n    return self.update_instance(sid, {\\'Body\\': \\'\\'})\\n', 'redact self sid return self update instance sid body', 'redact the specified message record s body field')] [('test_interface_import', 17, 'def test_interface_import():\\n    \"\"\"\\n    >>> import zope.interface.common.interfaces\\n    \"\"\"\\n', 'test interface import', 'import zope interface common interfaces'), ('test_suite', 22, 'def test_suite():\\n    return unittest.TestSuite((doctest.DocTestSuite(),))\\n', 'test suite return unittest testsuite doctest doctestsuite', '')] [('test_email', 4, \"@case({'email': {'valid': ['12345678@qq.com', 'python@gmail.com',\\n    '123@163.com', 'test-demo@vip.qq.com', 'i+box@gmail.com'], 'invalid': [\\n    '123123@123@163123@@163.com123@163.123@163.com123@163com',\\n    ' 123@163.com', '123 @163.com', '123@ 163.com', '123@163.com ',\\n    'qq.com', ' @163.com', '中文@qq.com', None]}, 'email&optional': [('', ''),\\n    (None, '')]})\\ndef test_email():\\n    pass\\n\", 'test email pass', '')] [('mdl_file', 35, '@property\\ndef mdl_file(self):\\n    return self._mdl_file\\n', 'mdl file self return self mdl file', ''), ('mdl_file', 39, \"@mdl_file.setter\\ndef mdl_file(self, mdl_file):\\n    if not mdl_file.endswith('.mdl'):\\n        raise ValueError('model file needs to be a vensim .mdl file')\\n    if not os.path.isfile(mdl_file):\\n        raise ValueError('mdl_file not found')\\n    self.py_model_name = mdl_file.replace('.mdl', '.py')\\n    self._mdl_file = mdl_file\\n\", 'mdl file self mdl file if not mdl file endswith mdl raise valueerror model file needs to be vensim mdl file if not os path isfile mdl file raise valueerror mdl file not found self py model name mdl file replace mdl py self mdl file mdl file', ''), ('__init__', 50, \"def __init__(self, name=None, mdl_file=None):\\n    self.mdl_file = mdl_file\\n    if name is None:\\n        name = pysd.utils.make_python_identifier(mdl_file)[0]\\n        name = name.replace('_', '')\\n    super(BasePysdModel, self).__init__(name)\\n\", 'init self name none mdl file none self mdl file mdl file if name is none name pysd utils make python identifier mdl file 0 name name replace super basepysdmodel self init name', ''), ('model_init', 57, '@method_logger\\ndef model_init(self, policy, **kwargs):\\n    AbstractModel.model_init(self, policy, **kwargs)\\n    self.model = pysd.read_vensim(self.mdl_file)\\n', 'model init self policy kwargs abstractmodel model init self policy kwargs self model pysd read vensim self mdl file', ''), ('run_experiment', 65, '@method_logger\\ndef run_experiment(self, experiment):\\n    res = self.model.run(params=experiment, return_columns=self.\\n        output_variables)\\n    output = {col: series.as_matrix() for col, series in res.iteritems()}\\n    return output\\n', 'run experiment self experiment res self model run params experiment return columns self output variables output col series as matrix for col series in res iteritems return output', ''), ('reset_model', 74, 'def reset_model(self):\\n    \"\"\"\\n        Method for reseting the model to its initial state. The default\\n        implementation only sets the outputs to an empty dict.\\n\\n        \"\"\"\\n    super(BasePysdModel, self).reset_model()\\n    if self.model is not None:\\n        self.model.reset_state()\\n', 'reset model self super basepysdmodel self reset model if self model is not none self model reset state', 'method for reseting the model to its initial state the default implementation only sets the outputs to an empty dict')] [('xterm_to_rgb', 42, 'def xterm_to_rgb(xcolor):\\n    \"\"\"Convert xterm Color ID to an RGB value\\n\\n    All 256 values are precalculated and stored in :data:`COLOR_TABLE`\\n    \"\"\"\\n    assert 0 <= xcolor <= 255\\n    if xcolor < 16:\\n        return BASIC16[xcolor]\\n    elif 16 <= xcolor <= 231:\\n        xcolor -= 16\\n        return CUBE_STEPS[xcolor // 36 % 6], CUBE_STEPS[xcolor // 6 % 6\\n            ], CUBE_STEPS[xcolor % 6]\\n    elif 232 <= xcolor <= 255:\\n        c = 8 + (xcolor - 232) * 10\\n        return c, c, c\\n', 'xterm to rgb xcolor assert 0 xcolor 255 if xcolor 16 return xcolor elif 16 xcolor 231 xcolor 16 return cube steps xcolor 36 6 cube steps xcolor 6 6 cube steps xcolor 6 elif 232 xcolor 255 8 xcolor 232 10 return', 'convert xterm color id to an rgb value'), ('rgb_to_xterm', 66, 'def rgb_to_xterm(r, g, b):\\n    \"\"\"Quantize RGB values to an xterm 256-color ID\\n\\n    This works by envisioning the RGB values for all 256 xterm colors\\n    as 3D euclidean space and brute-force searching for the nearest\\n    neighbor.\\n\\n    This is very slow.  If you\\'re very lucky, :func:`compile_speedup`\\n    will replace this function automatically with routines in\\n    `_xterm256.c`.\\n    \"\"\"\\n    if r < 5 and g < 5 and b < 5:\\n        return 16\\n    best_match = 0\\n    smallest_distance = 10000000000\\n    for c in range(16, 256):\\n        d = (COLOR_TABLE[c][0] - r) ** 2 + (COLOR_TABLE[c][1] - g) ** 2 + (\\n            COLOR_TABLE[c][2] - b) ** 2\\n        if d < smallest_distance:\\n            smallest_distance = d\\n            best_match = c\\n    return best_match\\n', 'rgb to xterm if 5 and 5 and 5 return 16 best match 0 smallest distance 10000000000 for in range 16 256 color table 0 2 color table 1 2 color table 2 2 if smallest distance smallest distance best match return best match', 'quantize rgb values to an xterm 256 color id'), ('compile_speedup', 91, 'def compile_speedup():\\n    \"\"\"Tries to compile/link the C version of this module\\n\\n    Like it really makes a huge difference.  With a little bit of luck\\n    this should *just work* for you.\\n\\n    You need:\\n\\n    - Python >= 2.5 for ctypes library\\n    - gcc (``sudo apt-get install gcc``)\\n\\n    \"\"\"\\n    import os\\n    import ctypes\\n    from os.path import join, dirname, getmtime, exists, expanduser\\n    library = expanduser(\\'~/.xterm256.so\\')\\n    sauce = join(dirname(__file__), \\'_xterm256.c\\')\\n    if not exists(library) or getmtime(sauce) > getmtime(library):\\n        build = \\'gcc -fPIC -shared -o %s %s\\' % (library, sauce)\\n        if os.system(build + \\' >/dev/null 2>&1\\') != 0:\\n            raise OSError(\\'GCC error\\')\\n    xterm256_c = ctypes.cdll.LoadLibrary(library)\\n    xterm256_c.init()\\n\\n    def xterm_to_rgb(xcolor):\\n        res = xterm256_c.xterm_to_rgb_i(xcolor)\\n        return res >> 16 & 255, res >> 8 & 255, res & 255\\n    return xterm256_c.rgb_to_xterm, xterm_to_rgb\\n', 'compile speedup import os import ctypes from os path import join dirname getmtime exists expanduser library expanduser so sauce join dirname file if not exists library or getmtime sauce getmtime library build gcc fpic shared library sauce if os system build dev null 2 1 0 raise oserror gcc error ctypes cdll loadlibrary library init def xterm to rgb xcolor res xterm to rgb xcolor return res 16 255 res 8 255 res 255 return rgb to xterm xterm to rgb', 'tries to compile link the c version of this module')] [('__init__', 10, 'def __init__(self, *args, **kwargs):\\n    super(AppsSubmaker, self).__init__(*args, **kwargs)\\n    self.patchTools = None\\n    self.patchTool = Patch()\\n', 'init self args kwargs super appssubmaker self init args kwargs self patchtools none self patchtool patch', ''), ('make', 15, 'def make(self, workDir):\\n    apps = self.getValue(\\'.\\', {})\\n    if \\'__make__\\' in apps:\\n        del apps[\\'__make__\\']\\n    if \\'__depend__\\' in apps:\\n        del apps[\\'__depend__\\']\\n    for pkgName, data in apps.items():\\n        apkPath = self.getProperty(pkgName.replace(\\'.\\', \\'\\\\\\\\.\\') + \\'.apk\\'\\n            ).resolveAsRelativePath()\\n        patchesList = []\\n        curr = self.getMaker().getConfig()\\n        patchesKey = \\'update.apps.\\' + pkgName.replace(\\'.\\', \\'\\\\\\\\.\\') + \\'.patches\\'\\n        currPatches = curr.get(patchesKey, [], directOnly=True)\\n        patchesList.extend([curr.resolveRelativePath(patch) for patch in\\n            currPatches if patch != \\'__override__\\'])\\n        if not \\'__override__\\' in currPatches:\\n            while not curr.isOrphan():\\n                curr = curr.getParent()\\n                currPatches = curr.get(patchesKey, [], directOnly=True)\\n                patchesList.extend([curr.resolveRelativePath(patch) for\\n                    patch in currPatches if patch != \\'__override__\\'])\\n                if \\'__override__\\' in currPatches:\\n                    break\\n        if \\'destination\\' in data:\\n            destination = data[\\'destination\\']\\n            targetDest = destination[1:] if destination[0\\n                ] == \\'/\\' else destination\\n            if not targetDest.lower().endswith(\\'.apk\\'):\\n                targetDest = os.path.join(targetDest, os.path.basename(apkPath)\\n                    )\\n        elif \\'system\\' in data and data[\\'system\\']:\\n            targetDest = os.path.join(\\'system/app\\', os.path.basename(apkPath))\\n        else:\\n            targetDest = os.path.join(\\'data/app\\', os.path.basename(apkPath))\\n        localDest = os.path.join(workDir, targetDest)\\n        if not os.path.exists(os.path.dirname(localDest)):\\n            os.makedirs(os.path.dirname(localDest))\\n        if len(patchesList):\\n            if not self.patchTools:\\n                key, apkTool = self.getHostBinary(\\'apktool\\')\\n                assert apkTool, \"Can\\'t patch APK without apktool. Please set %s\" % apkTool\\n                frameworks = self.getHostBinaryConfigProperty(\\n                    \\'apktool.frameworks_dir\\', None).resolveAsRelativePath()\\n                assert frameworks, \"Can\\'t patch APK without __config__.host.apktool.frameworks_dir set. See http://ibotpeaches.github.io/Apktool/documentation/#frameworks\"\\n                javakey, java = self.getHostBinary(\\'java\\')\\n                assert java, \"Can\\'t use apktool without java. Please set %s\" % javakey\\n                self.patchTools = namedtuple(\\'PatchTools\\', [\\'apkTool\\',\\n                    \\'java\\', \\'frameworks\\'])(apkTool, java, frameworks)\\n            self.patchApk(self.patchTools.java, self.patchTools.apkTool,\\n                self.patchTools.frameworks, apkPath, patchesList, localDest)\\n        else:\\n            shutil.copy(apkPath, localDest)\\n        self.registerApkFile(targetDest)\\n', 'make self workdir apps self getvalue if make in apps del apps make if depend in apps del apps depend for pkgname data in apps items apkpath self getproperty pkgname replace apk resolveasrelativepath patcheslist curr self getmaker getconfig patcheskey update apps pkgname replace patches currpatches curr get patcheskey directonly true patcheslist extend curr resolverelativepath patch for patch in currpatches if patch override if not override in currpatches while not curr isorphan curr curr getparent currpatches curr get patcheskey directonly true patcheslist extend curr resolverelativepath patch for patch in currpatches if patch override if override in currpatches break if destination in data destination data destination targetdest destination 1 if destination 0 else destination if not targetdest lower endswith apk targetdest os path join targetdest os path basename apkpath elif system in data and data system targetdest os path join system app os path basename apkpath else targetdest os path join data app os path basename apkpath localdest os path join workdir targetdest if not os path exists os path dirname localdest os makedirs os path dirname localdest if len patcheslist if not self patchtools key apktool self gethostbinary apktool assert apktool can patch apk without apktool please set apktool frameworks self gethostbinaryconfigproperty apktool frameworks dir none resolveasrelativepath assert frameworks can patch apk without config host apktool frameworks dir set see http ibotpeaches github io apktool documentation frameworks javakey java self gethostbinary java assert java can use apktool without java please set javakey self patchtools namedtuple patchtools apktool java frameworks apktool java frameworks self patchapk self patchtools java self patchtools apktool self patchtools frameworks apkpath patcheslist localdest else shutil copy apkpath localdest self registerapkfile targetdest', ''), ('registerApkFile', 77, \"def registerApkFile(self, path):\\n    self.setValue('update.files.add.%s' % path.replace('.', '\\\\\\\\.'), {\\n        'destination': '/' + path, 'gid': '1000', 'uid': '1000', 'mode':\\n        '0644'})\\n\", 'registerapkfile self path self setvalue update files add path replace destination path gid 1000 uid 1000 mode 0644', ''), ('patchApk', 85, 'def patchApk(self, java, apkTool, frameworks, apk, patches, dest):\\n    tmpDir = tempfile.mkdtemp()\\n    try:\\n        apkTool = ApkTool(java, apkTool)\\n        apkTool.decode(apk, frameworks, tmpDir, force=True)\\n        for patch in patches:\\n            self.patchTool.patch(tmpDir, patch, 1)\\n        apkTool.build(tmpDir, frameworks, dest)\\n        shutil.rmtree(tmpDir)\\n    except:\\n        shutil.rmtree(tmpDir)\\n        raise\\n', 'patchapk self java apktool frameworks apk patches dest tmpdir tempfile mkdtemp try apktool apktool java apktool apktool decode apk frameworks tmpdir force true for patch in patches self patchtool patch tmpdir patch 1 apktool build tmpdir frameworks dest shutil rmtree tmpdir except shutil rmtree tmpdir raise', '')] [('__init__', 36, 'def __init__(self, *, type=None, **kwargs) ->None:\\n    super(DataMigrationError, self).__init__(**kwargs)\\n    self.message = None\\n    self.type = type\\n', 'init self type none kwargs none super datamigrationerror self init kwargs self message none self type type', '')] [('test_empty_post', 13, \"def test_empty_post(self):\\n    r = self.post([])\\n    self.assert201(r)\\n    assert r.headers.get('Content-Type', None) == 'application/json'\\n    assert r.json['data'] == []\\n\", 'test empty post self self post self assert headers get content type none application json assert json data', ''), ('test_no_vehicle', 19, \"def test_no_vehicle(self):\\n    dict_ = deepcopy(dict_ads)\\n    dict_['vehicle_id'] = None\\n    r = self.post([dict_])\\n    self.assert400(r)\\n\", 'test no vehicle self dict deepcopy dict ads dict vehicle id none self post dict self', ''), ('test_simple', 25, \"def test_simple(self):\\n    self.init_zupc()\\n    dict_ = deepcopy(dict_ads)\\n    dict_['vehicle_id'] = None\\n    r = self.post([dict_])\\n    self.assert201(r)\\n    self.assertEqual(len(r.json['data']), 1)\\n    ads = r.json['data'][0]\\n    self.check_req_vs_dict(ads, dict_)\\n    list_ads = models.ADS.query.all()\\n    self.assertEqual(len(list_ads), 1)\\n    assert all(map(lambda ads: ads.zupc_id is not None, list_ads))\\n\", 'test simple self self init zupc dict deepcopy dict ads dict vehicle id none self post dict self self assertequal len json data 1 ads json data 0 self check req vs dict ads dict list ads models ads query all self assertequal len list ads 1 assert all map lambda ads ads zupc id is not none list ads', ''), ('test_vehicle', 38, \"def test_vehicle(self):\\n    self.init_zupc()\\n    dict_v = deepcopy(dict_vehicle)\\n    r = self.post([dict_v], url='/vehicles/')\\n    self.assert201(r)\\n    vehicle = r.json['data'][0]\\n    self.check_req_vs_dict(vehicle, dict_v)\\n    vehicle_id = vehicle['id']\\n    dict_a = deepcopy(dict_ads)\\n    dict_a['vehicle_id'] = vehicle_id\\n    r = self.post([dict_a])\\n    self.assert201(r)\\n    ads = r.json['data'][0]\\n    self.check_req_vs_dict(ads, dict_a)\\n    list_ads = models.ADS.query.all()\\n    self.assertEqual(len(list_ads), 1)\\n    assert all(map(lambda ads: ads.zupc_id is not None, list_ads))\\n    self.assertEquals(len(models.Vehicle.query.all()), 1)\\n\", 'test vehicle self self init zupc dict deepcopy dict vehicle self post dict url vehicles self vehicle json data 0 self check req vs dict vehicle dict vehicle id vehicle id dict deepcopy dict ads dict vehicle id vehicle id self post dict self ads json data 0 self check req vs dict ads dict list ads models ads query all self assertequal len list ads 1 assert all map lambda ads ads zupc id is not none list ads self assertequals len models vehicle query all 1', ''), ('test_two_ads', 59, \"def test_two_ads(self):\\n    self.init_zupc()\\n    dict_ = deepcopy(dict_ads)\\n    dict_['vehicle_id'] = None\\n    r = self.post([dict_, dict_])\\n    self.assert201(r)\\n    self.assertEqual(len(r.json['data']), 2)\\n    list_ads = models.ADS.query.all()\\n    self.assertEqual(len(list_ads), 2)\\n    assert all(map(lambda ads: ads.zupc_id is not None, list_ads))\\n\", 'test two ads self self init zupc dict deepcopy dict ads dict vehicle id none self post dict dict self self assertequal len json data 2 list ads models ads query all self assertequal len list ads 2 assert all map lambda ads ads zupc id is not none list ads', ''), ('test_too_many_ads', 70, 'def test_too_many_ads(self):\\n    dict_ = deepcopy(dict_ads)\\n    r = self.post([dict_ for x in range(0, 251)])\\n    self.assertEqual(r.status_code, 400)\\n    self.assertEqual(len(models.ADS.query.all()), 0)\\n', 'test too many ads self dict deepcopy dict ads self post dict for in range 0 251 self assertequal status code 400 self assertequal len models ads query all 0', ''), ('test_no_data', 76, \"def test_no_data(self):\\n    r = self.post({'d': None}, envelope_data=False)\\n    self.assert400(r)\\n\", 'test no data self self post none envelope data false self', ''), ('test_bad_vehicle_id', 80, \"def test_bad_vehicle_id(self):\\n    dict_ = deepcopy(dict_ads)\\n    dict_['vehicle_id'] = 1\\n    r = self.post([dict_])\\n    self.assert400(r)\\n\", 'test bad vehicle id self dict deepcopy dict ads dict vehicle id 1 self post dict self', ''), ('test_vehicle_id_O', 86, \"def test_vehicle_id_O(self):\\n    self.init_zupc()\\n    dict_ = deepcopy(dict_ads)\\n    dict_['vehicle_id'] = 0\\n    r = self.post([dict_])\\n    self.assert201(r)\\n\", 'test vehicle id self self init zupc dict deepcopy dict ads dict vehicle id 0 self post dict self', ''), ('test_no_vehicle_id', 93, 'def test_no_vehicle_id(self):\\n    self.init_zupc()\\n    dict_ = deepcopy(dict_ads)\\n    r = self.post([dict_])\\n    self.assert201(r)\\n', 'test no vehicle id self self init zupc dict deepcopy dict ads self post dict self', ''), ('test_bad_owner_type', 99, \"def test_bad_owner_type(self):\\n    self.init_zupc()\\n    dict_ = deepcopy(dict_ads)\\n    dict_['vehicle_id'] = None\\n    dict_['owner_type'] = 'string'\\n    r = self.post([dict_])\\n    self.assert400(r)\\n    assert 'data.0.owner_type' in r.json['errors']\\n\", 'test bad owner type self self init zupc dict deepcopy dict ads dict vehicle id none dict owner type string self post dict self assert data 0 owner type in json errors', ''), ('test_no_owner_type', 108, \"def test_no_owner_type(self):\\n    self.init_zupc()\\n    dict_ = deepcopy(dict_ads)\\n    dict_['vehicle_id'] = None\\n    dict_['owner_type'] = None\\n    r = self.post([dict_])\\n    self.assert400(r)\\n    assert 'data.0.owner_type' in r.json['errors']\\n\", 'test no owner type self self init zupc dict deepcopy dict ads dict vehicle id none dict owner type none self post dict self assert data 0 owner type in json errors', '')] [('__str__', 16, 'def __str__(self):\\n    return self.name\\n', 'str self return self name', ''), ('__str__', 39, 'def __str__(self):\\n    return self.name\\n', 'str self return self name', '')] [('setUp', 9, \"def setUp(self):\\n    self.bind = mim.Connection.get()\\n    self.bind.drop_all()\\n    self.semaphore = Semaphore(self.bind.db, 1, 'counter', 1)\\n\", 'setup self self bind mim connection get self bind drop all self semaphore semaphore self bind db 1 counter 1', ''), ('test_acquire', 14, \"def test_acquire(self):\\n    self.assertTrue(self.semaphore.acquire(), msg=\\n        'Failed to acquire intial semaphore')\\n\", 'test acquire self self asserttrue self semaphore acquire msg failed to acquire intial semaphore', ''), ('test_release', 17, 'def test_release(self):\\n    self.semaphore.acquire()\\n    self.semaphore.release()\\n', 'test release self self semaphore acquire self semaphore release', ''), ('test_sequence', 21, \"def test_sequence(self):\\n    self.assertTrue(self.semaphore.acquire(), msg=\\n        'Failed to acquire intial semaphore')\\n    self.assertFalse(self.semaphore.acquire(), msg=\\n        'Failed to block on second acquire')\\n    self.semaphore.release()\\n    self.assertTrue(self.semaphore.acquire(), msg=\\n        'Failed to reacquire after release')\\n    for i in xrange(10):\\n        self.semaphore.release()\\n        self.assertTrue(self.semaphore.acquire())\\n        self.semaphore.release()\\n\", 'test sequence self self asserttrue self semaphore acquire msg failed to acquire intial semaphore self assertfalse self semaphore acquire msg failed to block on second acquire self semaphore release self asserttrue self semaphore acquire msg failed to reacquire after release for in xrange 10 self semaphore release self asserttrue self semaphore acquire self semaphore release', ''), ('test_peek', 31, 'def test_peek(self):\\n    self.assertEquals(self.semaphore.peek(), 1)\\n    self.semaphore.acquire()\\n    self.assertEquals(self.semaphore.peek(), 0)\\n', 'test peek self self assertequals self semaphore peek 1 self semaphore acquire self assertequals self semaphore peek 0', ''), ('test_force_acquire', 36, 'def test_force_acquire(self):\\n    val = self.semaphore.peek()\\n    self.semaphore.force_acquire()\\n    self.assertEquals(val - 1, self.semaphore.peek())\\n    self.semaphore.force_acquire()\\n    self.assertEquals(val - 2, self.semaphore.peek())\\n', 'test force acquire self val self semaphore peek self semaphore force acquire self assertequals val 1 self semaphore peek self semaphore force acquire self assertequals val 2 self semaphore peek', ''), ('test_force_release', 43, 'def test_force_release(self):\\n    val = self.semaphore.peek()\\n    self.semaphore.force_release()\\n    self.assertEquals(val + 1, self.semaphore.peek())\\n    self.semaphore.force_release()\\n    self.assertEquals(val + 2, self.semaphore.peek())\\n', 'test force release self val self semaphore peek self semaphore force release self assertequals val 1 self semaphore peek self semaphore force release self assertequals val 2 self semaphore peek', ''), ('test_status', 50, \"def test_status(self):\\n    self.assertTrue(self.semaphore.status(), msg='Initial status is false')\\n    self.assertTrue(self.semaphore.acquire())\\n    self.assertFalse(self.semaphore.status(), msg=\\n        'Status after acquire is true')\\n    self.assertFalse(self.semaphore.acquire())\\n    self.semaphore.release()\\n    self.assertTrue(self.semaphore.status())\\n\", 'test status self self asserttrue self semaphore status msg initial status is false self asserttrue self semaphore acquire self assertfalse self semaphore status msg status after acquire is true self assertfalse self semaphore acquire self semaphore release self asserttrue self semaphore status', ''), ('setUp', 63, \"def setUp(self):\\n    self.bind = mim.Connection.get()\\n    self.bind.drop_all()\\n    self.semaphore = Semaphore(self.bind.db, 1, 'counter', self._MAX)\\n\", 'setup self self bind mim connection get self bind drop all self semaphore semaphore self bind db 1 counter self max', ''), ('test_acquire', 68, \"def test_acquire(self):\\n    for i in xrange(self._MAX):\\n        self.assertTrue(self.semaphore.acquire(), msg=\\n            'Failed to acquire intial semaphore')\\n    self.assertFalse(self.semaphore.acquire())\\n\", 'test acquire self for in xrange self max self asserttrue self semaphore acquire msg failed to acquire intial semaphore self assertfalse self semaphore acquire', ''), ('setUp', 76, \"def setUp(self):\\n    self.bind = mim.Connection.get()\\n    self.bind.drop_all()\\n    self.semaphore = Semaphore(self.bind.db, 1, 'counter', 1)\\n\", 'setup self self bind mim connection get self bind drop all self semaphore semaphore self bind db 1 counter 1', ''), ('test_counter_name', 81, \"def test_counter_name(self):\\n    self.assertEquals(self.semaphore._counter, 'counter')\\n\", 'test counter name self self assertequals self semaphore counter counter', ''), ('test_id', 84, 'def test_id(self):\\n    self.assertEquals(self.semaphore._id, 1)\\n', 'test id self self assertequals self semaphore id 1', ''), ('test_db_holds_sem', 87, \"def test_db_holds_sem(self):\\n    sem = self.semaphore\\n    q = sem._db[sem._name].find_one({'_id': sem._id})\\n    self.assertIsNotNone(q, msg='semaphore not in database')\\n\", 'test db holds sem self sem self semaphore sem db sem name find one id sem id self assertisnotnone msg semaphore not in database', '')] [('setUp', 28, \"def setUp(self):\\n    self.switch_mock = flexmock()\\n    self.switch_mock.switch_descriptor = SwitchDescriptor('dell', 'a_host')\\n    self.session_manager = SwitchSessionManager()\\n\", 'setup self self switch mock flexmock self switch mock switch descriptor switchdescriptor dell host self session manager switchsessionmanager', ''), ('tearDown', 33, 'def tearDown(self):\\n    for timer in self.session_manager.timers.values():\\n        timer.cancel()\\n', 'teardown self for timer in self session manager timers values timer cancel', ''), ('test_open_session_generates_with_passed_session_id', 37, \"def test_open_session_generates_with_passed_session_id(self):\\n    self.session_manager.session_storage = flexmock()\\n    self.session_manager.session_storage.should_receive('add').with_args(\\n        'patate', self.switch_mock.switch_descriptor).once()\\n    self.switch_mock.should_receive('connect').once()\\n    assert_that(self.session_manager.open_session(self.switch_mock,\\n        'patate'), is_('patate'))\\n    assert_that(self.session_manager.get_switch_for_session('patate'), is_(\\n        self.switch_mock))\\n\", 'test open session generates with passed session id self self session manager session storage flexmock self session manager session storage should receive add with args patate self switch mock switch descriptor once self switch mock should receive connect once assert that self session manager open session self switch mock patate is patate assert that self session manager get switch for session patate is self switch mock', ''), ('test_open_session_with_session_that_already_exists_raises_an_exception', 47, \"def test_open_session_with_session_that_already_exists_raises_an_exception(self\\n    ):\\n    self.session_manager.session_storage = flexmock()\\n    self.session_manager.session_storage.should_receive('add').with_args(\\n        'patate', self.switch_mock.switch_descriptor).once()\\n    self.switch_mock.should_receive('connect').once()\\n    self.session_manager.open_session(self.switch_mock, 'patate')\\n    with self.assertRaises(SessionAlreadyExists):\\n        self.session_manager.open_session(self.switch_mock, 'patate')\\n\", 'test open session with session that already exists raises an exception self self session manager session storage flexmock self session manager session storage should receive add with args patate self switch mock switch descriptor once self switch mock should receive connect once self session manager open session self switch mock patate with self assertraises sessionalreadyexists self session manager open session self switch mock patate', ''), ('test_close_session', 59, \"def test_close_session(self):\\n    self.session_manager.session_storage = flexmock()\\n    self.session_manager.session_storage.should_receive('add').with_args(\\n        'patate', self.switch_mock.switch_descriptor).once()\\n    self.switch_mock.should_receive('connect').once()\\n    assert_that(self.session_manager.open_session(self.switch_mock,\\n        'patate'), is_('patate'))\\n    assert_that(self.session_manager.get_switch_for_session('patate'), is_(\\n        self.switch_mock))\\n    self.switch_mock.should_receive('disconnect').once()\\n    self.session_manager.session_storage.should_receive('remove').with_args(\\n        'patate')\\n    self.session_manager.close_session('patate')\\n    with self.assertRaises(UnknownResource):\\n        self.session_manager.get_switch_for_session('patate')\\n\", 'test close session self self session manager session storage flexmock self session manager session storage should receive add with args patate self switch mock switch descriptor once self switch mock should receive connect once assert that self session manager open session self switch mock patate is patate assert that self session manager get switch for session patate is self switch mock self switch mock should receive disconnect once self session manager session storage should receive remove with args patate self session manager close session patate with self assertraises unknownresource self session manager get switch for session patate', ''), ('test_close_session_catches_exception_if_remote_remove_fails', 77, \"def test_close_session_catches_exception_if_remote_remove_fails(self):\\n    self.session_manager.session_storage = flexmock()\\n    self.session_manager.session_storage.should_receive('add').with_args(\\n        'patate', self.switch_mock.switch_descriptor).once().ordered()\\n    self.switch_mock.should_receive('connect').once()\\n    self.session_manager.open_session(self.switch_mock, 'patate')\\n    self.switch_mock.should_receive('disconnect').once()\\n    self.session_manager.session_storage.should_receive('remove').with_args(\\n        'patate').and_raise(NetmanException)\\n    self.session_manager.close_session('patate')\\n\", 'test close session catches exception if remote remove fails self self session manager session storage flexmock self session manager session storage should receive add with args patate self switch mock switch descriptor once ordered self switch mock should receive connect once self session manager open session self switch mock patate self switch mock should receive disconnect once self session manager session storage should receive remove with args patate and raise netmanexception self session manager close session patate', ''), ('test_session_should_close_itself_after_timeout', 92, \"def test_session_should_close_itself_after_timeout(self):\\n    self.session_manager.session_inactivity_timeout = 0.01\\n    switch_mock = Mock()\\n    assert_that(self.session_manager.open_session(switch_mock, 'patate'),\\n        is_('patate'))\\n    assert_that(self.session_manager.get_switch_for_session('patate'), is_(\\n        switch_mock))\\n    time.sleep(0.02)\\n    assert_that(switch_mock.rollback_transaction.called, is_(False))\\n    with self.assertRaises(UnknownResource):\\n        self.session_manager.get_switch_for_session('patate')\\n\", 'test session should close itself after timeout self self session manager session inactivity timeout 0 01 switch mock mock assert that self session manager open session switch mock patate is patate assert that self session manager get switch for session patate is switch mock time sleep 0 02 assert that switch mock rollback transaction called is false with self assertraises unknownresource self session manager get switch for session patate', ''), ('test_session_timeout_should_reset_on_activity', 107, \"def test_session_timeout_should_reset_on_activity(self):\\n    self.session_manager.session_inactivity_timeout = 0.1\\n    switch_mock = Mock()\\n    assert_that(self.session_manager.open_session(switch_mock, 'patate'),\\n        is_('patate'))\\n    assert_that(self.session_manager.get_switch_for_session('patate'), is_(\\n        switch_mock))\\n    time.sleep(0.05)\\n    self.session_manager.keep_alive('patate')\\n    time.sleep(0.05)\\n    assert_that(switch_mock.rollback_transaction.called, is_(False))\\n    self.session_manager.keep_alive('patate')\\n    time.sleep(0.11)\\n    assert_that(switch_mock.rollback_transaction.called, is_(False))\\n    with self.assertRaises(UnknownResource):\\n        self.session_manager.get_switch_for_session('patate')\\n\", 'test session timeout should reset on activity self self session manager session inactivity timeout 0 1 switch mock mock assert that self session manager open session switch mock patate is patate assert that self session manager get switch for session patate is switch mock time sleep 0 05 self session manager keep alive patate time sleep 0 05 assert that switch mock rollback transaction called is false self session manager keep alive patate time sleep 0 11 assert that switch mock rollback transaction called is false with self assertraises unknownresource self session manager get switch for session patate', ''), ('test_commit_transaction', 132, \"def test_commit_transaction(self):\\n    self.session_manager.keep_alive = Mock()\\n    self.session_manager.session_storage = flexmock()\\n    self.session_manager.session_storage.should_receive('add').with_args(\\n        'patate', self.switch_mock.switch_descriptor).once()\\n    self.switch_mock.should_receive('connect').once().ordered()\\n    session_id = self.session_manager.open_session(self.switch_mock, 'patate')\\n    self.switch_mock.should_receive('commit_transaction').once().ordered()\\n    self.assertEquals(session_id, 'patate')\\n    self.session_manager.commit_session(session_id)\\n    self.session_manager.keep_alive.assert_called_with(session_id)\\n\", 'test commit transaction self self session manager keep alive mock self session manager session storage flexmock self session manager session storage should receive add with args patate self switch mock switch descriptor once self switch mock should receive connect once ordered session id self session manager open session self switch mock patate self switch mock should receive commit transaction once ordered self assertequals session id patate self session manager commit session session id self session manager keep alive assert called with session id', ''), ('test_rollback_session', 151, \"def test_rollback_session(self):\\n    self.session_manager.keep_alive = Mock()\\n    self.session_manager.session_storage = flexmock()\\n    self.session_manager.session_storage.should_receive('add').with_args(\\n        'patate', self.switch_mock.switch_descriptor).once()\\n    self.switch_mock.should_receive('connect').once()\\n    session_id = self.session_manager.open_session(self.switch_mock, 'patate')\\n    self.assertEquals(session_id, 'patate')\\n    self.switch_mock.should_receive('rollback_transaction').once().ordered()\\n    self.session_manager.rollback_session(session_id)\\n    self.session_manager.keep_alive.assert_called_with(session_id)\\n\", 'test rollback session self self session manager keep alive mock self session manager session storage flexmock self session manager session storage should receive add with args patate self switch mock switch descriptor once self switch mock should receive connect once session id self session manager open session self switch mock patate self assertequals session id patate self switch mock should receive rollback transaction once ordered self session manager rollback session session id self session manager keep alive assert called with session id', ''), ('test_unknown_session', 170, \"def test_unknown_session(self):\\n    with self.assertRaises(UnknownResource):\\n        self.session_manager.get_switch_for_session('patate')\\n\", 'test unknown session self with self assertraises unknownresource self session manager get switch for session patate', ''), ('test_add_session_catches_exception_if_remote_add_fails', 174, \"def test_add_session_catches_exception_if_remote_add_fails(self):\\n    self.session_manager.session_storage = flexmock()\\n    self.session_manager.session_storage.should_receive('add').with_args(\\n        'patate', self.switch_mock.switch_descriptor).and_raise(NetmanException\\n        )\\n    self.switch_mock.should_receive('connect').once().ordered()\\n    self.session_manager.open_session(self.switch_mock, 'patate')\\n\", 'test add session catches exception if remote add fails self self session manager session storage flexmock self session manager session storage should receive add with args patate self switch mock switch descriptor and raise netmanexception self switch mock should receive connect once ordered self session manager open session self switch mock patate', ''), ('test_start_transaction', 183, \"def test_start_transaction(self):\\n    self.session_manager.keep_alive = Mock()\\n    self.session_manager.session_storage = flexmock()\\n    self.session_manager.session_storage.should_receive('add').with_args(\\n        'patate', self.switch_mock.switch_descriptor).once()\\n    self.switch_mock.should_receive('connect').once().ordered()\\n    session_id = self.session_manager.open_session(self.switch_mock, 'patate')\\n    self.switch_mock.should_receive('start_transaction').once().ordered()\\n    self.assertEquals(session_id, 'patate')\\n    self.session_manager.start_transaction(session_id)\\n    self.session_manager.keep_alive.assert_called_with(session_id)\\n\", 'test start transaction self self session manager keep alive mock self session manager session storage flexmock self session manager session storage should receive add with args patate self switch mock switch descriptor once self switch mock should receive connect once ordered session id self session manager open session self switch mock patate self switch mock should receive start transaction once ordered self assertequals session id patate self session manager start transaction session id self session manager keep alive assert called with session id', ''), ('test_end_transaction', 202, \"def test_end_transaction(self):\\n    self.session_manager.keep_alive = Mock()\\n    self.session_manager.session_storage = flexmock()\\n    self.session_manager.session_storage.should_receive('add').with_args(\\n        'patate', self.switch_mock.switch_descriptor).once()\\n    self.switch_mock.should_receive('connect').once().ordered()\\n    session_id = self.session_manager.open_session(self.switch_mock, 'patate')\\n    self.switch_mock.should_receive('end_transaction').once().ordered()\\n    self.assertEquals(session_id, 'patate')\\n    self.session_manager.end_transaction(session_id)\\n    self.session_manager.keep_alive.assert_called_with(session_id)\\n\", 'test end transaction self self session manager keep alive mock self session manager session storage flexmock self session manager session storage should receive add with args patate self switch mock switch descriptor once self switch mock should receive connect once ordered session id self session manager open session self switch mock patate self switch mock should receive end transaction once ordered self assertequals session id patate self session manager end transaction session id self session manager keep alive assert called with session id', '')] [('setUp', 20, 'def setUp(self):\\n    self.fh = StringIO()\\n    self.styles = Styles()\\n    self.styles._set_filehandle(self.fh)\\n', 'setup self self fh stringio self styles styles self styles set filehandle self fh', ''), ('test_write_font_1', 25, 'def test_write_font_1(self):\\n    \"\"\"Test the _write_font() method. Default properties.\"\"\"\\n    properties = {}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><sz val=\"11\"/><color theme=\"1\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 1 self properties xf format format properties self styles write font xf format exp font sz val 11 color theme 1 name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method default properties'), ('test_write_font_2', 38, 'def test_write_font_2(self):\\n    \"\"\"Test the _write_font() method. Bold.\"\"\"\\n    properties = {\\'bold\\': 1}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><b/><sz val=\"11\"/><color theme=\"1\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 2 self properties bold 1 xf format format properties self styles write font xf format exp font sz val 11 color theme 1 name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method bold'), ('test_write_font_3', 51, 'def test_write_font_3(self):\\n    \"\"\"Test the _write_font() method. Italic.\"\"\"\\n    properties = {\\'italic\\': 1}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><i/><sz val=\"11\"/><color theme=\"1\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 3 self properties italic 1 xf format format properties self styles write font xf format exp font sz val 11 color theme 1 name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method italic'), ('test_write_font_4', 64, 'def test_write_font_4(self):\\n    \"\"\"Test the _write_font() method. Underline.\"\"\"\\n    properties = {\\'underline\\': 1}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><u/><sz val=\"11\"/><color theme=\"1\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 4 self properties underline 1 xf format format properties self styles write font xf format exp font sz val 11 color theme 1 name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method underline'), ('test_write_font_5', 77, 'def test_write_font_5(self):\\n    \"\"\"Test the _write_font() method. Strikeout.\"\"\"\\n    properties = {\\'font_strikeout\\': 1}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><strike/><sz val=\"11\"/><color theme=\"1\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 5 self properties font strikeout 1 xf format format properties self styles write font xf format exp font strike sz val 11 color theme 1 name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method strikeout'), ('test_write_font_6', 90, 'def test_write_font_6(self):\\n    \"\"\"Test the _write_font() method. Superscript.\"\"\"\\n    properties = {\\'font_script\\': 1}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><vertAlign val=\"superscript\"/><sz val=\"11\"/><color theme=\"1\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 6 self properties font script 1 xf format format properties self styles write font xf format exp font vertalign val superscript sz val 11 color theme 1 name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method superscript'), ('test_write_font_7', 103, 'def test_write_font_7(self):\\n    \"\"\"Test the _write_font() method. Subscript.\"\"\"\\n    properties = {\\'font_script\\': 2}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><vertAlign val=\"subscript\"/><sz val=\"11\"/><color theme=\"1\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 7 self properties font script 2 xf format format properties self styles write font xf format exp font vertalign val subscript sz val 11 color theme 1 name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method subscript'), ('test_write_font_8', 116, 'def test_write_font_8(self):\\n    \"\"\"Test the _write_font() method. Font name.\"\"\"\\n    properties = {\\'font_name\\': \\'Arial\\'}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><sz val=\"11\"/><color theme=\"1\"/><name val=\"Arial\"/><family val=\"2\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 8 self properties font name arial xf format format properties self styles write font xf format exp font sz val 11 color theme 1 name val arial family val 2 font got self fh getvalue self assertequal got exp', 'test the write font method font name'), ('test_write_font_9', 129, 'def test_write_font_9(self):\\n    \"\"\"Test the _write_font() method. Font size.\"\"\"\\n    properties = {\\'size\\': 12}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><sz val=\"12\"/><color theme=\"1\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 9 self properties size 12 xf format format properties self styles write font xf format exp font sz val 12 color theme 1 name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method font size'), ('test_write_font_10', 142, 'def test_write_font_10(self):\\n    \"\"\"Test the _write_font() method. Outline.\"\"\"\\n    properties = {\\'font_outline\\': 1}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><outline/><sz val=\"11\"/><color theme=\"1\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 10 self properties font outline 1 xf format format properties self styles write font xf format exp font outline sz val 11 color theme 1 name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method outline'), ('test_write_font_11', 155, 'def test_write_font_11(self):\\n    \"\"\"Test the _write_font() method. Shadow.\"\"\"\\n    properties = {\\'font_shadow\\': 1}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><shadow/><sz val=\"11\"/><color theme=\"1\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 11 self properties font shadow 1 xf format format properties self styles write font xf format exp font shadow sz val 11 color theme 1 name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method shadow'), ('test_write_font_12', 168, 'def test_write_font_12(self):\\n    \"\"\"Test the _write_font() method. Colour = red.\"\"\"\\n    properties = {\\'color\\': \\'#FF0000\\'}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><sz val=\"11\"/><color rgb=\"FFFF0000\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 12 self properties color xf format format properties self styles write font xf format exp font sz val 11 color rgb name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method colour red'), ('test_write_font_13', 181, 'def test_write_font_13(self):\\n    \"\"\"Test the _write_font() method. All font attributes to check order.\"\"\"\\n    properties = {\\'bold\\': 1, \\'color\\': \\'#FF0000\\', \\'font_outline\\': 1,\\n        \\'font_script\\': 1, \\'font_shadow\\': 1, \\'font_strikeout\\': 1, \\'italic\\': \\n        1, \\'size\\': 12, \\'underline\\': 1}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><b/><i/><strike/><outline/><shadow/><u/><vertAlign val=\"superscript\"/><sz val=\"12\"/><color rgb=\"FFFF0000\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 13 self properties bold 1 color font outline 1 font script 1 font shadow 1 font strikeout 1 italic 1 size 12 underline 1 xf format format properties self styles write font xf format exp font strike outline shadow vertalign val superscript sz val 12 color rgb name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method all font attributes to check order'), ('test_write_font_14', 204, 'def test_write_font_14(self):\\n    \"\"\"Test the _write_font() method. Double underline.\"\"\"\\n    properties = {\\'underline\\': 2}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><u val=\"double\"/><sz val=\"11\"/><color theme=\"1\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 14 self properties underline 2 xf format format properties self styles write font xf format exp font val double sz val 11 color theme 1 name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method double underline'), ('test_write_font_15', 217, 'def test_write_font_15(self):\\n    \"\"\"Test the _write_font() method. Double underline.\"\"\"\\n    properties = {\\'underline\\': 33}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><u val=\"singleAccounting\"/><sz val=\"11\"/><color theme=\"1\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 15 self properties underline 33 xf format format properties self styles write font xf format exp font val singleaccounting sz val 11 color theme 1 name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method double underline'), ('test_write_font_16', 230, 'def test_write_font_16(self):\\n    \"\"\"Test the _write_font() method. Double underline.\"\"\"\\n    properties = {\\'underline\\': 34}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><u val=\"doubleAccounting\"/><sz val=\"11\"/><color theme=\"1\"/><name val=\"Calibri\"/><family val=\"2\"/><scheme val=\"minor\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 16 self properties underline 34 xf format format properties self styles write font xf format exp font val doubleaccounting sz val 11 color theme 1 name val calibri family val 2 scheme val minor font got self fh getvalue self assertequal got exp', 'test the write font method double underline'), ('test_write_font_17', 243, 'def test_write_font_17(self):\\n    \"\"\"Test the _write_font() method. Hyperlink.\"\"\"\\n    properties = {\\'hyperlink\\': 1}\\n    xf_format = Format(properties)\\n    self.styles._write_font(xf_format)\\n    exp = (\\n        \\'<font><u/><sz val=\"11\"/><color theme=\"10\"/><name val=\"Calibri\"/><family val=\"2\"/></font>\\'\\n        )\\n    got = self.fh.getvalue()\\n    self.assertEqual(got, exp)\\n', 'test write font 17 self properties hyperlink 1 xf format format properties self styles write font xf format exp font sz val 11 color theme 10 name val calibri family val 2 font got self fh getvalue self assertequal got exp', 'test the write font method hyperlink')] [('color_analysis', 9, \"def color_analysis(image):\\n    if image.mode != 'RGB':\\n        image = image.convert('RGB')\\n    color_counter = Counter({color: (0) for color in Counter(CSS3_COLORS)})\\n    for pixel_count, RGB in image.getcolors(image.width * image.height):\\n        color_name = get_color_name(RGB)\\n        color_counter[color_name] += pixel_count\\n    for color in color_counter:\\n        pixel_count = image.width * image.height\\n        color_counter[color] = color_counter[color] / pixel_count\\n    return color_counter\\n\", 'color analysis image if image mode rgb image image convert rgb color counter counter color 0 for color in counter colors for pixel count rgb in image getcolors image width image height color name get color name rgb color counter color name pixel count for color in color counter pixel count image width image height color counter color color counter color pixel count return color counter', ''), ('closest_color', 27, 'def closest_color(requested_color):\\n    min_colors = {}\\n    for key, name in webcolors.css3_hex_to_names.items():\\n        r_c, g_c, b_c = webcolors.hex_to_rgb(key)\\n        rd = (r_c - requested_color[0]) ** 2\\n        gd = (g_c - requested_color[1]) ** 2\\n        bd = (b_c - requested_color[2]) ** 2\\n        min_colors[rd + gd + bd] = name\\n    return min_colors[min(min_colors.keys())]\\n', 'closest color requested color min colors for key name in webcolors hex to names items webcolors hex to rgb key rd requested color 0 2 gd requested color 1 2 bd requested color 2 2 min colors rd gd bd name return min colors min min colors keys', ''), ('get_color_name', 38, 'def get_color_name(requested_color):\\n    try:\\n        color_name = webcolors.rgb_to_name(requested_color)\\n    except ValueError:\\n        color_name = closest_color(requested_color)\\n    return color_name\\n', 'get color name requested color try color name webcolors rgb to name requested color except valueerror color name closest color requested color return color name', '')] [('_print_error', 49, \"def _print_error(seeker, opposite, error_str):\\n    import sys\\n    sys.stderr.write('Error from {opposite}: {error_str}\\\\n'.format(opposite\\n        =opposite, error_str=error_str))\\n\", 'print error seeker opposite error str import sys sys stderr write error from opposite error str format opposite opposite error str error str', ''), ('_seekerCallback', 57, \"def _seekerCallback(seeker, svca):\\n    global SERVICES_DISCOVERED\\n    SERVICES_DISCOVERED.append(SCOOPool(svca.addr, svca.port, svca.aname))\\n    scoop.logger.info('Discovery seeker has found a broker.')\\n\", 'seekercallback seeker svca global services discovered services discovered append scoopool svca addr svca port svca aname scoop logger info discovery seeker has found broker', ''), ('Advertise', 69, 'def Advertise(port, stype=\\'SCOOP\\', sname=\\'Broker\\', advertisername=\\'Broker\\',\\n    location=\\'\\'):\\n    \"\"\"\\n    stype = always SCOOP\\n    port = comma separated ports\\n    sname = broker unique name\\n    location = routable location (ip or dns)\\n    \"\"\"\\n    scoop.logger.info(\\'Launching advertiser...\\')\\n    service = minusconf.Service(stype, port, sname, location)\\n    advertiser = minusconf.ThreadAdvertiser([service], advertisername)\\n    advertiser.start()\\n    scoop.logger.info(\\'Advertiser launched.\\')\\n    return advertiser\\n', 'advertise port stype scoop sname broker advertisername broker location scoop logger info launching advertiser service minusconf service stype port sname location advertiser minusconf threadadvertiser service advertisername advertiser start scoop logger info advertiser launched return advertiser', 'stype always scoop port comma separated ports sname broker unique name location routable location ip or dns'), ('Seek', 86, \"def Seek(stype='SCOOP', sname='Broker', advertisername=''):\\n    scoop.logger.info('Launching discovery seeker...')\\n    se = minusconf.Seeker(stype=stype, aname=advertisername, sname=sname,\\n        find_callback=_seekerCallback, error_callback=_print_error)\\n    se.run()\\n    return SERVICES_DISCOVERED\\n\", 'seek stype scoop sname broker advertisername scoop logger info launching discovery seeker se minusconf seeker stype stype aname advertisername sname sname find callback seekercallback error callback print error se run return services discovered', ''), ('__init__', 24, 'def __init__(self, host, ports, name):\\n    self._host = host\\n    self._ports = ports\\n    self._name = name\\n', 'init self host ports name self host host self ports ports self name name', ''), ('host', 29, '@property\\ndef host(self):\\n    \"\"\"Get address of discovered broker.\\n        Will convert back to IPv4 accordingly.\"\"\"\\n    host = self._host\\n    if host[:7] == \\'::ffff:\\':\\n        host = host[7:]\\n    return host\\n', 'host self host self host if host 7 ffff host host 7 return host', 'get address of discovered broker will convert back to ipv4 accordingly'), ('ports', 40, \"@property\\ndef ports(self):\\n    return self._ports.split(',')\\n\", 'ports self return self ports split', ''), ('name', 44, '@property\\ndef name(self):\\n    return self._name\\n', 'name self return self name', '')] [('__str__', 28, 'def __str__(self):\\n    return self.label\\n', 'str self return self label', '')] [('__init__', 16, 'def __init__(self, server_address, bind_and_activate=True):\\n    \"\"\"Constructor.  May be extended, do not override.\"\"\"\\n    socketserver.UDPServer.__init__(self, server_address, None,\\n        bind_and_activate)\\n    self.debug_message_size = False\\n', 'init self server address bind and activate true socketserver udpserver init self server address none bind and activate self debug message size false', 'constructor may be extended do not override'), ('service_actions', 23, 'def service_actions(self):\\n    \"\"\"Called by the server_forever() loop\"\"\"\\n    pass\\n', 'service actions self pass', 'called by the server forever loop'), ('finish_request', 27, \"def finish_request(self, request, socket_address):\\n    if self.debug_message_size:\\n        print('[SOCKET INCOMING SIZE] {}'.format(sys.getsizeof(request[0])))\\n    self.message_received(request[0], socket_address)\\n\", 'finish request self request socket address if self debug message size print socket incoming size format sys getsizeof request 0 self message received request 0 socket address', ''), ('sendto', 34, 'def sendto(self, address, data):\\n    \"\"\"Send data to specific address\"\"\"\\n    if self.debug_message_size:\\n        print(\\'[SOCKET OUTGOING SIZE] {}\\'.format(sys.getsizeof(data)))\\n    self.socket.sendto(data, address)\\n', 'sendto self address data if self debug message size print socket outgoing size format sys getsizeof data self socket sendto data address', 'send data to specific address'), ('message_received', 40, 'def message_received(self, data, socket_address):\\n    \"\"\" This is called when we receive data. Override this. \"\"\"\\n    pass\\n', 'message received self data socket address pass', 'this is called when we receive data override this'), ('__init__', 52, 'def __init__(self, server_address, bind_and_activate=True):\\n    ThreadedUDPServer.__init__(self, server_address, bind_and_activate)\\n    self.clients = []\\n    self.heartbeat_rate = 30\\n    self._heartbeats = {}\\n    self._last_time = time.time()\\n    self.handlers = {}\\n    self._message_protocol = MessageProtocol()\\n    self.debug_message_unhandled = True\\n', 'init self server address bind and activate true threadedudpserver init self server address bind and activate self clients self heartbeat rate 30 self heartbeats self last time time time self handlers self message protocol messageprotocol self debug message unhandled true', ''), ('service_actions', 75, 'def service_actions(self):\\n    \"\"\"Called by the server_forever() loop\"\"\"\\n    time_now = time.time()\\n    delta = time_now - self._last_time\\n    self._last_time = time_now\\n    dead_clients = []\\n    if self.heartbeat_rate > 0:\\n        for client in self._heartbeats:\\n            heart = self._heartbeats[client]\\n            heart += delta\\n            if heart > self.heartbeat_rate:\\n                dead_clients.append(client)\\n            else:\\n                self._heartbeats[client] = heart\\n    for client in dead_clients:\\n        del self._heartbeats[client]\\n        self.clients.remove(client)\\n        self._trigger(\\'disconnected\\', None, client)\\n', 'service actions self time now time time delta time now self last time self last time time now dead clients if self heartbeat rate 0 for client in self heartbeats heart self heartbeats client heart delta if heart self heartbeat rate dead clients append client else self heartbeats client heart for client in dead clients del self heartbeats client self clients remove client self trigger disconnected none client', 'called by the server forever loop'), ('_trigger', 102, \"def _trigger(self, event, data, addr):\\n    if event in self.handlers:\\n        self.handlers[event](data, addr)\\n    elif self.debug_message_unhandled:\\n        print('Unhandled event [{}]. Payload: {}'.format(event, data))\\n\", 'trigger self event data addr if event in self handlers self handlers event data addr elif self debug message unhandled print unhandled event payload format event data', ''), ('on', 108, 'def on(self, event, handler=None):\\n    \"\"\" Used to register a function/method to handle a particular message \"\"\"\\n\\n    def set_handler(handler):\\n        self.handlers[event] = handler\\n        return handler\\n    if handler is None:\\n        return set_handler\\n    set_handler(handler)\\n', 'on self event handler none def set handler handler self handlers event handler return handler if handler is none return set handler set handler handler', 'used to register a function method to handle a particular message'), ('send', 118, 'def send(self, client, event, payload):\\n    \"\"\"Send message to specific client\"\"\"\\n    msg = self._message_protocol.create(event, payload)\\n    if self.debug_message_size:\\n        print(\\'[SOCKET OUTGOING SIZE] {}\\'.format(sys.getsizeof(msg)))\\n    super(EventServer, self).sendto(client, msg)\\n', 'send self client event payload msg self message protocol create event payload if self debug message size print socket outgoing size format sys getsizeof msg super eventserver self sendto client msg', 'send message to specific client'), ('send_all', 125, 'def send_all(self, event, payload):\\n    \"\"\" Send message to all connected clients. \"\"\"\\n    for client in self.clients:\\n        self.send(client, event, payload)\\n', 'send all self event payload for client in self clients self send client event payload', 'send message to all connected clients'), ('message_received', 130, \"def message_received(self, data, socket_address):\\n    if socket_address not in self.clients:\\n        self.clients.append(socket_address)\\n        self._heartbeats[socket_address] = 0\\n        self._trigger('connected', None, socket_address)\\n    else:\\n        self._heartbeats[socket_address] = 0\\n    message = self._message_protocol.parse(data)\\n    message_type = message[0]\\n    payload = message[3]\\n    self._trigger(message_type, payload, socket_address)\\n\", 'message received self data socket address if socket address not in self clients self clients append socket address self heartbeats socket address 0 self trigger connected none socket address else self heartbeats socket address 0 message self message protocol parse data message type message 0 payload message 3 self trigger message type payload socket address', '')] [('get_texts', 31, 'def get_texts(self):\\n    \"\"\"\\n        Parse documents from the .cor file provided in the constructor. Lowercase\\n        each document and ignore some stopwords.\\n\\n        .cor format: one document per line, words separated by whitespace.\\n\\n        \"\"\"\\n    for doc in self.getstream():\\n        yield [word for word in utils.to_unicode(doc).lower().split() if \\n            word not in CorpusMiislita.stoplist]\\n', 'get texts self for doc in self getstream yield word for word in utils to unicode doc lower split if word not in corpusmiislita stoplist', 'parse documents from the cor file provided in the constructor lowercase each document and ignore some stopwords'), ('__len__', 43, 'def __len__(self):\\n    \"\"\"Define this so we can use `len(corpus)`\"\"\"\\n    if \\'length\\' not in self.__dict__:\\n        logger.info(\\'caching corpus size (calculating number of documents)\\')\\n        self.length = sum(1 for _ in self.get_texts())\\n    return self.length\\n', 'len self if length not in self dict logger info caching corpus size calculating number of documents self length sum 1 for in self get texts return self length', 'define this so we can use len corpus'), ('test_textcorpus', 52, 'def test_textcorpus(self):\\n    \"\"\"Make sure TextCorpus can be serialized to disk. \"\"\"\\n    miislita = CorpusMiislita(datapath(\\'head500.noblanks.cor.bz2\\'))\\n    ftmp = get_tmpfile(\\'test_textcorpus.mm\\')\\n    corpora.MmCorpus.save_corpus(ftmp, miislita)\\n    self.assertTrue(os.path.exists(ftmp))\\n    miislita2 = corpora.MmCorpus(ftmp)\\n    self.assertEqual(list(miislita), list(miislita2))\\n', 'test textcorpus self miislita corpusmiislita datapath noblanks cor ftmp get tmpfile test textcorpus mm corpora mmcorpus save corpus ftmp miislita self asserttrue os path exists ftmp corpora mmcorpus ftmp self assertequal list miislita list', 'make sure textcorpus can be serialized to disk'), ('test_save_load_ability', 66, 'def test_save_load_ability(self):\\n    \"\"\"\\n        Make sure we can save and load (un/pickle) TextCorpus objects (as long\\n        as the underlying input isn\\'t a file-like object; we cannot pickle those).\\n        \"\"\"\\n    corpusname = datapath(\\'miIslita.cor\\')\\n    miislita = CorpusMiislita(corpusname)\\n    tmpf = get_tmpfile(\\'tc_test.cpickle\\')\\n    miislita.save(tmpf)\\n    miislita2 = CorpusMiislita.load(tmpf)\\n    self.assertEqual(len(miislita), len(miislita2))\\n    self.assertEqual(miislita.dictionary.token2id, miislita2.dictionary.\\n        token2id)\\n', 'test save load ability self corpusname datapath miislita cor miislita corpusmiislita corpusname tmpf get tmpfile tc test cpickle miislita save tmpf corpusmiislita load tmpf self assertequal len miislita len self assertequal miislita dictionary dictionary', 'make sure we can save and load un pickle textcorpus objects as long as the underlying input isn t a file like object we cannot pickle those'), ('test_miislita_high_level', 84, \"def test_miislita_high_level(self):\\n    miislita = CorpusMiislita(datapath('miIslita.cor'))\\n    tfidf = models.TfidfModel(miislita, miislita.dictionary, normalize=False)\\n    index = similarities.SparseMatrixSimilarity(tfidf[miislita],\\n        num_features=len(miislita.dictionary))\\n    query = 'latent semantic indexing'\\n    vec_bow = miislita.dictionary.doc2bow(query.lower().split())\\n    vec_tfidf = tfidf[vec_bow]\\n    sims_tfidf = index[vec_tfidf]\\n    expected = [0.0, 0.256, 0.7022, 0.1524, 0.3334]\\n    for i, value in enumerate(expected):\\n        self.assertAlmostEqual(sims_tfidf[i], value, 2)\\n\", 'test miislita high level self miislita corpusmiislita datapath miislita cor tfidf models tfidfmodel miislita miislita dictionary normalize false index similarities sparsematrixsimilarity tfidf miislita num features len miislita dictionary query latent semantic indexing vec bow miislita dictionary query lower split vec tfidf tfidf vec bow sims tfidf index vec tfidf expected 0 0 0 256 0 7022 0 1524 0 3334 for value in enumerate expected self assertalmostequal sims tfidf value 2', '')] [('test01_sum_01', 5, \"def test01_sum_01(self):\\n    m = CPlexModel()\\n    x = m.new((2, 2), name='x')\\n    A = arange(1, 5).reshape((2, 2))\\n    m.constrain(x.sum(axis=0) <= 1)\\n    m.constrain(x >= 0)\\n    m.maximize((A * x.A).sum())\\n    self.assertEqual(x[0, 0], 0)\\n    self.assertEqual(x[0, 1], 1)\\n    self.assertEqual(x[1, 0], 0)\\n    self.assertEqual(x[1, 1], 1)\\n\", 'sum 01 self cplexmodel new 2 2 name arange 1 5 reshape 2 2 constrain sum axis 0 1 constrain 0 maximize sum self assertequal 0 0 0 self assertequal 0 1 1 self assertequal 1 0 0 self assertequal 1 1 1', ''), ('test01_sum_02', 23, \"def test01_sum_02(self):\\n    m = CPlexModel()\\n    x = m.new((2, 2), name='x')\\n    A = arange(1, 5).reshape((2, 2))\\n    m.constrain(x.T.sum(axis=1) <= ar([1, 2]))\\n    m.constrain(x >= 0)\\n    m.maximize((A * x.A).sum())\\n    self.assertEqual(x[0, 0], 0)\\n    self.assertEqual(x[0, 1], 1)\\n    self.assertEqual(x[1, 0], 0)\\n    self.assertEqual(x[1, 1], 2)\\n\", 'sum 02 self cplexmodel new 2 2 name arange 1 5 reshape 2 2 constrain sum axis 1 ar 1 2 constrain 0 maximize sum self assertequal 0 0 0 self assertequal 0 1 1 self assertequal 1 0 0 self assertequal 1 1 2', '')] [('check_prerequisites', 44, \"def check_prerequisites(self):\\n    has_server = self.check_num_servers(1)\\n    srv = self.servers.get_server(0)\\n    if not srv.check_version_compat(5, 7, 6):\\n        raise MUTLibError('Test requires server version 5.7.6 and later.')\\n    return has_server\\n\", 'check prerequisites self has server self check num servers 1 srv self servers get server 0 if not srv check version compat 5 7 6 raise mutliberror test requires server version 5 7 6 and later return has server', ''), ('setup', 51, \"def setup(self):\\n    self.start_cmd_fl = os.path.join(os.getcwd(), 'restart_server.bat' if \\n        os.name == 'nt' else 'restart_server.sh')\\n    return True\\n\", 'setup self self start cmd fl os path join os getcwd restart server bat if os name nt else restart server sh return true', ''), ('_test_server_clone', 59, 'def _test_server_clone(self, cmd_str, comment, kill=True, capture_all=False,\\n    restart=None):\\n    \"\"\"Test server clone.\\n\\n        cmd_str[in]       Command to be executed.\\n        comment[in]       Test comment.\\n        kill[in]          True for kill process.\\n        capture_all[in]   True for capture all rows.\\n        restart[in]       True for restart server.\\n        \"\"\"\\n    quote_char = \"\\'\" if os.name == \\'posix\\' else \\'\"\\'\\n    self.results.append(comment + \\'\\\\n\\')\\n    port1 = int(self.servers.get_next_port())\\n    cmd_str = \\'{0} --new-port={1} \\'.format(cmd_str, port1)\\n    full_datadir = os.path.join(os.getcwd(), \\'temp with spaces {0}\\'.format(\\n        port1))\\n    cmd_str = \\'{0} --new-data={2}{1}{2} --delete \\'.format(cmd_str,\\n        full_datadir, quote_char)\\n    res = self.exec_util(cmd_str, \\'start.txt\\')\\n    with open(\\'start.txt\\') as f:\\n        for line in f:\\n            index = line.find(\\'[Warning]\\')\\n            if capture_all or index <= 0 and line[0] == \\'#\\':\\n                self.results.append(line)\\n    if res:\\n        raise MUTLibError(\\'{0}: failed\\'.format(comment))\\n    conn = {\\'user\\': \\'root\\', \\'passwd\\': \\'root\\', \\'host\\': \\'localhost\\', \\'port\\':\\n        port1}\\n    server_options = {\\'conn_info\\': conn, \\'role\\': \\'cloned_server_2\\'}\\n    new_server = Server(server_options)\\n    if new_server is None:\\n        return False\\n    if kill:\\n        try:\\n            new_server.connect()\\n        except UtilError:\\n            new_server = None\\n            raise MUTLibError(\\'Cannot connect to spawned server.\\')\\n        drop = False if restart else True\\n        self.servers.stop_server(new_server, drop=drop)\\n        time.sleep(5)\\n        if restart:\\n            if os.name == \\'posix\\':\\n                fl_st = os.stat(self.start_cmd_fl)\\n                os.chmod(self.start_cmd_fl, fl_st.st_mode | stat.S_IEXEC)\\n            time.sleep(3)\\n            run_script = restart\\n            with open(\\'restart.txt\\', \\'a+\\') as f_out:\\n                if self.debug:\\n                    print\\n                    print(\\'executing script: {0}\\'.format(run_script))\\n                    subprocess.Popen(run_script)\\n                else:\\n                    subprocess.Popen(run_script, stdout=f_out, stderr=f_out)\\n            max_tries, attempt = 3, 0\\n            while attempt < max_tries:\\n                try:\\n                    time.sleep(3)\\n                    new_server.connect()\\n                    break\\n                except UtilError:\\n                    attempt += 1\\n            else:\\n                new_server = None\\n                raise MUTLibError(CANNOT_START_SERVER_ERR)\\n            self.servers.stop_server(new_server)\\n    return True\\n', 'test server clone self cmd str comment kill true capture all false restart none quote char if os name posix else self results append comment int self servers get next port cmd str 0 new port 1 format cmd str full datadir os path join os getcwd temp with spaces 0 format cmd str 0 new data 2 1 2 delete format cmd str full datadir quote char res self exec util cmd str start txt with open start txt as for line in index line find warning if capture all or index 0 and line 0 self results append line if res raise mutliberror 0 failed format comment conn user root passwd root host localhost port server options conn info conn role cloned server 2 new server server server options if new server is none return false if kill try new server connect except utilerror new server none raise mutliberror cannot connect to spawned server drop false if restart else true self servers stop server new server drop drop time sleep 5 if restart if os name posix fl st os stat self start cmd fl os chmod self start cmd fl fl st st mode stat iexec time sleep 3 run script restart with open restart txt as out if self debug print print executing script 0 format run script subprocess popen run script else subprocess popen run script stdout out stderr out max tries attempt 3 0 while attempt max tries try time sleep 3 new server connect break except utilerror attempt 1 else new server none raise mutliberror cannot start server err self servers stop server new server return true', 'test server clone'), ('run', 141, 'def run(self):\\n    self.res_fname = \\'result.txt\\'\\n    base_cmd = \\'mysqlserverclone.py --server={0} --root-password=root \\'.format(\\n        self.build_connection_string(self.servers.get_server(0)))\\n    test_cases = [(\\'show help\\', \\' --help \\', False, True, False), (\\n        \\'write command to file\\', \\' --write-command=startme.sh \\', True,\\n        False, False), (\\'write command to file shortcut\\', \\' -w startme.sh \\',\\n        True, False, False), (\\'verbosity = -v\\', \\' -v \\', True, False, False),\\n        (\\'verbosity = -vv\\', \\' -vv \\', True, False, False), (\\n        \\'verbosity = -vvv\\', \\' -vvv \\', True, False, False), (\\n        \\'-vvv and write command to file shortcut\\', \\' -vvv -w startme.sh \\',\\n        True, False, False), (\\'write command to file and run it\\',\\n        \\' -w {0} \\'.format(self.start_cmd_fl), True, False, self.start_cmd_fl)]\\n    test_num = 1\\n    for row in test_cases:\\n        new_comment = \\'Test case {0} : {1}\\'.format(test_num, row[0])\\n        if not self._test_server_clone(\\'{0}{1}\\'.format(base_cmd, row[1]),\\n            new_comment, row[2], row[3], row[4]):\\n            raise MUTLibError(\\'{0}: failed\\'.format(new_comment))\\n        test_num += 1\\n    user = None\\n    try:\\n        user = os.environ[\\'USERNAME\\']\\n    except KeyError:\\n        user = os.environ[\\'LOGNAME\\']\\n    finally:\\n        if not user:\\n            raise MUTLibError(\\'Cannot obtain user name for test case.\\')\\n    comment = \\'Test case {0}: - User the --user option\\'.format(test_num)\\n    if not self._test_server_clone(\\'{0}--user={1}\\'.format(base_cmd, user),\\n        comment, True, False):\\n        raise MUTLibError(\\'{0}: failed\\'.format(comment))\\n    test_num += 1\\n    self.replace_result(\\'#  -uroot\\', \\'#  -uroot [...]\\\\n\\')\\n    self.replace_result(\\'#                       mysqld:\\',\\n        \"\"\"#                       mysqld: XXXXXXXXXXXX\\n\"\"\")\\n    self.replace_result(\\'#                   mysqladmin:\\',\\n        \"\"\"#                   mysqladmin: XXXXXXXXXXXX\\n\"\"\")\\n    self.replace_result(\\'# Cloning the MySQL server running on \\',\\n        \"\"\"# Cloning the MySQL server running on XXXXX-XXXXX.\\n\"\"\")\\n    self.remove_result(\\'# trying again...\\')\\n    self.remove_result(\"# WARNING: The socket file path \\'\")\\n    self.remove_result_and_lines_after(\\n        \\'MySQL Utilities mysqlserverclone version\\', 6)\\n    return True\\n', 'run self self res fname result txt base cmd mysqlserverclone py server 0 root password root format self build connection string self servers get server 0 test cases show help help false true false write command to file write command startme sh true false false write command to file shortcut startme sh true false false verbosity true false false verbosity vv vv true false false verbosity vvv vvv true false false vvv and write command to file shortcut vvv startme sh true false false write command to file and run it 0 format self start cmd fl true false self start cmd fl test num 1 for row in test cases new comment test case 0 1 format test num row 0 if not self test server clone 0 1 format base cmd row 1 new comment row 2 row 3 row 4 raise mutliberror 0 failed format new comment test num 1 user none try user os environ username except keyerror user os environ logname finally if not user raise mutliberror cannot obtain user name for test case comment test case 0 user the user option format test num if not self test server clone 0 user 1 format base cmd user comment true false raise mutliberror 0 failed format comment test num 1 self replace result uroot uroot self replace result mysqld mysqld xxxxxxxxxxxx self replace result mysqladmin mysqladmin xxxxxxxxxxxx self replace result cloning the mysql server running on cloning the mysql server running on xxxxx xxxxx self remove result trying again self remove result warning the socket file path self remove result and lines after mysql utilities mysqlserverclone version 6 return true', ''), ('get_result', 208, 'def get_result(self):\\n    return self.compare(__name__, self.results)\\n', 'get result self return self compare name self results', ''), ('record', 211, 'def record(self):\\n    return self.save_result_file(__name__, self.results)\\n', 'record self return self save result file name self results', ''), ('_remove_file', 214, '@staticmethod\\ndef _remove_file(filename):\\n    \"\"\"Remove file.\\n\\n        filename[in]   Filename to be removed.\\n        \"\"\"\\n    try:\\n        os.unlink(filename)\\n    except OSError:\\n        pass\\n', 'remove file filename try os unlink filename except oserror pass', 'remove file'), ('cleanup', 225, \"def cleanup(self):\\n    files = [self.res_fname, 'start.txt', 'startme.sh', self.start_cmd_fl,\\n        'restart.txt']\\n    for file_ in files:\\n        self._remove_file(file_)\\n    return True\\n\", 'cleanup self files self res fname start txt startme sh self start cmd fl restart txt for file in files self remove file file return true', '')] [('_real_extract', 50, \"def _real_extract(self, url):\\n    site, path, display_id = re.match(self._VALID_URL, url).groups()\\n    webpage = self._download_webpage(url, display_id)\\n    react_data = self._parse_json(self._search_regex(\\n        'window\\\\\\\\.__reactTransmitPacket\\\\\\\\s*=\\\\\\\\s*({.+?});', webpage,\\n        'react data'), display_id)\\n    content_blocks = react_data['layout'][path]['contentBlocks']\\n    video = next(cb for cb in content_blocks if cb.get('type') == 'video')[\\n        'content']['items'][0]\\n    video_id = video['id']\\n    access_token = None\\n    cookies = self._get_cookies(url)\\n    auth_storage_cookie = cookies.get('eosAf') or cookies.get('eosAn')\\n    if auth_storage_cookie and auth_storage_cookie.value:\\n        auth_storage = self._parse_json(compat_urllib_parse_unquote(\\n            compat_urllib_parse_unquote(auth_storage_cookie.value)),\\n            video_id, fatal=False) or {}\\n        access_token = auth_storage.get('a') or auth_storage.get('access_token'\\n            )\\n    if not access_token:\\n        access_token = self._download_json('https://www.%s.com/anonymous' %\\n            site, display_id, query={'authRel': 'authorization',\\n            'client_id': try_get(react_data, lambda x: x['application'][\\n            'apiClientId'], compat_str) or '3020a40c2356a645b4b4', 'nonce':\\n            ''.join([random.choice(string.ascii_letters) for _ in range(32)\\n            ]), 'redirectUri': \\n            'https://fusion.ddmcdn.com/app/mercury-sdk/180/redirectHandler.html?https://www.%s.com'\\n             % site})['access_token']\\n    try:\\n        stream = self._download_json(\\n            'https://api.discovery.com/v1/streaming/video/' + video_id,\\n            display_id, headers={'Authorization': 'Bearer ' + access_token})\\n    except ExtractorError as e:\\n        if isinstance(e.cause, compat_HTTPError) and e.cause.code in (401, 403\\n            ):\\n            e_description = self._parse_json(e.cause.read().decode(),\\n                display_id)['description']\\n            if 'resource not available for country' in e_description:\\n                self.raise_geo_restricted(countries=self._GEO_COUNTRIES)\\n            if 'Authorized Networks' in e_description:\\n                raise ExtractorError(\\n                    'This video is only available via cable service provider subscription that is not currently supported. You may want to use --cookies.'\\n                    , expected=True)\\n            raise ExtractorError(e_description)\\n        raise\\n    return self._extract_video_info(video, stream, display_id)\\n\", 'real extract self url site path display id re match self valid url url groups webpage self download webpage url display id react data self parse json self search regex window reacttransmitpacket webpage react data display id content blocks react data layout path contentblocks video next cb for cb in content blocks if cb get type video content items 0 video id video id access token none cookies self get cookies url auth storage cookie cookies get eosaf or cookies get eosan if auth storage cookie and auth storage cookie value auth storage self parse json compat urllib parse unquote compat urllib parse unquote auth storage cookie value video id fatal false or access token auth storage get or auth storage get access token if not access token access token self download json https www com anonymous site display id query authrel authorization client id try get react data lambda application apiclientid compat str or nonce join random choice string ascii letters for in range 32 redirecturi https fusion ddmcdn com app mercury sdk 180 redirecthandler html https www com site access token try stream self download json https api discovery com streaming video video id display id headers authorization bearer access token except extractorerror as if isinstance cause compat httperror and cause code in 401 403 description self parse json cause read decode display id description if resource not available for country in description self raise geo restricted countries self geo countries if authorized networks in description raise extractorerror this video is only available via cable service provider subscription that is not currently supported you may want to use cookies expected true raise extractorerror description raise return self extract video info video stream display id', '')] [('__init__', 16, \"def __init__(self, config_options):\\n    if not requests_available:\\n        print('Requests package is not available, cannot use SlackAlerter.')\\n        print('Try: pip install -r requirements.txt')\\n        return\\n    Alerter.__init__(self, config_options)\\n    self.url = Alerter.get_config_option(config_options, 'url', required=\\n        True, allow_empty=False)\\n    self.channel = Alerter.get_config_option(config_options, 'channel')\\n    self.username = Alerter.get_config_option(config_options, 'username')\\n\", 'init self config options if not requests available print requests package is not available cannot use slackalerter print try pip install requirements txt return alerter init self config options self url alerter get config option config options url required true allow empty false self channel alerter get config option config options channel self username alerter get config option config options username', ''), ('send_alert', 33, 'def send_alert(self, name, monitor):\\n    \"\"\"Send the message.\"\"\"\\n    type = self.should_alert(monitor)\\n    days, hours, minutes, seconds = self.get_downtime(monitor)\\n    if self.channel is not None:\\n        message_json = {\\'channel\\': self.channel}\\n    elif self.username is not None:\\n        message_json = {\\'username\\': self.username}\\n    else:\\n        message_json = {}\\n    message_json[\\'attachments\\'] = [{}]\\n    if type == \\'\\':\\n        return\\n    elif type == \\'failure\\':\\n        message_json[\\'text\\'] = \\'Monitor {} failed!\\'.format(name)\\n        message_json[\\'attachments\\'][0][\\'color\\'] = \\'danger\\'\\n        fields = [{\\'title\\': \\'Failed at\\', \\'value\\': self.format_datetime(\\n            monitor.first_failure_time()), \\'short\\': True}, {\\'title\\':\\n            \\'Downtime\\', \\'value\\': \\'{}+{:02d}:{:02d}:{:02d}\\'.format(days,\\n            hours, minutes, seconds), \\'short\\': True}, {\\'title\\':\\n            \\'Virtual failure count\\', \\'value\\': monitor.virtual_fail_count(),\\n            \\'short\\': True}, {\\'title\\': \\'Host\\', \\'value\\': self.hostname,\\n            \\'short\\': True}, {\\'title\\': \\'Additional info\\', \\'value\\': monitor.\\n            get_result()}, {\\'title\\': \\'Description\\', \\'value\\': monitor.\\n            describe()}]\\n        try:\\n            if monitor.recover_info != \\'\\':\\n                fields.append({\\'title\\': \\'Recovery info\\', \\'value\\': \\n                    \\'Recovery info: %s\\' % monitor.recover_info})\\n                message_json[\\'attachments\\'][0][\\'color\\'] = \\'warning\\'\\n        except AttributeError:\\n            pass\\n        message_json[\\'attachments\\'][0][\\'fields\\'] = fields\\n    elif type == \\'success\\':\\n        message_json[\\'text\\'] = \\'Monitor {} succeeded.\\'.format(name)\\n        fields = [{\\'title\\': \\'Failed at\\', \\'value\\': self.format_datetime(\\n            monitor.first_failure_time()), \\'short\\': True}, {\\'title\\':\\n            \\'Downtime\\', \\'value\\': \\'{}+{:02d}:{:02d}:{:02d}\\'.format(days,\\n            hours, minutes, seconds), \\'short\\': True}, {\\'title\\': \\'Host\\',\\n            \\'value\\': self.hostname, \\'short\\': True}, {\\'title\\': \\'Description\\',\\n            \\'value\\': monitor.describe()}]\\n        message_json[\\'attachments\\'][0][\\'color\\'] = \\'good\\'\\n        message_json[\\'attachments\\'][0][\\'fields\\'] = fields\\n    else:\\n        print(\\'Unknown alert type %s\\' % type)\\n        return\\n    if not self.dry_run:\\n        try:\\n            r = requests.post(self.url, json=message_json)\\n            if not r.status_code == 200:\\n                print(\\'POST to slack webhook failed\\')\\n                print(r)\\n        except Exception as e:\\n            print(\\'Failed to post to slack webhook\\')\\n            print(e)\\n            print(message_json)\\n            self.available = False\\n    else:\\n        print(\\'dry_run: would send slack: %s\\' % message_json.__repr__())\\n', 'send alert self name monitor type self should alert monitor days hours minutes seconds self get downtime monitor if self channel is not none message json channel self channel elif self username is not none message json username self username else message json message json attachments if type return elif type failure message json text monitor failed format name message json attachments 0 color danger fields title failed at value self format datetime monitor first failure time short true title downtime value format days hours minutes seconds short true title virtual failure count value monitor virtual fail count short true title host value self hostname short true title additional info value monitor get result title description value monitor describe try if monitor recover info fields append title recovery info value recovery info monitor recover info message json attachments 0 color warning except attributeerror pass message json attachments 0 fields fields elif type success message json text monitor succeeded format name fields title failed at value self format datetime monitor first failure time short true title downtime value format days hours minutes seconds short true title host value self hostname short true title description value monitor describe message json attachments 0 color good message json attachments 0 fields fields else print unknown alert type type return if not self dry run try requests post self url json message json if not status code 200 print post to slack webhook failed print except exception as print failed to post to slack webhook print print message json self available false else print dry run would send slack message json repr', 'send the message')] [('visit_todo_node', 40, 'def visit_todo_node(self, node):\\n    self.visit_admonition(node)\\n', 'visit todo node self node self visit admonition node', ''), ('depart_todo_node', 43, 'def depart_todo_node(self, node):\\n    self.depart_admonition(node)\\n', 'depart todo node self node self depart admonition node', ''), ('column_type_str', 46, 'def column_type_str(column):\\n    \"\"\"Extract the type name from a SQLA column\\n    \"\"\"\\n    type_ = column.type\\n    if type(type_) in (types.Integer, types.SmallInteger):\\n        return \\'int\\'\\n    if type(type_) == types.Boolean:\\n        return \\'bool\\'\\n    if type(type_) == types.Unicode:\\n        return \\'unicode – %s\\' % column.info[\\'format\\']\\n    if type(type_) == types.UnicodeText:\\n        return \\'unicode – %s\\' % column.info[\\'format\\']\\n    if type(type_) == types.Enum:\\n        return \\'enum: [%s]\\' % \\', \\'.join(type_.enums)\\n    if type(type_) == markdown.MarkdownColumn:\\n        return \\'markdown\\'\\n    raise ValueError(repr(type_))\\n', 'column type str column type column type if type type in types integer types smallinteger return int if type type types boolean return bool if type type types unicode return unicode column info format if type type types unicodetext return unicode column info format if type type types enum return enum join type enums if type type markdown markdowncolumn return markdown raise valueerror repr type', 'extract the type name from a sqla column'), ('column_header', 67, 'def column_header(c, class_name=None, transl_name=None, show_type=True,\\n    relation=None, relation_name=None):\\n    \"\"\"Return the column header for the given column\"\"\"\\n    result = []\\n    if relation_name:\\n        name = relation_name\\n    else:\\n        name = c.name\\n    if class_name:\\n        result.append(\\'%s.\\\\\\\\ **%s**\\' % (class_name, name))\\n    else:\\n        result.append(\\'**%s**\\' % c.name)\\n    if c.foreign_keys:\\n        for fk in c.foreign_keys:\\n            if fk.column in column_to_cls:\\n                foreign_cls = column_to_cls[fk.column]\\n                if relation_name and relation_name + \\'_id\\' == c.name:\\n                    result.append(\\'(%s →\\' % c.name)\\n                elif relation_name:\\n                    result.append(\\'(**%s** →\\' % c.name)\\n                else:\\n                    result.append(\\'(→\\')\\n                result.append(\\':class:`~pokedex.db.tables.%s`.%s)\\' % (\\n                    foreign_cls.__name__, fk.column.name))\\n                break\\n    elif show_type:\\n        result.append(\\'(*%s*)\\' % column_type_str(c))\\n    if transl_name:\\n        result.append(\\'via *%s*\\' % transl_name)\\n    return \\' \\'.join(result)\\n', 'column header class name none transl name none show type true relation none relation name none result if relation name name relation name else name name if class name result append class name name else result append name if foreign keys for fk in foreign keys if fk column in column to cls foreign cls column to cls fk column if relation name and relation name id name result append name elif relation name result append name else result append result append class pokedex db tables foreign cls name fk column name break elif show type result append column type str if transl name result append via transl name return join result', 'return the column header for the given column'), ('with_header', 101, 'def with_header(header=None):\\n    \"\"\"Decorator that adds a section header if there\\'s a any output\\n\\n    The decorated function should yield output lines; if there are any the\\n    header gets added.\\n    \"\"\"\\n\\n    def wrap(func):\\n\\n        @functools.wraps(func)\\n        def wrapped(cls, remaining_attrs):\\n            result = list(func(cls, remaining_attrs))\\n            if result:\\n                yield \\'\\'\\n                yield \\'.. raw:: html\\'\\n                yield \\'\\'\\n                yield \\'    <hr>\\'\\n                yield \\'\\'\\n                if header:\\n                    yield header + \\':\\'\\n                    yield \\'\\'\\n                for row in result:\\n                    yield row\\n        return wrapped\\n    return wrap\\n', 'with header header none def wrap func functools wraps func def wrapped cls remaining attrs result list func cls remaining attrs if result yield yield raw html yield yield hr yield if header yield header yield for row in result yield row return wrapped return wrap', 'decorator that adds a section header if there s a any output'), ('generate_table_header', 128, \"def generate_table_header(cls, remaining_attrs):\\n    first_line, sep, next_lines = six.text_type(cls.__doc__).partition('\\\\n')\\n    yield first_line\\n    for line in textwrap.dedent(next_lines).split('\\\\n'):\\n        yield line\\n    yield ''\\n    yield 'Table name: *%s*' % cls.__tablename__\\n    try:\\n        yield '(single: *%s*)' % cls.__singlename__\\n    except AttributeError:\\n        pass\\n    yield ''\\n    yield 'Primary key: %s.' % ', '.join('**%s**' % col.key for col in cls.\\n        __table__.primary_key.columns)\\n    yield ''\\n\", 'generate table header cls remaining attrs first line sep next lines six text type cls doc partition yield first line for line in textwrap dedent next lines split yield line yield yield table name cls tablename try yield single cls singlename except attributeerror pass yield yield primary key join col key for col in cls table primary key columns yield', ''), ('generate_common', 146, \"def generate_common(cls, remaining_attrs):\\n    common_col_headers = []\\n    for c in cls.__table__.c:\\n        if c.name in common_columns:\\n            common_col_headers.append(column_header(c, show_type=False))\\n            remaining_attrs.remove(c.name)\\n    for translation_class in cls.translation_classes:\\n        for c in translation_class.__table__.c:\\n            if c.name in common_columns:\\n                common_col_headers.append(column_header(c, None,\\n                    translation_class.__table__.name, show_type=False))\\n                remaining_attrs.remove(c.name)\\n    if common_col_headers:\\n        if len(common_col_headers) > 1:\\n            common_col_headers[-1] = 'and ' + common_col_headers[-1]\\n        if len(common_col_headers) > 2:\\n            separator = ', '\\n        else:\\n            separator = ' '\\n        yield 'Has'\\n        yield separator.join(common_col_headers) + '.'\\n        yield ''\\n\", 'generate common cls remaining attrs common col headers for in cls table if name in common columns common col headers append column header show type false remaining attrs remove name for translation class in cls translation classes for in translation class table if name in common columns common col headers append column header none translation class table name show type false remaining attrs remove name if common col headers if len common col headers 1 common col headers 1 and common col headers 1 if len common col headers 2 separator else separator yield has yield separator join common col headers yield', ''), ('generate_columns', 170, \"@with_header('Columns')\\ndef generate_columns(cls, remaining_attrs):\\n    name = cls.__name__\\n    for c in [c for c in cls.__table__.c if c.name not in common_columns]:\\n        remaining_attrs.remove(c.name)\\n        relation_name = c.name[:-3]\\n        if c.name.endswith('_id') and relation_name in remaining_attrs:\\n            relation = getattr(cls, relation_name)\\n            yield column_header(c, name, relation=relation, relation_name=\\n                relation_name)\\n            remaining_attrs.remove(relation_name)\\n        else:\\n            yield column_header(c, name) + ':'\\n        yield ''\\n        if c.doc:\\n            yield '  ' + six.text_type(c.doc)\\n            yield ''\\n\", 'generate columns cls remaining attrs name cls name for in for in cls table if name not in common columns remaining attrs remove name relation name name 3 if name endswith id and relation name in remaining attrs relation getattr cls relation name yield column header name relation relation relation name relation name remaining attrs remove relation name else yield column header name yield if doc yield six text type doc yield', ''), ('generate_strings', 188, \"@with_header('Internationalized strings')\\ndef generate_strings(cls, remaining_attrs):\\n    for translation_class in cls.translation_classes:\\n        for c in translation_class.__table__.c:\\n            if 'format' in c.info:\\n                remaining_attrs.discard(c.name)\\n                remaining_attrs.discard(c.name + '_map')\\n                if c.name in common_columns:\\n                    continue\\n                yield column_header(c, cls.__name__, translation_class.\\n                    __table__.name)\\n                yield ''\\n                if c.doc:\\n                    yield '  ' + six.text_type(c.doc)\\n                    yield ''\\n\", 'generate strings cls remaining attrs for translation class in cls translation classes for in translation class table if format in info remaining attrs discard name remaining attrs discard name map if name in common columns continue yield column header cls name translation class table name yield if doc yield six text type doc yield', ''), ('generate_relationships', 204, \"@with_header('Relationships')\\ndef generate_relationships(cls, remaining_attrs):\\n\\n    def isrelationship(prop):\\n        return isinstance(prop, InstrumentedAttribute) and isinstance(prop.\\n            property, RelationshipProperty)\\n    for attr_name in sorted(remaining_attrs):\\n        prop = getattr(cls, attr_name)\\n        if not isrelationship(prop):\\n            continue\\n        rel = prop.property\\n        yield '%s.\\\\\\\\ **%s**' % (cls.__name__, attr_name)\\n        class_name = (':class:`~pokedex.db.tables.%s`' % rel.mapper.class_.\\n            __name__)\\n        if rel.uselist:\\n            class_name = '[%s]' % class_name\\n        yield '(→ %s)' % class_name\\n        if rel.doc:\\n            yield ''\\n            yield '  ' + six.text_type(rel.doc)\\n        if rel.secondary is not None:\\n            yield ''\\n            yield '  Association table: ``%s``' % rel.secondary\\n        if rel.order_by:\\n            yield ''\\n            yield '  '\\n            yield '  Ordered by: ' + ', '.join('``%s``' % o for o in rel.\\n                order_by)\\n        yield ''\\n        remaining_attrs.remove(attr_name)\\n\", 'generate relationships cls remaining attrs def isrelationship prop return isinstance prop instrumentedattribute and isinstance prop property relationshipproperty for attr name in sorted remaining attrs prop getattr cls attr name if not isrelationship prop continue rel prop property yield cls name attr name class name class pokedex db tables rel mapper class name if rel uselist class name class name yield class name if rel doc yield yield six text type rel doc if rel secondary is not none yield yield association table rel secondary if rel order by yield yield yield ordered by join for in rel order by yield remaining attrs remove attr name', ''), ('generate_associationproxies', 238, \"@with_header('Association Proxies')\\ndef generate_associationproxies(cls, remaining_attrs):\\n    for attr_name in sorted(remaining_attrs):\\n        prop = getattr(cls, attr_name)\\n        if isinstance(prop, AssociationProxy):\\n            yield '%s.\\\\\\\\ **%s**:' % (cls.__name__, attr_name)\\n            yield '``{prop.remote_attr.key}`` of ``self.{prop.local_attr.key}``'.format(\\n                prop=prop)\\n            yield ''\\n            remaining_attrs.remove(attr_name)\\n\", 'generate associationproxies cls remaining attrs for attr name in sorted remaining attrs prop getattr cls attr name if isinstance prop associationproxy yield cls name attr name yield prop remote attr key of self prop local attr key format prop prop yield remaining attrs remove attr name', ''), ('generate_undocumented', 250, \"@with_header('Undocumented')\\ndef generate_undocumented(cls, remaining_attrs):\\n    for c in sorted([c for c in remaining_attrs if isinstance(getattr(cls,\\n        c), (InstrumentedAttribute, AssociationProxy, MoveEffectPropertyMap,\\n        MoveEffectProperty))]):\\n        yield ''\\n        yield '%s.\\\\\\\\ **%s**' % (cls.__name__, c)\\n        remaining_attrs.remove(c)\\n\", 'generate undocumented cls remaining attrs for in sorted for in remaining attrs if isinstance getattr cls instrumentedattribute associationproxy moveeffectpropertymap moveeffectproperty yield yield cls name remaining attrs remove', ''), ('generate_other', 259, \"@with_header(None)\\ndef generate_other(cls, remaining_attrs):\\n    for c in sorted(remaining_attrs):\\n        yield ''\\n        member = getattr(cls, c)\\n        if callable(member):\\n            yield '.. automethod:: %s.%s' % (cls.__name__, c)\\n        else:\\n            yield '.. autoattribute:: %s.%s' % (cls.__name__, c)\\n        yield ''\\n    remaining_attrs.clear()\\n\", 'generate other cls remaining attrs for in sorted remaining attrs yield member getattr cls if callable member yield automethod cls name else yield autoattribute cls name yield remaining attrs clear', ''), ('setup', 329, \"def setup(app):\\n    app.add_directive('dex-table', DexTable)\\n\", 'setup app app add directive dex table dextable', ''), ('get_signature_prefix', 280, \"def get_signature_prefix(self, sig):\\n    return ''\\n\", 'get signature prefix self sig return', ''), ('run', 284, \"def run(self):\\n    section = nodes.section()\\n    super_result = super(DexTable, self).run()\\n    title_text = self.names[0][0]\\n    section += nodes.title(text=title_text)\\n    section += super_result\\n    section['ids'] = ['dex-table-%s' % title_text.lower()]\\n    return [section]\\n\", 'run self section nodes section super result super dextable self run title text self names 0 0 section nodes title text title text section super result section ids dex table title text lower return section', ''), ('before_content', 293, \"def before_content(self):\\n    name = self.names[0][0]\\n    for cls in tables.mapped_classes:\\n        if name == cls.__name__:\\n            break\\n    else:\\n        raise ValueError('Table %s not found' % name)\\n    remaining_attrs = set(x for x in dir(cls) if not x.startswith('_'))\\n    remaining_attrs.difference_update(['metadata', 'translation_classes',\\n        'add_relationships', 'summary_column'])\\n    for transl_class in cls.translation_classes:\\n        remaining_attrs.difference_update([transl_class.relation_name, \\n            transl_class.relation_name + '_table', transl_class.\\n            relation_name + '_local'])\\n    generated_content = []\\n    generated_content.extend(generate_table_header(cls, remaining_attrs))\\n    generated_content.extend(generate_common(cls, remaining_attrs))\\n    generated_content.extend(generate_columns(cls, remaining_attrs))\\n    generated_content.extend(generate_strings(cls, remaining_attrs))\\n    generated_content.extend(generate_relationships(cls, remaining_attrs))\\n    generated_content.extend(generate_associationproxies(cls, remaining_attrs))\\n    generated_content.extend(generate_undocumented(cls, remaining_attrs))\\n    generated_content.extend(generate_other(cls, remaining_attrs))\\n    generated_content.append('')\\n    self.content = ViewList(generated_content + list(self.content))\\n    return super(DexTable, self).before_content()\\n\", 'before content self name self names 0 0 for cls in tables mapped classes if name cls name break else raise valueerror table not found name remaining attrs set for in dir cls if not startswith remaining attrs difference update metadata translation classes add relationships summary column for transl class in cls translation classes remaining attrs difference update transl class relation name transl class relation name table transl class relation name local generated content generated content extend generate table header cls remaining attrs generated content extend generate common cls remaining attrs generated content extend generate columns cls remaining attrs generated content extend generate strings cls remaining attrs generated content extend generate relationships cls remaining attrs generated content extend generate associationproxies cls remaining attrs generated content extend generate undocumented cls remaining attrs generated content extend generate other cls remaining attrs generated content append self content viewlist generated content list self content return super dextable self before content', ''), ('get_index_text', 326, \"def get_index_text(self, modname, name_cls):\\n    return '%s (mapped class)' % name_cls[0]\\n\", 'get index text self modname name cls return mapped class name cls 0', '')] [('parse_options', 180, 'def parse_options():\\n    description = (\\n        \"This script is an example implementation of SAP\\'s Enqueue Server Monitor program (ens_mon). It allows the monitoring of a Enqueue Server service and allows sending different admin commands and opcodes. Includes some commands not available on the ensmon program.\"\\n        )\\n    epilog = \\'pysap %(version)s - %(url)s - %(repo)s\\' % {\\'version\\': pysap.\\n        __version__, \\'url\\': pysap.__url__, \\'repo\\': pysap.__repo__}\\n    usage = \\'Usage: %prog [options] -d <remote host>\\'\\n    parser = OptionParser(usage=usage, description=description, epilog=epilog)\\n    target = OptionGroup(parser, \\'Target\\')\\n    target.add_option(\\'-d\\', \\'--remote-host\\', dest=\\'remote_host\\', help=\\n        \\'Remote host\\')\\n    target.add_option(\\'-p\\', \\'--remote-port\\', dest=\\'remote_port\\', type=\\'int\\',\\n        default=3200, help=\\'Remote port [%default]\\')\\n    target.add_option(\\'--route-string\\', dest=\\'route_string\\', help=\\n        \\'Route string for connecting through a SAP Router\\')\\n    parser.add_option_group(target)\\n    misc = OptionGroup(parser, \\'Misc options\\')\\n    misc.add_option(\\'-c\\', \\'--client\\', dest=\\'client\\', default=\\n        \"pysap\\'s-monitor\", help=\\'Client name [%default]\\')\\n    misc.add_option(\\'-v\\', \\'--verbose\\', dest=\\'verbose\\', action=\\'store_true\\',\\n        default=False, help=\\'Verbose output [%default]\\')\\n    misc.add_option(\\'--log-file\\', dest=\\'logfile\\', metavar=\\'FILE\\', help=\\n        \\'Log file\\')\\n    misc.add_option(\\'--console-log\\', dest=\\'consolelog\\', metavar=\\'FILE\\',\\n        help=\\'Console log file\\')\\n    misc.add_option(\\'--script\\', dest=\\'script\\', metavar=\\'FILE\\', help=\\n        \\'Script file to run\\')\\n    parser.add_option_group(misc)\\n    options, _ = parser.parse_args()\\n    if not (options.remote_host or options.route_string):\\n        parser.error(\\'Remote host or route string is required\\')\\n    return options\\n', 'parse options description this script is an example implementation of sap enqueue server monitor program ens mon it allows the monitoring of enqueue server service and allows sending different admin commands and opcodes includes some commands not available on the ensmon program epilog pysap version url repo version pysap version url pysap url repo pysap repo usage usage prog options remote host parser optionparser usage usage description description epilog epilog target optiongroup parser target target add option remote host dest remote host help remote host target add option remote port dest remote port type int default 3200 help remote port default target add option route string dest route string help route string for connecting through sap router parser add option group target misc optiongroup parser misc options misc add option client dest client default pysap monitor help client name default misc add option verbose dest verbose action store true default false help verbose output default misc add option log file dest logfile metavar file help log file misc add option console log dest consolelog metavar file help console log file misc add option script dest script metavar file help script file to run parser add option group misc options parser parse args if not options remote host or options route string parser error remote host or route string is required return options', ''), ('main', 224, \"def main():\\n    options = parse_options()\\n    if options.verbose:\\n        logging.basicConfig(level=logging.DEBUG)\\n    en_console = SAPEnqueueAdminConsole(options)\\n    try:\\n        if options.script:\\n            en_console.do_script(options.script)\\n        else:\\n            en_console.cmdloop()\\n    except KeyboardInterrupt:\\n        print('Cancelled by the user !')\\n        en_console.do_exit(None)\\n\", 'main options parse options if options verbose logging basicconfig level logging debug en console sapenqueueadminconsole options try if options script en console do script options script else en console cmdloop except keyboardinterrupt print cancelled by the user en console do exit none', ''), ('__init__', 49, \"def __init__(self, options):\\n    super(SAPEnqueueAdminConsole, self).__init__(options)\\n    self.runtimeoptions['client_name'] = self.options.client\\n    self.runtimeoptions['client_recv_length'] = 1000\\n    self.runtimeoptions['client_send_length'] = 1000\\n    self.runtimeoptions['client_version'] = 3\\n\", 'init self options super sapenqueueadminconsole self init options self runtimeoptions client name self options client self runtimeoptions client recv length 1000 self runtimeoptions client send length 1000 self runtimeoptions client version 3', ''), ('preloop', 57, 'def preloop(self):\\n    super(SAPEnqueueAdminConsole, self).preloop()\\n    self.do_connect(None)\\n', 'preloop self super sapenqueueadminconsole self preloop self do connect none', ''), ('do_connect', 63, 'def do_connect(self, args):\\n    \"\"\"Initiate the connection to the Enqueue Server.\"\"\"\\n    try:\\n        self.connection = SAPEnqueueStreamSocket.get_nisocket(self.options.\\n            remote_host, self.options.remote_port, self.options.route_string)\\n    except SocketError as e:\\n        self._error(\\'Error connecting with the Enqueue Server\\')\\n        self._error(str(e))\\n        return\\n    self._print(\\'Attached to %s / %d\\' % (self.options.remote_host, self.\\n        options.remote_port))\\n    params = [SAPEnqueueParam(param=0, value=int(self.runtimeoptions[\\n        \\'client_recv_length\\'])), SAPEnqueueParam(param=1, value=int(self.\\n        runtimeoptions[\\'client_send_length\\'])), SAPEnqueueParam(param=3,\\n        set_name=self.runtimeoptions[\\'client_name\\']), SAPEnqueueParam(param\\n        =2, value=59), SAPEnqueueParam(param=5, value=int(self.\\n        runtimeoptions[\\'client_version\\'])), SAPEnqueueParam(param=6, value=\\n        1, len=4)]\\n    p = SAPEnqueue(dest=6, opcode=1, params=params)\\n    self._debug(\\'Retrieving parameters\\')\\n    response = self.connection.sr(p)[SAPEnqueue]\\n    for param in response.params:\\n        self._debug(\\'Server parameter: %s=%s\\' % (enqueue_param_values[param\\n            .param], param.value if param.param not in [3] else param.set_name)\\n            )\\n        if param.param == 3:\\n            self.runtimeoptions[\\'server_name\\'] = param.set_name\\n        if param.param == 5:\\n            self.runtimeoptions[\\'server_version\\'] = param.value\\n    self._print(\\'Server name: %s\\' % self.runtimeoptions[\\'server_name\\'])\\n    self._print(\\'Server version: %d\\' % self.runtimeoptions[\\'server_version\\'])\\n    self.connected = True\\n', 'do connect self args try self connection sapenqueuestreamsocket get nisocket self options remote host self options remote port self options route string except socketerror as self error error connecting with the enqueue server self error str return self print attached to self options remote host self options remote port params sapenqueueparam param 0 value int self runtimeoptions client recv length sapenqueueparam param 1 value int self runtimeoptions client send length sapenqueueparam param 3 set name self runtimeoptions client name sapenqueueparam param 2 value 59 sapenqueueparam param 5 value int self runtimeoptions client version sapenqueueparam param 6 value 1 len 4 sapenqueue dest 6 opcode 1 params params self debug retrieving parameters response self connection sr sapenqueue for param in response params self debug server parameter enqueue param values param param param value if param param not in 3 else param set name if param param 3 self runtimeoptions server name param set name if param param 5 self runtimeoptions server version param value self print server name self runtimeoptions server name self print server version self runtimeoptions server version self connected true', 'initiate the connection to the enqueue server'), ('do_disconnect', 105, 'def do_disconnect(self, args):\\n    \"\"\"Disconnects from the Enqueue Server service. \"\"\"\\n    if not self.connected:\\n        self._error(\\'You need to connect to the server first !\\')\\n        return\\n    self.connection.close()\\n    self._print(\\'Dettached from %s / %d ...\\' % (self.options.remote_host,\\n        self.options.remote_port))\\n    self.connected = False\\n', 'do disconnect self args if not self connected self error you need to connect to the server first return self connection close self print dettached from self options remote host self options remote port self connected false', 'disconnects from the enqueue server service'), ('do_exit', 116, 'def do_exit(self, args):\\n    if self.connected:\\n        self.do_disconnect(None)\\n    return super(SAPEnqueueAdminConsole, self).do_exit(args)\\n', 'do exit self args if self connected self do disconnect none return super sapenqueueadminconsole self do exit args', ''), ('do_dummy_request', 121, 'def do_dummy_request(self, args):\\n    \"\"\"Send a dummy request to the server to check if it is alive. \"\"\"\\n    if not self.connected:\\n        self._error(\\'You need to connect to the server first !\\')\\n        return\\n    p = SAPEnqueue(dest=3, adm_opcode=1)\\n    self._debug(\\'Sending dummy request\\')\\n    response = self.connection.sr(p)[SAPEnqueue]\\n    response.show()\\n    self._debug(\\'Performed dummy request\\')\\n', 'do dummy request self args if not self connected self error you need to connect to the server first return sapenqueue dest 3 adm opcode 1 self debug sending dummy request response self connection sr sapenqueue response show self debug performed dummy request', 'send a dummy request to the server to check if it is alive'), ('do_get_replication_info', 136, 'def do_get_replication_info(self, args):\\n    \"\"\"Get information about the status and statistics of the replication.\"\"\"\\n    if not self.connected:\\n        self._error(\\'You need to connect to the server first !\\')\\n        return\\n    p = SAPEnqueue(dest=3, adm_opcode=4)\\n    self._debug(\\'Sending get replication info request\\')\\n    response = self.connection.sr(p)[SAPEnqueue]\\n    response.show()\\n    self._debug(\\'Obtained replication info\\')\\n', 'do get replication info self args if not self connected self error you need to connect to the server first return sapenqueue dest 3 adm opcode 4 self debug sending get replication info request response self connection sr sapenqueue response show self debug obtained replication info', 'get information about the status and statistics of the replication'), ('do_check_pattern_loop_dos', 151, 'def do_check_pattern_loop_dos(self, args):\\n    \"\"\"Tests if the server is vulnerable to an endless loop when a trace\\n        pattern with a wildcard is set (CVE-2014-0995).\"\"\"\\n    if not self.connected:\\n        self._error(\\'You need to connect to the server first !\\')\\n        return\\n    patterns = [SAPEnqueueTracePattern(pattern=\\'*\\', len=1)]\\n    p = SAPEnqueue(dest=3, adm_opcode=6, adm_trace_level=3,\\n        adm_trace_action=4, adm_trace_logging=1, adm_trace_nopatterns=1,\\n        adm_trace_nopatterns1=1, adm_trace_patterns=patterns)\\n    self._debug(\\'Sending trace pattern with wildcards\\')\\n    self.connection.send(p)\\n    self._debug(\\'Trace pattern set\\')\\n    p = SAPEnqueue(dest=3, adm_opcode=1)\\n    try:\\n        self.connection.sr(p)[SAPEnqueue]\\n        self._print(\\n            \\'Server available, probably not vulnerable to CVE-2014-0995\\')\\n    except:\\n        self._print(\\'Server unavailable, probably vulnerable to CVE-2014-0995.\\'\\n            )\\n', 'do check pattern loop dos self args if not self connected self error you need to connect to the server first return patterns sapenqueuetracepattern pattern len 1 sapenqueue dest 3 adm opcode 6 adm trace level 3 adm trace action 4 adm trace logging 1 adm trace nopatterns 1 adm trace 1 adm trace patterns patterns self debug sending trace pattern with wildcards self connection send self debug trace pattern set sapenqueue dest 3 adm opcode 1 try self connection sr sapenqueue self print server available probably not vulnerable to cve 2014 0995 except self print server unavailable probably vulnerable to cve 2014 0995', 'tests if the server is vulnerable to an endless loop when a trace pattern with a wildcard is set cve 2014 0995')] [('read_bam_file', 13, \"def read_bam_file(in_bam_dir, out_dir):\\n    system('mkdir %s' % out_dir)\\n    system('rm %s*' % out_dir)\\n    for i_read_file in listdir(in_bam_dir):\\n        info_dict = {'ref_id': '', 'read_name': '', 'read_start': '',\\n            'read_end': '', 'read_len': '', 'ref_start': '', 'ref_end': '',\\n            'ref_len': '', 'cigar': '', 'cigar_str': '', 'orient': '',\\n            'unmapped': '', 'seq': ''}\\n        info_list = []\\n        write_header = True\\n        clip = set([4, 5])\\n        count_write_line = 0\\n        read_bam = pysam.AlignmentFile(in_bam_dir + i_read_file, 'rb')\\n        count_total_read = pysam.AlignmentFile(in_bam_dir + i_read_file, 'rb'\\n            ).count(until_eof=True)\\n        write_error_file = open(out_dir + 'ref_error_' + i_read_file.split(\\n            '.')[0], 'a')\\n        write_info_file = open(out_dir + 'info_' + i_read_file.split('.')[0\\n            ], 'a')\\n        if write_header == True:\\n            write_info_file.writelines('\\\\t'.join(['ref_id', 'read_name',\\n                'read_start', 'read_end', 'read_length', 'ref_start',\\n                'ref_end', 'ref_length', 'cigar', 'clip_start', 'clip_end',\\n                'strand', 'unmapped', 'seq']) + '\\\\n')\\n            write_header = False\\n        for num_read, each_read in enumerate(tqdm(list(read_bam.fetch(\\n            until_eof=True)))):\\n            if each_read.is_reverse == True:\\n                orient_read = 'Reverse'\\n            else:\\n                orient_read = 'Forward'\\n            if each_read.tid >= 0:\\n                ref_name = read_bam.getrname(each_read.tid)\\n            else:\\n                ref_name = each_read.tid\\n                write_error_file = open(out_dir + 'ref_error_' +\\n                    i_read_file.split('.')[0], 'a')\\n                write_error_file.writelines('\\\\t'.join([str(num_read), str(\\n                    each_read.tid), str(ref_name), str(each_read.qname),\\n                    str(each_read.cigarstring)]) + '\\\\n')\\n            info_dict = {'ref_id': ref_name, 'read_name': each_read.qname,\\n                'read_start': each_read.qstart, 'read_end': each_read.qend,\\n                'read_len': each_read.qlen, 'ref_start': each_read.\\n                reference_start, 'ref_end': each_read.aend, 'ref_len':\\n                each_read.alen, 'cigar': each_read.cigar, 'cigar_str':\\n                each_read.cigarstring, 'orient': orient_read, 'unmapped':\\n                each_read.is_unmapped, 'seq': each_read.seq}\\n            info_list.append(info_dict)\\n            if len(info_dict['cigar']) <= 1:\\n                continue\\n            value_end = info_dict['cigar'][-1]\\n            value_start = info_dict['cigar'][0]\\n            clip_end = value_end[0]\\n            if clip_end in clip:\\n                clip_end = '1'\\n            else:\\n                clip_end = '0'\\n            clip_start = value_start[0]\\n            if clip_start in clip:\\n                clip_start = '1'\\n            else:\\n                clip_start = '0'\\n            if value_start[0] in clip or value_end[0] in clip:\\n                write_info_file.writelines('\\\\t'.join([str(info_dict[\\n                    'ref_id']), str(info_dict['read_name']), str(info_dict[\\n                    'read_start']), str(info_dict['read_end']), str(\\n                    info_dict['read_len']), str(info_dict['ref_start']),\\n                    str(info_dict['ref_end']), str(info_dict['ref_len']),\\n                    str(info_dict['cigar_str']), clip_start, clip_end, str(\\n                    info_dict['orient']), str(info_dict['unmapped']), str(\\n                    info_dict['seq'])]) + '\\\\n')\\n                count_write_line += 1\\n        print('======')\\n        print('Number of reads: %d' % num_read)\\n        print('Number or reads from count method:%d' % count_total_read)\\n        print('======\\\\n')\\n        print('Number of written items: %d' % count_write_line)\\n        write_error_file.close()\\n        write_info_file.close()\\n        read_bam.close()\\n\", 'read bam file in bam dir out dir system mkdir out dir system rm out dir for read file in listdir in bam dir info dict ref id read name read start read end read len ref start ref end ref len cigar cigar str orient unmapped seq info list write header true clip set 4 5 count write line 0 read bam pysam alignmentfile in bam dir read file rb count total read pysam alignmentfile in bam dir read file rb count until eof true write error file open out dir ref error read file split 0 write info file open out dir info read file split 0 if write header true write info file writelines join ref id read name read start read end read length ref start ref end ref length cigar clip start clip end strand unmapped seq write header false for num read each read in enumerate tqdm list read bam fetch until eof true if each read is reverse true orient read reverse else orient read forward if each read tid 0 ref name read bam getrname each read tid else ref name each read tid write error file open out dir ref error read file split 0 write error file writelines join str num read str each read tid str ref name str each read qname str each read cigarstring info dict ref id ref name read name each read qname read start each read qstart read end each read qend read len each read qlen ref start each read reference start ref end each read aend ref len each read alen cigar each read cigar cigar str each read cigarstring orient orient read unmapped each read is unmapped seq each read seq info list append info dict if len info dict cigar 1 continue value end info dict cigar 1 value start info dict cigar 0 clip end value end 0 if clip end in clip clip end 1 else clip end 0 clip start value start 0 if clip start in clip clip start 1 else clip start 0 if value start 0 in clip or value end 0 in clip write info file writelines join str info dict ref id str info dict read name str info dict read start str info dict read end str info dict read len str info dict ref start str info dict ref end str info dict ref len str info dict cigar str clip start clip end str info dict orient str info dict unmapped str info dict seq count write line 1 print print number of reads num read print number or reads from count method count total read print print number of written items count write line write error file close write info file close read bam close', '')] [] [('__init__', 42, 'def __init__(self, title=None, description=None, short_description=None):\\n    super(Vulnerability, self).__init__()\\n    self.title = title\\n    self.descriptions = StructuredTextList(description)\\n    self.short_descriptions = StructuredTextList(short_description)\\n    self.references = []\\n', 'init self title none description none short description none super vulnerability self init self title title self descriptions structuredtextlist description self short descriptions structuredtextlist short description self references', ''), ('description', 49, '@property\\ndef description(self):\\n    \"\"\"A single description about the contents or purpose of this object.\\n\\n        Default Value: ``None``\\n\\n        Note:\\n            If this object has more than one description set, this will return\\n            the description with the lowest ordinality value.\\n\\n        Returns:\\n            An instance of :class:`.StructuredText`\\n        \"\"\"\\n    return next(iter(self.descriptions or []), None)\\n', 'description self return next iter self descriptions or none', 'a single description about the contents or purpose of this object'), ('description', 64, '@description.setter\\ndef description(self, value):\\n    self.descriptions = value\\n', 'description self value self descriptions value', ''), ('add_description', 68, 'def add_description(self, description):\\n    \"\"\"Adds a description to the ``descriptions`` collection.\\n\\n        This is the same as calling \"foo.descriptions.add(bar)\".\\n        \"\"\"\\n    self.descriptions.add(description)\\n', 'add description self description self descriptions add description', 'adds a description to the descriptions collection'), ('short_description', 75, '@property\\ndef short_description(self):\\n    \"\"\"A single short description about the contents or purpose of this\\n        object.\\n\\n        Default Value: ``None``\\n\\n        Note:\\n            If this object has more than one short description set, this will\\n            return the description with the lowest ordinality value.\\n\\n        Returns:\\n            An instance of :class:`.StructuredText`\\n        \"\"\"\\n    return next(iter(self.short_descriptions or []), None)\\n', 'short description self return next iter self short descriptions or none', 'a single short description about the contents or purpose of this object'), ('short_description', 91, '@short_description.setter\\ndef short_description(self, value):\\n    self.short_descriptions = value\\n', 'short description self value self short descriptions value', ''), ('add_short_description', 95, 'def add_short_description(self, description):\\n    \"\"\"Adds a description to the ``short_descriptions`` collection.\\n\\n        This is the same as calling \"foo.short_descriptions.add(bar)\".\\n        \"\"\"\\n    self.short_descriptions.add(description)\\n', 'add short description self description self short descriptions add description', 'adds a description to the short descriptions collection'), ('add_reference', 102, 'def add_reference(self, reference):\\n    if not reference:\\n        return\\n    self.references.append(reference)\\n', 'add reference self reference if not reference return self references append reference', ''), ('__init__', 125, 'def __init__(self):\\n    super(CVSSVector, self).__init__()\\n', 'init self super cvssvector self init', '')] [('__init__', 9, \"def __init__(self, app):\\n    self.app = app\\n    self.token_map = {pygments.token.Comment: 'comment', pygments.token.\\n        Comment.Single: 'comment', pygments.token.Operator: 'keyword',\\n        pygments.token.Name.Function: 'entity.name.function', pygments.\\n        token.Name.Class: 'entity.name.class', pygments.token.Name.Tag:\\n        'entity.name.tag', pygments.token.Name.Attribute:\\n        'entity.other.attribute-name', pygments.token.Name.Variable:\\n        'variable', pygments.token.Name.Builtin.Pseudo: 'constant.language',\\n        pygments.token.Literal.String: 'string', pygments.token.Literal.\\n        String.Doc: 'string', pygments.token.Punctuation: 'punctuation',\\n        pygments.token.Literal.Number: 'constant.numeric', pygments.token.\\n        Name: 'entity.name', pygments.token.Keyword: 'keyword', pygments.\\n        token.Generic.Deleted: 'invalid'}\\n\", 'init self app self app app self token map pygments token comment comment pygments token comment single comment pygments token operator keyword pygments token name function entity name function pygments token name class entity name class pygments token name tag entity name tag pygments token name attribute entity other attribute name pygments token name variable variable pygments token name builtin pseudo constant language pygments token literal string string pygments token literal string doc string pygments token punctuation punctuation pygments token literal number constant numeric pygments token name entity name pygments token keyword keyword pygments token generic deleted invalid', ''), ('lex', 30, 'def lex(self, code, lex):\\n    \"\"\"Return tokenified code.\\n\\n        Return a list of tuples (scope, word) where word is the word to be\\n        printed and scope the scope name representing the context.\\n\\n        :param str code: Code to tokenify.\\n        :param lex: Lexer to use.\\n        :return:\\n        \"\"\"\\n    if lex is None:\\n        if not type(code) is str:\\n            code = code.decode(\\'utf-8\\')\\n        return (\\'global\\', code),\\n    words = pygments.lex(code, lex)\\n    scopes = []\\n    for word in words:\\n        token = word[0]\\n        scope = \\'global\\'\\n        if token in self.token_map.keys():\\n            scope = self.token_map[token]\\n        scopes.append((scope, word[1]))\\n    return scopes\\n', 'lex self code lex if lex is none if not type code is str code code decode utf 8 return global code words pygments lex code lex scopes for word in words token word 0 scope global if token in self token map keys scope self token map token scopes append scope word 1 return scopes', 'return tokenified code')] [('__init__', 13, \"def __init__(self, test_sub_dir, test_suffix):\\n    self.test_sub_dir = os.path.normcase(str(test_sub_dir)).split(';')\\n    self.test_suffix = str(test_suffix)\\n    if kIsWindows:\\n        self.test_suffix += '.exe'\\n\", 'init self test sub dir test suffix self test sub dir os path normcase str test sub dir split self test suffix str test suffix if kiswindows self test suffix exe', ''), ('getGTestTests', 21, 'def getGTestTests(self, path, litConfig, localConfig):\\n    \"\"\"getGTestTests(path) - [name]\\n\\n        Return the tests available in gtest executable.\\n\\n        Args:\\n          path: String path to a gtest executable\\n          litConfig: LitConfig instance\\n          localConfig: TestingConfig instance\"\"\"\\n    try:\\n        lines = lit.util.capture([path, \\'--gtest_list_tests\\'], env=\\n            localConfig.environment)\\n        if kIsWindows:\\n            lines = lines.replace(\\'\\\\r\\', \\'\\')\\n        lines = lines.split(\\'\\\\n\\')\\n    except:\\n        litConfig.error(\\'unable to discover google-tests in %r\\' % path)\\n        raise StopIteration\\n    nested_tests = []\\n    for ln in lines:\\n        if not ln.strip():\\n            continue\\n        prefix = \\'\\'\\n        index = 0\\n        while ln[index * 2:index * 2 + 2] == \\'  \\':\\n            index += 1\\n        while len(nested_tests) > index:\\n            nested_tests.pop()\\n        ln = ln[index * 2:]\\n        if ln.endswith(\\'.\\'):\\n            nested_tests.append(ln)\\n        elif any([name.startswith(\\'DISABLED_\\') for name in nested_tests + [ln]]\\n            ):\\n            continue\\n        else:\\n            yield \\'\\'.join(nested_tests) + ln\\n', 'getgtesttests self path litconfig localconfig try lines lit util capture path gtest list tests env localconfig environment if kiswindows lines lines replace lines lines split except litconfig error unable to discover google tests in path raise stopiteration nested tests for ln in lines if not ln strip continue prefix index 0 while ln index 2 index 2 2 index 1 while len nested tests index nested tests pop ln ln index 2 if ln endswith nested tests append ln elif any name startswith disabled for name in nested tests ln continue else yield join nested tests ln', 'getgtesttests path name'), ('getTestsInExecutable', 65, 'def getTestsInExecutable(self, testSuite, path_in_suite, execpath,\\n    litConfig, localConfig):\\n    if not execpath.endswith(self.test_suffix):\\n        return\\n    dirname, basename = os.path.split(execpath)\\n    for testname in self.getGTestTests(execpath, litConfig, localConfig):\\n        testPath = path_in_suite + (basename, testname)\\n        yield lit.Test.Test(testSuite, testPath, localConfig, file_path=\\n            execpath)\\n', 'gettestsinexecutable self testsuite path in suite execpath litconfig localconfig if not execpath endswith self test suffix return dirname basename os path split execpath for testname in self getgtesttests execpath litconfig localconfig testpath path in suite basename testname yield lit test test testsuite testpath localconfig file path execpath', ''), ('getTestsInDirectory', 75, \"def getTestsInDirectory(self, testSuite, path_in_suite, litConfig, localConfig\\n    ):\\n    source_path = testSuite.getSourcePath(path_in_suite)\\n    for filename in os.listdir(source_path):\\n        filepath = os.path.join(source_path, filename)\\n        if os.path.isdir(filepath):\\n            if not os.path.normcase(filename) in self.test_sub_dir:\\n                continue\\n            dirpath_in_suite = path_in_suite + (filename,)\\n            for subfilename in os.listdir(filepath):\\n                execpath = os.path.join(filepath, subfilename)\\n                for test in self.getTestsInExecutable(testSuite,\\n                    dirpath_in_suite, execpath, litConfig, localConfig):\\n                    yield test\\n        elif '.' in self.test_sub_dir:\\n            for test in self.getTestsInExecutable(testSuite, path_in_suite,\\n                filepath, litConfig, localConfig):\\n                yield test\\n\", 'gettestsindirectory self testsuite path in suite litconfig localconfig source path testsuite getsourcepath path in suite for filename in os listdir source path filepath os path join source path filename if os path isdir filepath if not os path normcase filename in self test sub dir continue dirpath in suite path in suite filename for subfilename in os listdir filepath execpath os path join filepath subfilename for test in self gettestsinexecutable testsuite dirpath in suite execpath litconfig localconfig yield test elif in self test sub dir for test in self gettestsinexecutable testsuite path in suite filepath litconfig localconfig yield test', ''), ('execute', 97, \"def execute(self, test, litConfig):\\n    testPath, testName = os.path.split(test.getSourcePath())\\n    while not os.path.exists(testPath):\\n        testPath, namePrefix = os.path.split(testPath)\\n        testName = namePrefix + '/' + testName\\n    cmd = [testPath, '--gtest_filter=' + testName]\\n    if litConfig.useValgrind:\\n        cmd = litConfig.valgrindArgs + cmd\\n    if litConfig.noExecute:\\n        return lit.Test.PASS, ''\\n    try:\\n        out, err, exitCode = lit.util.executeCommand(cmd, env=test.config.\\n            environment, timeout=litConfig.maxIndividualTestTime)\\n    except lit.util.ExecuteCommandTimeoutException:\\n        return lit.Test.TIMEOUT, 'Reached timeout of {} seconds'.format(\\n            litConfig.maxIndividualTestTime)\\n    if exitCode:\\n        return lit.Test.FAIL, out + err\\n    passing_test_line = '[  PASSED  ] 1 test.'\\n    if passing_test_line not in out:\\n        msg = 'Unable to find %r in gtest output:\\\\n\\\\n%s%s' % (passing_test_line\\n            , out, err)\\n        return lit.Test.UNRESOLVED, msg\\n    return lit.Test.PASS, ''\\n\", 'execute self test litconfig testpath testname os path split test getsourcepath while not os path exists testpath testpath nameprefix os path split testpath testname nameprefix testname cmd testpath gtest filter testname if litconfig usevalgrind cmd litconfig valgrindargs cmd if litconfig noexecute return lit test pass try out err exitcode lit util executecommand cmd env test config environment timeout litconfig maxindividualtesttime except lit util executecommandtimeoutexception return lit test timeout reached timeout of seconds format litconfig maxindividualtesttime if exitcode return lit test fail out err passing test line passed 1 test if passing test line not in out msg unable to find in gtest output passing test line out err return lit test unresolved msg return lit test pass', '')] [('__init__', 44, \"def __init__(self):\\n    self.notify_initialized = None\\n    if Notify:\\n        self.notify_initialized = Notify.init('SickRage')\\n\", 'init self self notify initialized none if notify self notify initialized notify init sickrage', ''), ('diagnose', 49, '@staticmethod\\ndef diagnose():\\n    \"\"\"\\n        Check the environment for reasons libnotify isn\\'t working.  Return a\\n        user-readable message indicating possible issues.\\n        \"\"\"\\n    if not Notify:\\n        return (\\n            \\'<p>Error: gir-notify isn\\\\\\'t installed. On Ubuntu/Debian, install the <a href=\"apt:gir1.2-notify-0.7\">gir1.2-notify-0.7</a> or <a href=\"apt:gir1.0-notify-0.4\">gir1.0-notify-0.4</a> package.\\'\\n            )\\n    if (\\'DISPLAY\\' not in os.environ and \\'DBUS_SESSION_BUS_ADDRESS\\' not in\\n        os.environ):\\n        return (\\n            \"<p>Error: Environment variables DISPLAY and DBUS_SESSION_BUS_ADDRESS aren\\'t set.  libnotify will only work when you run SickRage from a desktop login.\"\\n            )\\n    try:\\n        import dbus\\n    except (ImportError, Exception):\\n        dbus = None\\n    if dbus:\\n        try:\\n            bus = dbus.SessionBus()\\n        except dbus.DBusException as e:\\n            return (\\n                \\'<p>Error: unable to connect to D-Bus session bus: <code>{}</code>.<p>Are you running SickRage in a desktop session?\\'\\n                .format(cgi.escape(e)))\\n        try:\\n            bus.get_object(\\'org.freedesktop.Notifications\\',\\n                \\'/org/freedesktop/Notifications\\')\\n        except dbus.DBusException as e:\\n            return (\\n                \"<p>Error: there doesn\\'t seem to be a notification daemon available: <code>{}</code> <p>Try installing notification-daemon or notify-osd.\"\\n                .format(cgi.escape(e)))\\n    return \\'<p>Error: Unable to send notification.\\'\\n', 'diagnose if not notify return error gir notify isn installed on ubuntu debian install the href apt 2 notify 0 7 2 notify 0 7 or href apt 0 notify 0 4 0 notify 0 4 package if display not in os environ and dbus session bus address not in os environ return error environment variables display and dbus session bus address aren set libnotify will only work when you run sickrage from desktop login try import dbus except importerror exception dbus none if dbus try bus dbus sessionbus except dbus dbusexception as return error unable to connect to bus session bus code code are you running sickrage in desktop session format cgi escape try bus get object org freedesktop notifications org freedesktop notifications except dbus dbusexception as return error there doesn seem to be notification daemon available code code try installing notification daemon or notify osd format cgi escape return error unable to send notification', 'check the environment for reasons libnotify isn t working return a user readable message indicating possible issues'), ('notify_snatch', 85, 'def notify_snatch(self, ep_name):\\n    if sickbeard.LIBNOTIFY_NOTIFY_ONSNATCH:\\n        self._notify(common.notifyStrings[common.NOTIFY_SNATCH], ep_name)\\n', 'notify snatch self ep name if sickbeard libnotify notify onsnatch self notify common notifystrings common notify snatch ep name', ''), ('notify_download', 89, 'def notify_download(self, ep_name):\\n    if sickbeard.LIBNOTIFY_NOTIFY_ONDOWNLOAD:\\n        self._notify(common.notifyStrings[common.NOTIFY_DOWNLOAD], ep_name)\\n', 'notify download self ep name if sickbeard libnotify notify ondownload self notify common notifystrings common notify download ep name', ''), ('notify_subtitle_download', 93, \"def notify_subtitle_download(self, ep_name, lang):\\n    if sickbeard.LIBNOTIFY_NOTIFY_ONSUBTITLEDOWNLOAD:\\n        self._notify(common.notifyStrings[common.NOTIFY_SUBTITLE_DOWNLOAD],\\n            ep_name + ': ' + lang)\\n\", 'notify subtitle download self ep name lang if sickbeard libnotify notify onsubtitledownload self notify common notifystrings common notify subtitle download ep name lang', ''), ('notify_git_update', 97, \"def notify_git_update(self, new_version='??'):\\n    if sickbeard.USE_LIBNOTIFY:\\n        update_text = common.notifyStrings[common.NOTIFY_GIT_UPDATE_TEXT]\\n        title = common.notifyStrings[common.NOTIFY_GIT_UPDATE]\\n        self._notify(title, update_text + new_version)\\n\", 'notify git update self new version if sickbeard use libnotify update text common notifystrings common notify git update text title common notifystrings common notify git update self notify title update text new version', ''), ('notify_login', 103, \"def notify_login(self, ipaddress=''):\\n    if sickbeard.USE_LIBNOTIFY:\\n        update_text = common.notifyStrings[common.NOTIFY_LOGIN_TEXT]\\n        title = common.notifyStrings[common.NOTIFY_LOGIN]\\n        self._notify(title, update_text.format(ipaddress))\\n\", 'notify login self ipaddress if sickbeard use libnotify update text common notifystrings common notify login text title common notifystrings common notify login self notify title update text format ipaddress', ''), ('test_notify', 109, \"def test_notify(self):\\n    return self._notify('Test notification',\\n        'This is a test notification from SickRage', force=True)\\n\", 'test notify self return self notify test notification this is test notification from sickrage force true', ''), ('_notify', 112, \"def _notify(self, title, message, force=False):\\n    if self.notify_initialized and sickbeard.USE_LIBNOTIFY | force:\\n        icon = ek(os.path.join, sickbeard.PROG_DIR, 'gui', 'slick',\\n            'images', 'ico', 'favicon-120.png')\\n        try:\\n            n = Notify.Notification.new(title, message, icon)\\n            return n.show()\\n        except Exception:\\n            return False\\n\", 'notify self title message force false if self notify initialized and sickbeard use libnotify force icon ek os path join sickbeard prog dir gui slick images ico favicon 120 png try notify notification new title message icon return show except exception return false', '')] [('__mi_ident__', 6, \"def __mi_ident__(params={}):\\n    if coshsh.util.compare_attr('type', params,\\n        '.*red\\\\\\\\s*hat.*|.*sles.*|.*linux.*|.*limux.*|.*debian.*|.*ubuntu.*|.*centos.*'\\n        ):\\n        return Linux\\n\", 'mi ident params if coshsh util compare attr type params red hat sles linux limux debian ubuntu centos return linux', ''), ('__new__', 19, \"def __new__(cls, params={}):\\n    if compare_attr('version', params, '.*embedded.*'):\\n        cls = EmbeddedLinux\\n    return object.__new__(cls)\\n\", 'new cls params if compare attr version params embedded cls embeddedlinux return object new cls', ''), ('__init__', 24, 'def __init__(self, params):\\n    self.test4_linux = True\\n', 'init self params self linux true', '')] [('main', 9, \"def main(args):\\n    ap = argparse.ArgumentParser()\\n    ap.add_argument('job_id', nargs='?', type=int, help=\\n        'ID of a running background job')\\n    ns = ap.parse_args(args)\\n    _stash = globals()['_stash']\\n    worker_registry = _stash.runtime.worker_registry\\n    if ns.job_id is None:\\n        worker = worker_registry.get_first_bg_worker()\\n    else:\\n        worker = worker_registry.get_worker(ns.job_id)\\n    if worker is None:\\n        print('no background job running' + (' with id {}'.format(ns.job_id\\n            ) if ns.job_id else ''))\\n        return\\n\\n    def f():\\n        _stash.runtime.push_to_foreground(worker)\\n    t = threading.Timer(1.0, f)\\n    print('pushing job {} to foreground ...'.format(worker.job_id))\\n    t.start()\\n\", 'main args ap argparse argumentparser ap add argument job id nargs type int help id of running background job ns ap parse args args stash globals stash worker registry stash runtime worker registry if ns job id is none worker worker registry get first bg worker else worker worker registry get worker ns job id if worker is none print no background job running with id format ns job id if ns job id else return def stash runtime push to foreground worker threading timer 1 0 print pushing job to foreground format worker job id start', '')] [('build', 16, 'def build(self):\\n    cmake = CMake(self.settings)\\n    self.run(\\'cmake \"%s\" %s\\' % (self.conanfile_directory, cmake.command_line))\\n    self.run(\\'cmake --build . %s\\' % cmake.build_config)\\n', 'build self cmake cmake self settings self run cmake self conanfile directory cmake command line self run cmake build cmake build config', ''), ('test', 21, \"def test(self):\\n    if self.settings.os == 'Windows':\\n        self.run('activate && %s %s' % (os.sep.join(['.', 'bin',\\n            'helloworld']), 'conan'))\\n        self.run('activate && %s %s' % (os.sep.join(['.', 'bin',\\n            'helloworld2']), 'conan'))\\n    else:\\n        self.run('%s %s' % (os.sep.join(['.', 'bin', 'helloworld']), 'conan'))\\n        self.run('%s %s' % (os.sep.join(['.', 'bin', 'helloworld2']), 'conan'))\\n\", 'test self if self settings os windows self run activate os sep join bin helloworld conan self run activate os sep join bin conan else self run os sep join bin helloworld conan self run os sep join bin conan', '')] [('testConvnetBenchmarks', 23, \"def testConvnetBenchmarks(self):\\n    all_args = [\\n        '--batch_size 16 --order NCHW --iterations 1 --warmup_iterations 1',\\n        '--batch_size 16 --order NCHW --iterations 1 --warmup_iterations 1 --forward_only'\\n        ]\\n    for model in [cb.AlexNet, cb.OverFeat, cb.VGGA, cb.Inception]:\\n        for arg_str in all_args:\\n            args = cb.GetArgumentParser().parse_args(arg_str.split(' '))\\n            cb.Benchmark(model, args)\\n\", 'testconvnetbenchmarks self all args batch size 16 order nchw iterations 1 warmup iterations 1 batch size 16 order nchw iterations 1 warmup iterations 1 forward only for model in cb alexnet cb overfeat cb vgga cb inception for arg str in all args args cb getargumentparser parse args arg str split cb benchmark model args', '')] [('init', 29, 'def init(self):\\n    cr = self._cr\\n    tools.drop_view_if_exists(cr, \\'buy_order_detail\\')\\n    cr.execute(\\n        \"\"\"\\n            CREATE or REPLACE VIEW buy_order_detail AS (\\n                SELECT  MIN(wml.id) AS id,\\n                    wm.date AS date,\\n                    wm.name AS order_name,\\n                    (CASE WHEN wm.origin = \\'buy.receipt.buy\\' THEN \\'购货\\'\\n                        ELSE \\'退货\\' END) AS type,\\n                    wm.partner_id AS partner_id,\\n                    goods.code AS goods_code,\\n                    goods.id AS goods_id,\\n                    attr.name AS attribute,\\n                    wh.id AS warehouse_dest_id,\\n                    SUM(CASE WHEN wm.origin = \\'buy.receipt.buy\\' THEN wml.goods_qty\\n                        ELSE - wml.goods_qty END) AS qty,\\n                    uom.name AS uom,\\n                    wml.price AS price,\\n                    SUM(CASE WHEN wm.origin = \\'buy.receipt.buy\\' THEN wml.amount\\n                        ELSE - wml.amount END) AS amount,\\n                    SUM(CASE WHEN wm.origin = \\'buy.receipt.buy\\' THEN wml.tax_amount\\n                        ELSE - wml.tax_amount END) AS tax_amount,\\n                    SUM(CASE WHEN wm.origin = \\'buy.receipt.buy\\' THEN wml.subtotal\\n                        ELSE - wml.subtotal END) AS subtotal,\\n                    wml.note AS note\\n\\n                FROM wh_move_line AS wml\\n                    LEFT JOIN wh_move wm ON wml.move_id = wm.id\\n                    LEFT JOIN partner ON wm.partner_id = partner.id\\n                    LEFT JOIN goods ON wml.goods_id = goods.id\\n                    LEFT JOIN attribute AS attr ON wml.attribute_id = attr.id\\n                    LEFT JOIN warehouse AS wh ON wml.warehouse_id = wh.id\\n                         OR wml.warehouse_dest_id = wh.id\\n                    LEFT JOIN uom ON goods.uom_id = uom.id\\n                    LEFT JOIN buy_receipt AS br ON wm.id = br.buy_move_id\\n\\n                WHERE wml.state = \\'done\\'\\n                  AND wm.origin like \\'buy.receipt%%\\'\\n                  AND wh.type = \\'stock\\'\\n\\n                GROUP BY wm.date, wm.name, origin, partner_id,\\n                    goods_code, goods.id, attribute, wh.id, uom,\\n                    wml.price, wml.note\\n                )\\n        \"\"\"\\n        )\\n', 'init self cr self cr tools drop view if exists cr buy order detail cr execute create or replace view buy order detail as select min wml id as id wm date as date wm name as order name case when wm origin buy receipt buy then else end as type wm partner id as partner id goods code as goods code goods id as goods id attr name as attribute wh id as warehouse dest id sum case when wm origin buy receipt buy then wml goods qty else wml goods qty end as qty uom name as uom wml price as price sum case when wm origin buy receipt buy then wml amount else wml amount end as amount sum case when wm origin buy receipt buy then wml tax amount else wml tax amount end as tax amount sum case when wm origin buy receipt buy then wml subtotal else wml subtotal end as subtotal wml note as note from wh move line as wml left join wh move wm on wml move id wm id left join partner on wm partner id partner id left join goods on wml goods id goods id left join attribute as attr on wml attribute id attr id left join warehouse as wh on wml warehouse id wh id or wml warehouse dest id wh id left join uom on goods uom id uom id left join buy receipt as br on wm id br buy move id where wml state done and wm origin like buy receipt and wh type stock group by wm date wm name origin partner id goods code goods id attribute wh id uom wml price wml note', ''), ('view_detail', 76, '@api.multi\\ndef view_detail(self):\\n    \"\"\"查看明细按钮\"\"\"\\n    self.ensure_one()\\n    order = self.env[\\'buy.receipt\\'].search([(\\'name\\', \\'=\\', self.order_name)])\\n    if order:\\n        if not order.is_return:\\n            view = self.env.ref(\\'buy.buy_receipt_form\\')\\n        else:\\n            view = self.env.ref(\\'buy.buy_return_form\\')\\n        return {\\'name\\': \\'采购入库单\\', \\'view_type\\': \\'form\\', \\'view_mode\\': \\'form\\',\\n            \\'view_id\\': False, \\'views\\': [(view.id, \\'form\\')], \\'res_model\\':\\n            \\'buy.receipt\\', \\'type\\': \\'ir.actions.act_window\\', \\'res_id\\': order.id}\\n', 'view detail self self ensure one order self env buy receipt search name self order name if order if not order is return view self env ref buy buy receipt form else view self env ref buy buy return form return name view type form view mode form view id false views view id form res model buy receipt type ir actions act window res id order id', '')] [('_makeRandomString', 10, \"def _makeRandomString(self):\\n    str_len = random.randint(3, 10)\\n    s = ''\\n    for _ in range(str_len):\\n        s += random.choice(string.ascii_letters)\\n    return s\\n\", 'makerandomstring self str len random randint 3 10 for in range str len random choice string ascii letters return', ''), ('_makeRandomTFile', 17, 'def _makeRandomTFile(self):\\n    from GangaCore.GPI import File\\n    name = self._makeRandomString()\\n    subdir = self._makeRandomString()\\n    return File(name=name, subdir=subdir)\\n', 'makerandomtfile self from gangacore gpi import file name self makerandomstring subdir self makerandomstring return file name name subdir subdir', ''), ('setUp', 23, 'def setUp(self):\\n    super(TestNestedLists, self).setUp()\\n    self.filelist = []\\n    self.gangalist = None\\n    for _ in range(10):\\n        self.filelist.append([self._makeRandomTFile() for _ in range(3)])\\n    self.gangalist = addProxy(makeGangaList([]))\\n', 'setup self super testnestedlists self setup self filelist self gangalist none for in range 10 self filelist append self makerandomtfile for in range 3 self gangalist addproxy makegangalist', ''), ('test_a_Add', 34, 'def test_a_Add(self):\\n    new_list = self.gangalist + self.filelist\\n    assert len(new_list), len(self.gangalist) + len(self.filelist)\\n', 'test add self new list self gangalist self filelist assert len new list len self gangalist len self filelist', ''), ('test_b_SetItem', 39, 'def test_b_SetItem(self):\\n    for _ in range(10):\\n        self.gangalist.append(self._makeRandomTFile())\\n    self.gangalist[0] = self.filelist\\n    assert self.gangalist[0] == self.filelist\\n', 'test setitem self for in range 10 self gangalist append self makerandomtfile self gangalist 0 self filelist assert self gangalist 0 self filelist', ''), ('test_c_SetSlice', 47, 'def test_c_SetSlice(self):\\n    for _ in range(10):\\n        self.gangalist.append(self._makeRandomTFile())\\n    self.gangalist[0:4] = self.filelist[0:4]\\n    assert self.gangalist[0:4] == self.filelist[0:4]\\n', 'test setslice self for in range 10 self gangalist append self makerandomtfile self gangalist 0 4 self filelist 0 4 assert self gangalist 0 4 self filelist 0 4', ''), ('test_d_Append', 55, 'def test_d_Append(self):\\n    gl_len = len(self.gangalist)\\n    self.gangalist.append(self.filelist)\\n    assert len(self.gangalist), gl_len + len(self.filelist)\\n', 'test append self gl len len self gangalist self gangalist append self filelist assert len self gangalist gl len len self filelist', ''), ('test_e_Extend', 61, 'def test_e_Extend(self):\\n    gl_len = len(self.gangalist)\\n    self.gangalist.extend(self.filelist)\\n    assert len(self.gangalist), gl_len + len(self.filelist)\\n', 'test extend self gl len len self gangalist self gangalist extend self filelist assert len self gangalist gl len len self filelist', ''), ('test_f_Insert', 68, 'def test_f_Insert(self):\\n    gl_len = len(self.gangalist)\\n    for _ in range(10):\\n        self.gangalist.append(self._makeRandomTFile())\\n    self.gangalist.insert(5, self.filelist)\\n    assert len(self.gangalist), gl_len + 10 + len(self.filelist)\\n', 'test insert self gl len len self gangalist for in range 10 self gangalist append self makerandomtfile self gangalist insert 5 self filelist assert len self gangalist gl len 10 len self filelist', '')] [('__init__', 88, 'def __init__(self, rconf=None, logger=None):\\n    super(CustomTTSWrapper, self).__init__(rconf=rconf, logger=logger)\\n', 'init self rconf none logger none super customttswrapper self init rconf rconf logger logger', ''), ('_synthesize_single_python_helper', 91, 'def _synthesize_single_python_helper(self, text, voice_code,\\n    output_file_path=None, return_audio_data=True):\\n    \"\"\"\\n        This is an helper function to synthesize a single text fragment via a Python call.\\n\\n        If ``output_file_path`` is ``None``,\\n        the audio data will not persist to file at the end of the method.\\n\\n        :rtype: tuple (result, (duration, sample_rate, encoding, data))\\n        \"\"\"\\n    if len(text) == 0:\\n        self.log(\\'len(text) is zero: returning 0.000\\')\\n        return True, (TimeValue(\\'0.000\\'), None, None, None)\\n    voice_json_path = gf.safe_str(gf.absolute_path(\\'voice.json\\', __file__))\\n    voice = speect.SVoice(voice_json_path)\\n    utt = voice.synth(text)\\n    audio = utt.features[\\'audio\\']\\n    if output_file_path is None:\\n        self.log(\\'output_file_path is None => not saving to file\\')\\n    else:\\n        self.log(\\'output_file_path is not None => saving to file...\\')\\n        audio.save_riff(gf.safe_str(output_file_path))\\n        self.log(\\'output_file_path is not None => saving to file... done\\')\\n    if not return_audio_data:\\n        self.log(\\'return_audio_data is True => return immediately\\')\\n        return True, None\\n    self.log(\\'return_audio_data is True => read and return audio data\\')\\n    waveform = audio.get_audio_waveform()\\n    audio_sample_rate = int(waveform[\\'samplerate\\'])\\n    audio_length = TimeValue(audio.num_samples() / audio_sample_rate)\\n    audio_format = \\'pcm16\\'\\n    audio_samples = numpy.fromstring(waveform[\\'samples\\'], dtype=numpy.int16\\n        ).astype(\\'float64\\') / 32768\\n    return True, (audio_length, audio_sample_rate, audio_format, audio_samples)\\n', 'synthesize single python helper self text voice code output file path none return audio data true if len text 0 self log len text is zero returning 0 000 return true timevalue 0 000 none none none voice json path gf safe str gf absolute path voice json file voice speect svoice voice json path utt voice synth text audio utt features audio if output file path is none self log output file path is none not saving to file else self log output file path is not none saving to file audio save riff gf safe str output file path self log output file path is not none saving to file done if not return audio data self log return audio data is true return immediately return true none self log return audio data is true read and return audio data waveform audio get audio waveform audio sample rate int waveform samplerate audio length timevalue audio num samples audio sample rate audio format audio samples numpy fromstring waveform samples dtype numpy astype 32768 return true audio length audio sample rate audio format audio samples', 'this is an helper function to synthesize a single text fragment via a python call')] [('__init__', 6, 'def __init__(self, session, page, cursor_start, cursor_end, color=None,\\n    stroke=None, size=1):\\n    super(PDFLine, self).__init__(session, page, color, None, stroke, size=size\\n        )\\n    self.start = cursor_start\\n    self.end = cursor_end\\n', 'init self session page cursor start cursor end color none stroke none size 1 super pdfline self init session page color none stroke size size self start cursor start self end cursor end', ''), ('_draw', 11, \"def _draw(self):\\n    self._draw_color()\\n    if self._stroke is not None:\\n        self._draw_stroke()\\n    self._draw_line_size()\\n    s = '%.2f %.2f m %.2f %.2f l %s' % (self.start.x, self.start.y_prime,\\n        self.end.x, self.end.y_prime, self._style)\\n    self.session._out(s, self.page)\\n\", 'draw self self draw color if self stroke is not none self draw stroke self draw line size self start self start prime self end self end prime self style self session out self page', '')] [('test_get', 13, \"def test_get(self):\\n    user = self.create_user(email='a@example.com')\\n    self.login_as(user=user)\\n    url = reverse('sentry-api-0-user-avatar', kwargs={'user_id': 'me'})\\n    response = self.client.get(url, format='json')\\n    assert response.status_code == 200, response.content\\n    assert response.data['id'] == six.text_type(user.id)\\n    assert response.data['avatar']['avatarType'] == 'letter_avatar'\\n    assert response.data['avatar']['avatarUuid'] is None\\n\", 'test get self user self create user email example com self login as user user url reverse sentry api 0 user avatar kwargs user id me response self client get url format json assert response status code 200 response content assert response data id six text type user id assert response data avatar avatartype letter avatar assert response data avatar avataruuid is none', ''), ('test_gravatar', 30, \"def test_gravatar(self):\\n    user = self.create_user(email='a@example.com')\\n    self.login_as(user=user)\\n    url = reverse('sentry-api-0-user-avatar', kwargs={'user_id': 'me'})\\n    response = self.client.put(url, data={'avatar_type': 'gravatar'},\\n        format='json')\\n    avatar = UserAvatar.objects.get(user=user)\\n    assert response.status_code == 200, response.content\\n    assert avatar.get_avatar_type_display() == 'gravatar'\\n\", 'test gravatar self user self create user email example com self login as user user url reverse sentry api 0 user avatar kwargs user id me response self client put url data avatar type gravatar format json avatar useravatar objects get user user assert response status code 200 response content assert avatar get avatar type display gravatar', ''), ('test_upload', 46, \"def test_upload(self):\\n    user = self.create_user(email='a@example.com')\\n    self.login_as(user=user)\\n    url = reverse('sentry-api-0-user-avatar', kwargs={'user_id': 'me'})\\n    response = self.client.put(url, data={'avatar_type': 'upload',\\n        'avatar_photo': b64encode(self.load_fixture('avatar.jpg'))}, format\\n        ='json')\\n    avatar = UserAvatar.objects.get(user=user)\\n    assert response.status_code == 200, response.content\\n    assert avatar.get_avatar_type_display() == 'upload'\\n    assert avatar.file\\n\", 'test upload self user self create user email example com self login as user user url reverse sentry api 0 user avatar kwargs user id me response self client put url data avatar type upload avatar photo self load fixture avatar jpg format json avatar useravatar objects get user user assert response status code 200 response content assert avatar get avatar type display upload assert avatar file', ''), ('test_put_bad', 70, \"def test_put_bad(self):\\n    user = self.create_user(email='a@example.com')\\n    UserAvatar.objects.create(user=user)\\n    self.login_as(user=user)\\n    url = reverse('sentry-api-0-user-avatar', kwargs={'user_id': 'me'})\\n    response = self.client.put(url, data={'avatar_type': 'upload'}, format=\\n        'json')\\n    avatar = UserAvatar.objects.get(user=user)\\n    assert response.status_code == 400\\n    assert avatar.get_avatar_type_display() == 'letter_avatar'\\n    response = self.client.put(url, data={'avatar_type': 'foo'}, format='json')\\n    assert response.status_code == 400\\n    assert avatar.get_avatar_type_display() == 'letter_avatar'\\n\", 'test put bad self user self create user email example com useravatar objects create user user self login as user user url reverse sentry api 0 user avatar kwargs user id me response self client put url data avatar type upload format json avatar useravatar objects get user user assert response status code 400 assert avatar get avatar type display letter avatar response self client put url data avatar type foo format json assert response status code 400 assert avatar get avatar type display letter avatar', ''), ('test_put_forbidden', 91, \"def test_put_forbidden(self):\\n    user = self.create_user(email='a@example.com')\\n    user2 = self.create_user(email='b@example.com')\\n    self.login_as(user=user)\\n    url = reverse('sentry-api-0-user-avatar', kwargs={'user_id': user2.id})\\n    response = self.client.put(url, data={'avatar_type': 'gravatar'},\\n        format='json')\\n    assert response.status_code == 403\\n\", 'test put forbidden self user self create user email example com self create user email example com self login as user user url reverse sentry api 0 user avatar kwargs user id id response self client put url data avatar type gravatar format json assert response status code 403', '')] [('__init__', 6, 'def __init__(self, memory_size):\\n    self.memory = []\\n    self.memory_size = memory_size\\n    self.stored_transitions = 0\\n    self.oldest_transition = 0\\n', 'init self memory size self memory self memory size memory size self stored transitions 0 self oldest transition 0', ''), ('store_transition', 13, 'def store_transition(self, transition):\\n    if self.stored_transitions < self.memory_size:\\n        self.memory.append(transition)\\n        self.stored_transitions += 1\\n    else:\\n        self.memory[self.oldest_transition] = transition\\n        self.oldest_transition = (self.oldest_transition + 1\\n            ) % self.memory_size\\n', 'store transition self transition if self stored transitions self memory size self memory append transition self stored transitions 1 else self memory self oldest transition transition self oldest transition self oldest transition 1 self memory size', ''), ('sample_transition', 21, 'def sample_transition(self):\\n    return self.memory[np.random.randint(0, self.stored_transitions)]\\n', 'sample transition self return self memory np random randint 0 self stored transitions', ''), ('dump_memory', 27, 'def dump_memory(self):\\n    for t in self.memory:\\n        print(t[0].shape, t[1], t[2], t[3].shape)\\n', 'dump memory self for in self memory print 0 shape 1 2 3 shape', ''), ('split_transition', 32, 'def split_transition(self, t):\\n    sa = np.split(t, 4, 2)\\n    print(sa[0].shape)\\n    plt.imshow(sa[0].reshape(84, 84), cmap=matplotlib.cm.Greys_r)\\n    plt.show()\\n', 'split transition self sa np split 4 2 print sa 0 shape plt imshow sa 0 reshape 84 84 cmap matplotlib cm greys plt show', ''), ('show_memory', 39, 'def show_memory(self):\\n    for t in self.memory:\\n        self.split_transition(t[0])\\n', 'show memory self for in self memory self split transition 0', '')] [('__init__', 47, \"def __init__(self, filename, filename_info, filetype_info):\\n    super(FCIFDHSIFileHandler, self).__init__(filename, filename_info,\\n        filetype_info)\\n    logger.debug('Reading: {}'.format(filename))\\n    logger.debug('Start: {}'.format(self.start_time))\\n    logger.debug('End: {}'.format(self.end_time))\\n    self.nc = h5py.File(filename, 'r')\\n    self.filename = filename\\n    self.cache = {}\\n\", 'init self filename filename info filetype info super fcifdhsifilehandler self init filename filename info filetype info logger debug reading format filename logger debug start format self start time logger debug end format self end time self nc file filename self filename filename self cache', ''), ('start_time', 58, \"@property\\ndef start_time(self):\\n    return self.filename_info['start_time']\\n\", 'start time self return self filename info start time', ''), ('end_time', 62, \"@property\\ndef end_time(self):\\n    return self.filename_info['end_time']\\n\", 'end time self return self filename info end time', ''), ('get_dataset', 66, 'def get_dataset(self, key, info=None):\\n    \"\"\"Load a dataset\\n        \"\"\"\\n    if key in self.cache:\\n        return self.cache[key]\\n    logger.debug(\\'Reading {}\\'.format(key.name))\\n    variable = self.nc[\\'/data/{}/measured/effective_radiance\\'.format(key.name)]\\n    radiances = xr.DataArray(np.asarray(variable, np.float32), dims=[\\'y\\', \\'x\\'])\\n    radiances.attrs[\\'scale_factor\\'] = variable.attrs[\\'scale_factor\\']\\n    radiances.attrs[\\'offset\\'] = variable.attrs.get(\\'add_offset\\', 0)\\n    radiances.attrs[\\'FillValue\\'] = variable.attrs[\\'_FillValue\\']\\n    radiances.values[radiances == radiances.attrs[\\'FillValue\\']] = np.nan\\n    radiances = radiances * (radiances.attrs[\\'scale_factor\\'] * 1.0\\n        ) + radiances.attrs[\\'offset\\']\\n    res = self.calibrate(radiances, key)\\n    self.cache[key] = res\\n    self.nlines, self.ncols = res.shape\\n    return res\\n', 'get dataset self key info none if key in self cache return self cache key logger debug reading format key name variable self nc data measured effective radiance format key name radiances xr dataarray np asarray variable np dims radiances attrs scale factor variable attrs scale factor radiances attrs offset variable attrs get add offset 0 radiances attrs fillvalue variable attrs fillvalue radiances values radiances radiances attrs fillvalue np nan radiances radiances radiances attrs scale factor 1 0 radiances attrs offset res self calibrate radiances key self cache key res self nlines self ncols res shape return res', 'load a dataset'), ('calc_area_extent', 95, 'def calc_area_extent(self, key):\\n    \"\"\"Calculate area extent for a dataset.\\n        \"\"\"\\n    xyres = {(500): 22272, (1000): 11136, (2000): 5568}\\n    chkres = xyres[key.resolution]\\n    measured = self.nc[\\'/data/{}/measured\\'.format(key.name)]\\n    variable = self.nc[\\'/data/{}/measured/effective_radiance\\'.format(key.name)]\\n    self.startline = int(measured[\\'start_position_row\\'][...])\\n    self.endline = int(measured[\\'end_position_row\\'][...])\\n    self.startcol = int(measured[\\'start_position_column\\'][...])\\n    self.endcol = int(measured[\\'end_position_column\\'][...])\\n    self.nlines, self.ncols = variable[:].shape\\n    logger.debug(\\'Channel {} resolution: {}\\'.format(key.name, chkres))\\n    logger.debug(\\'Row/Cols: {} / {}\\'.format(self.nlines, self.ncols))\\n    logger.debug(\\'Start/End row: {} / {}\\'.format(self.startline, self.endline))\\n    logger.debug(\\'Start/End col: {} / {}\\'.format(self.startcol, self.endcol))\\n    total_segments = 70\\n    max_y = 5432229.931711678\\n    min_y = -5429229.528545862\\n    full_y = max_y + abs(min_y)\\n    res_y = full_y / chkres\\n    startl = min_y + res_y * self.startline - 0.5 * res_y\\n    endl = min_y + res_y * self.endline + 0.5 * res_y\\n    logger.debug(\\'Start / end extent: {} / {}\\'.format(startl, endl))\\n    chk_extent = -5432229.931711678, endl, 5429229.528545862, startl\\n    return chk_extent\\n', 'calc area extent self key xyres 500 22272 1000 11136 2000 5568 chkres xyres key resolution measured self nc data measured format key name variable self nc data measured effective radiance format key name self startline int measured start position row self endline int measured end position row self startcol int measured start position column self endcol int measured end position column self nlines self ncols variable shape logger debug channel resolution format key name chkres logger debug row cols format self nlines self ncols logger debug start end row format self startline self endline logger debug start end col format self startcol self endcol total segments 70 max 5432229 931711678 min 5429229 528545862 full max abs min res full chkres startl min res self startline 0 5 res endl min res self endline 0 5 res logger debug start end extent format startl endl chk extent 5432229 931711678 endl 5429229 528545862 startl return chk extent', 'calculate area extent for a dataset'), ('get_area_def', 136, 'def get_area_def(self, key, info=None):\\n    \"\"\"Calculate on-fly area definition for 0 degree geos-projection\\n        for a dataset\\n        \"\"\"\\n    a = 6378169.0\\n    h = 35785831.0\\n    b = 6356583.8\\n    lon_0 = 0.0\\n    area_extent = self.calc_area_extent(key)\\n    logger.debug(\\'Calculated area extent: {}\\'.format(\\'\\'.join(str(area_extent)))\\n        )\\n    proj_dict = {\\'a\\': float(a), \\'b\\': float(b), \\'lon_0\\': float(lon_0), \\'h\\':\\n        float(h), \\'proj\\': \\'geos\\', \\'units\\': \\'m\\'}\\n    area = geometry.AreaDefinition(\\'some_area_name\\', \\'On-the-fly area\\',\\n        \\'geosfci\\', proj_dict, self.ncols, self.nlines, area_extent)\\n    self.area = area\\n    return area\\n', 'get area def self key info none 6378169 0 35785831 0 6356583 8 lon 0 0 0 area extent self calc area extent key logger debug calculated area extent format join str area extent proj dict float float lon 0 float lon 0 float proj geos units area geometry areadefinition some area name on the fly area geosfci proj dict self ncols self nlines area extent self area area return area', 'calculate on fly area definition for 0 degree geos projection for a dataset'), ('calibrate', 195, 'def calibrate(self, data, key):\\n    \"\"\"Data calibration.\\n        \"\"\"\\n    logger.warning(\\'Calibration disabled!\\')\\n    if key.calibration == \\'brightness_temperature\\':\\n        pass\\n    elif key.calibration == \\'reflectance\\':\\n        pass\\n    else:\\n        pass\\n    return data\\n', 'calibrate self data key logger warning calibration disabled if key calibration brightness temperature pass elif key calibration reflectance pass else pass return data', 'data calibration'), ('_ir_calibrate', 212, 'def _ir_calibrate(self, data, key):\\n    \"\"\"IR channel calibration\\n        \"\"\"\\n    Lv = data.data * self.nc[\\n        \\'/data/{}/measured/radiance_unit_conversion_coefficient\\'.format(key\\n        .name)][...]\\n    vc = self.nc[\\'/data/{}/central_wavelength_actual\\'.format(key.name)][...]\\n    a, b, dummy = self.nc[\\n        \\'/data/{}/measured/radiance_to_bt_conversion_coefficients\\'.format(\\n        key.name)][...]\\n    c1, c2 = self.nc[\\n        \\'/data/{}/measured/radiance_to_bt_conversion_constants\\'.format(key.\\n        name)][...]\\n    nom = c2 * vc\\n    denom = a * np.log(1 + c1 * vc ** 3 / Lv)\\n    data.data[:] = nom / denom - b / a\\n', 'ir calibrate self data key lv data data self nc data measured radiance unit conversion coefficient format key name vc self nc data central wavelength actual format key name dummy self nc data measured radiance to bt conversion coefficients format key name self nc data measured radiance to bt conversion constants format key name nom vc denom np log 1 vc 3 lv data data nom denom', 'ir channel calibration'), ('_vis_calibrate', 236, 'def _vis_calibrate(self, data, key):\\n    \"\"\"VIS channel calibration\\n        \"\"\"\\n    sirr = self.nc[\\'/data/{}/measured/channel_effective_solar_irradiance\\'.\\n        format(key.name)][...]\\n    data.data[:] /= sirr\\n    data.data[:] *= 100\\n', 'vis calibrate self data key sirr self nc data measured channel effective solar irradiance format key name data data sirr data data 100', 'vis channel calibration')] [('setUp', 28, \"def setUp(self):\\n    super(RodcCmdTestCase, self).setUp()\\n    self.lp = samba.param.LoadParm()\\n    self.lp.load(os.environ['SMB_CONF_PATH'])\\n    self.creds = Credentials()\\n    self.creds.set_username(os.environ['DC_USERNAME'])\\n    self.creds.set_password(os.environ['DC_PASSWORD'])\\n    self.creds.guess(self.lp)\\n    self.session = system_session()\\n    self.ldb = SamDB('ldap://' + os.environ['DC_SERVER'], session_info=self\\n        .session, credentials=self.creds, lp=self.lp)\\n    self.base_dn = self.ldb.domain_dn()\\n    self.ldb.newuser('sambatool1', '1qazXSW@')\\n    self.ldb.newuser('sambatool2', '2wsxCDE#')\\n    self.ldb.newuser('sambatool3', '3edcVFR$')\\n    self.ldb.newuser('sambatool4', '4rfvBGT%')\\n    self.ldb.newuser('sambatool5', '5tjbNHY*')\\n    self.ldb.newuser('sambatool6', '6yknMJU*')\\n    self.ldb.add_remove_group_members('Allowed RODC Password Replication Group'\\n        , ['sambatool1', 'sambatool2', 'sambatool3', 'sambatool4',\\n        'sambatool5'], add_members_operation=True)\\n\", 'setup self super rodccmdtestcase self setup self lp samba param loadparm self lp load os environ smb conf path self creds credentials self creds set username os environ dc username self creds set password os environ dc password self creds guess self lp self session system session self ldb samdb ldap os environ dc server session info self session credentials self creds lp self lp self base dn self ldb domain dn self ldb newuser self ldb newuser self ldb newuser self ldb newuser self ldb newuser self ldb newuser self ldb add remove group members allowed rodc password replication group add members operation true', ''), ('tearDown', 54, \"def tearDown(self):\\n    super(RodcCmdTestCase, self).tearDown()\\n    self.ldb.deleteuser('sambatool1')\\n    self.ldb.deleteuser('sambatool2')\\n    self.ldb.deleteuser('sambatool3')\\n    self.ldb.deleteuser('sambatool4')\\n    self.ldb.deleteuser('sambatool5')\\n    self.ldb.deleteuser('sambatool6')\\n    result, out, err = self.runsubcmd('drs', 'replicate', '--local',\\n        'unused', os.environ['DC_SERVER'], self.base_dn)\\n\", 'teardown self super rodccmdtestcase self teardown self ldb deleteuser self ldb deleteuser self ldb deleteuser self ldb deleteuser self ldb deleteuser self ldb deleteuser result out err self runsubcmd drs replicate local unused os environ dc server self base dn', ''), ('test_single_by_account_name', 66, \"def test_single_by_account_name(self):\\n    result, out, err = self.runsubcmd('rodc', 'preload', 'sambatool1',\\n        '--server', os.environ['DC_SERVER'])\\n    self.assertCmdSuccess(result, out, err,\\n        'ensuring rodc prefetch ran successfully')\\n    self.assertEqual(out, 'Replicating DN CN=sambatool1,CN=Users,%s\\\\n' %\\n        self.base_dn)\\n    self.assertEqual(err, '')\\n\", 'test single by account name self result out err self runsubcmd rodc preload server os environ dc server self assertcmdsuccess result out err ensuring rodc prefetch ran successfully self assertequal out replicating dn cn cn users self base dn self assertequal err', ''), ('test_single_by_dn', 73, \"def test_single_by_dn(self):\\n    result, out, err = self.runsubcmd('rodc', 'preload', \\n        'cn=sambatool2,cn=users,%s' % self.base_dn, '--server', os.environ[\\n        'DC_SERVER'])\\n    self.assertCmdSuccess(result, out, err,\\n        'ensuring rodc prefetch ran successfully')\\n    self.assertEqual(out, 'Replicating DN CN=sambatool2,CN=Users,%s\\\\n' %\\n        self.base_dn)\\n\", 'test single by dn self result out err self runsubcmd rodc preload cn cn users self base dn server os environ dc server self assertcmdsuccess result out err ensuring rodc prefetch ran successfully self assertequal out replicating dn cn cn users self base dn', ''), ('test_multi_by_account_name', 79, 'def test_multi_by_account_name(self):\\n    result, out, err = self.runsubcmd(\\'rodc\\', \\'preload\\', \\'sambatool1\\',\\n        \\'sambatool2\\', \\'--server\\', os.environ[\\'DC_SERVER\\'])\\n    self.assertCmdSuccess(result, out, err,\\n        \\'ensuring rodc prefetch ran successfully\\')\\n    self.assertEqual(out, \\n        \"\"\"Replicating DN CN=sambatool1,CN=Users,%s\\nReplicating DN CN=sambatool2,CN=Users,%s\\n\"\"\"\\n         % (self.base_dn, self.base_dn))\\n', 'test multi by account name self result out err self runsubcmd rodc preload server os environ dc server self assertcmdsuccess result out err ensuring rodc prefetch ran successfully self assertequal out replicating dn cn cn users replicating dn cn cn users self base dn self base dn', ''), ('test_multi_by_dn', 85, 'def test_multi_by_dn(self):\\n    result, out, err = self.runsubcmd(\\'rodc\\', \\'preload\\', \\n        \\'cn=sambatool3,cn=users,%s\\' % self.base_dn, \\n        \\'cn=sambatool4,cn=users,%s\\' % self.base_dn, \\'--server\\', os.environ[\\n        \\'DC_SERVER\\'])\\n    self.assertCmdSuccess(result, out, err,\\n        \\'ensuring rodc prefetch ran successfully\\')\\n    self.assertEqual(out, \\n        \"\"\"Replicating DN CN=sambatool3,CN=Users,%s\\nReplicating DN CN=sambatool4,CN=Users,%s\\n\"\"\"\\n         % (self.base_dn, self.base_dn))\\n', 'test multi by dn self result out err self runsubcmd rodc preload cn cn users self base dn cn cn users self base dn server os environ dc server self assertcmdsuccess result out err ensuring rodc prefetch ran successfully self assertequal out replicating dn cn cn users replicating dn cn cn users self base dn self base dn', ''), ('test_multi_in_file', 91, 'def test_multi_in_file(self):\\n    tempf = os.path.join(self.tempdir, \\'accountlist\\')\\n    open(tempf, \\'w\\').write(\\'sambatool1\\\\nsambatool2\\')\\n    result, out, err = self.runsubcmd(\\'rodc\\', \\'preload\\', \\'--file\\', tempf,\\n        \\'--server\\', os.environ[\\'DC_SERVER\\'])\\n    self.assertCmdSuccess(result, out, err,\\n        \\'ensuring rodc prefetch ran successfully\\')\\n    self.assertEqual(out, \\n        \"\"\"Replicating DN CN=sambatool1,CN=Users,%s\\nReplicating DN CN=sambatool2,CN=Users,%s\\n\"\"\"\\n         % (self.base_dn, self.base_dn))\\n    os.unlink(tempf)\\n', 'test multi in file self tempf os path join self tempdir accountlist open tempf write result out err self runsubcmd rodc preload file tempf server os environ dc server self assertcmdsuccess result out err ensuring rodc prefetch ran successfully self assertequal out replicating dn cn cn users replicating dn cn cn users self base dn self base dn os unlink tempf', ''), ('test_multi_with_missing_name_success', 100, \"def test_multi_with_missing_name_success(self):\\n    result, out, err = self.runsubcmd('rodc', 'preload', 'nonexistentuser1',\\n        'sambatool5', 'nonexistentuser2', '--server', os.environ[\\n        'DC_SERVER'], '--ignore-errors')\\n    self.assertCmdSuccess(result, out, err,\\n        'ensuring rodc prefetch ran successfully')\\n    self.assertEqual(out, 'Replicating DN CN=sambatool5,CN=Users,%s\\\\n' %\\n        self.base_dn)\\n\", 'test multi with missing name success self result out err self runsubcmd rodc preload server os environ dc server ignore errors self assertcmdsuccess result out err ensuring rodc prefetch ran successfully self assertequal out replicating dn cn cn users self base dn', ''), ('test_multi_with_missing_name_failure', 109, \"def test_multi_with_missing_name_failure(self):\\n    result, out, err = self.runsubcmd('rodc', 'preload', 'nonexistentuser1',\\n        'sambatool5', 'nonexistentuser2', '--server', os.environ['DC_SERVER'])\\n    self.assertCmdFail(result, 'ensuring rodc prefetch quit on missing user')\\n\", 'test multi with missing name failure self result out err self runsubcmd rodc preload server os environ dc server self assertcmdfail result ensuring rodc prefetch quit on missing user', ''), ('test_multi_without_group_success', 116, 'def test_multi_without_group_success(self):\\n    result, out, err = self.runsubcmd(\\'rodc\\', \\'preload\\', \\'sambatool6\\',\\n        \\'sambatool5\\', \\'--server\\', os.environ[\\'DC_SERVER\\'], \\'--ignore-errors\\')\\n    self.assertCmdSuccess(result, out, err,\\n        \\'ensuring rodc prefetch ran successfully\\')\\n    self.assertEqual(out, \\n        \"\"\"Replicating DN CN=sambatool6,CN=Users,%s\\nReplicating DN CN=sambatool5,CN=Users,%s\\n\"\"\"\\n         % (self.base_dn, self.base_dn))\\n', 'test multi without group success self result out err self runsubcmd rodc preload server os environ dc server ignore errors self assertcmdsuccess result out err ensuring rodc prefetch ran successfully self assertequal out replicating dn cn cn users replicating dn cn cn users self base dn self base dn', ''), ('test_multi_without_group_failure', 124, \"def test_multi_without_group_failure(self):\\n    result, out, err = self.runsubcmd('rodc', 'preload', 'sambatool6',\\n        'sambatool5', '--server', os.environ['DC_SERVER'])\\n    self.assertCmdFail(result,\\n        'ensuring rodc prefetch quit on non-replicated user')\\n\", 'test multi without group failure self result out err self runsubcmd rodc preload server os environ dc server self assertcmdfail result ensuring rodc prefetch quit on non replicated user', '')] [('__init__', 40, 'def __init__(self, qux, spam=False):\\n    \"\"\"Start the Foo.\\n\\n        :param qux: The first argument to initialize class.\\n        :type qux: string\\n        :param spam: Spam me yes or no...\\n        :type spam: bool\\n\\n        \"\"\"\\n    self.qux = 3\\n    self.spam = 4\\n    \"\"\"Docstring for instance attribute spam.\"\"\"\\n', 'init self qux spam false self qux 3 self spam 4 docstring for instance attribute spam', 'start the foo'), ('add', 55, 'def add(self, val1, val2):\\n    \"\"\"Return the added values.\\n\\n        :param val1: First number to add.\\n        :type val1: int\\n        :param val2: Second number to add.\\n        :type val2: int\\n        :rtype: int\\n\\n        \"\"\"\\n    return val1 + val2\\n', 'add self return', 'return the added values'), ('capitalize', 68, 'def capitalize(self, myvalue):\\n    \"\"\"Return a string as uppercase.\\n\\n        :param myvalue: String to change\\n        :type myvalue: string\\n        :rtype: string\\n\\n        \"\"\"\\n    return myvalue.upper()\\n', 'capitalize self myvalue return myvalue upper', 'return a string as uppercase'), ('another_function', 79, 'def another_function(self, a, b, **kwargs):\\n    \"\"\"\\n        Here is another function.\\n\\n        :param a: The number of green hats you own.\\n        :type a: int\\n\\n        :param b: The number of non-green hats you own.\\n        :type b: int\\n\\n        :param kwargs: Additional keyword arguments. Each keyword parameter\\n                       should specify the name of your favorite cuisine.\\n                       The values should be floats, specifying the mean price\\n                       of your favorite dish in that cooking style.\\n        :type kwargs: float\\n\\n        :returns: A 2-tuple.  The first element is the mean price of all dishes\\n                  across cuisines.  The second element is the total number of\\n                  hats you own: :math:`a + b`.\\n        :rtype: tuple\\n\\n        :raises ValueError: When ``a`` is not an integer.\\n\\n        \"\"\"\\n    return sum(kwargs.values()) / len(kwargs), a + b\\n', 'another function self kwargs return sum kwargs values len kwargs', 'here is another function')] [('__init__', 35, \"def __init__(self, *args, **kwargs):\\n    super(ILPostalCodeField, self).__init__('^\\\\\\\\d{5}$', *args, **kwargs)\\n\", 'init self args kwargs super ilpostalcodefield self init 5 args kwargs', ''), ('clean', 38, \"def clean(self, value):\\n    if value not in EMPTY_VALUES:\\n        value = value.replace(' ', '')\\n    return super(ILPostalCodeField, self).clean(value)\\n\", 'clean self value if value not in empty values value value replace return super ilpostalcodefield self clean value', ''), ('clean', 54, \"def clean(self, value):\\n    value = super(ILIDNumberField, self).clean(value)\\n    if value in EMPTY_VALUES:\\n        return ''\\n    match = id_number_re.match(value)\\n    if not match:\\n        raise ValidationError(self.error_messages['invalid'])\\n    value = match.group('number') + match.group('check')\\n    if not luhn(value):\\n        raise ValidationError(self.error_messages['invalid'])\\n    return value\\n\", 'clean self value value super ilidnumberfield self clean value if value in empty values return match id number re match value if not match raise validationerror self error messages invalid value match group number match group check if not luhn value raise validationerror self error messages invalid return value', '')] [('assertFullyNewStyle', 24, 'def assertFullyNewStyle(self, instance):\\n    \"\"\"\\n        Assert that the given object is an instance of a new-style class and\\n        that there are no classic classes in the inheritance hierarchy of\\n        that class.\\n\\n        This is a beneficial condition because PyPy is better able to\\n        optimize attribute lookup on such classes.\\n        \"\"\"\\n    self.assertIsInstance(instance, object)\\n    mro = inspect.getmro(type(instance))\\n    for subclass in mro:\\n        self.assertTrue(issubclass(subclass, object), \\'%r is not new-style\\' %\\n            (subclass,))\\n', 'assertfullynewstyle self instance self assertisinstance instance object mro inspect getmro type instance for subclass in mro self asserttrue issubclass subclass object is not new style subclass', 'assert that the given object is an instance of a new style class and that there are no classic classes in the inheritance hierarchy of that class'), ('test_newstyleReactor', 47, 'def test_newstyleReactor(self):\\n    \"\"\"\\n        Checks that all reactors on a platform have method resolution order\\n        containing only new style classes.\\n        \"\"\"\\n    reactor = self.buildReactor()\\n    self.assertFullyNewStyle(reactor)\\n', 'test newstylereactor self reactor self buildreactor self assertfullynewstyle reactor', 'checks that all reactors on a platform have method resolution order containing only new style classes'), ('test_stopWhenNotStarted', 62, 'def test_stopWhenNotStarted(self):\\n    \"\"\"\\n        C{reactor.stop()} raises L{RuntimeError} when called when the reactor\\n        has not been started.\\n        \"\"\"\\n    reactor = self.buildReactor()\\n    self.assertRaises(RuntimeError, reactor.stop)\\n', 'test stopwhennotstarted self reactor self buildreactor self assertraises runtimeerror reactor stop', 'c reactor stop raises l runtimeerror when called when the reactor has not been started'), ('test_stopWhenAlreadyStopped', 71, 'def test_stopWhenAlreadyStopped(self):\\n    \"\"\"\\n        C{reactor.stop()} raises L{RuntimeError} when called after the reactor\\n        has been stopped.\\n        \"\"\"\\n    reactor = self.buildReactor()\\n    reactor.callWhenRunning(reactor.stop)\\n    reactor.run()\\n    self.assertRaises(RuntimeError, reactor.stop)\\n', 'test stopwhenalreadystopped self reactor self buildreactor reactor callwhenrunning reactor stop reactor run self assertraises runtimeerror reactor stop', 'c reactor stop raises l runtimeerror when called after the reactor has been stopped'), ('test_callWhenRunningOrder', 82, 'def test_callWhenRunningOrder(self):\\n    \"\"\"\\n        Functions are run in the order that they were passed to\\n        L{reactor.callWhenRunning}.\\n        \"\"\"\\n    reactor = self.buildReactor()\\n    events = []\\n    reactor.callWhenRunning(events.append, \\'first\\')\\n    reactor.callWhenRunning(events.append, \\'second\\')\\n    reactor.callWhenRunning(reactor.stop)\\n    reactor.run()\\n    self.assertEqual(events, [\\'first\\', \\'second\\'])\\n', 'test callwhenrunningorder self reactor self buildreactor events reactor callwhenrunning events append first reactor callwhenrunning events append second reactor callwhenrunning reactor stop reactor run self assertequal events first second', 'functions are run in the order that they were passed to l reactor callwhenrunning'), ('test_runningForStartupEvents', 96, 'def test_runningForStartupEvents(self):\\n    \"\"\"\\n        The reactor is not running when C{\"before\"} C{\"startup\"} triggers are\\n        called and is running when C{\"during\"} and C{\"after\"} C{\"startup\"}\\n        triggers are called.\\n        \"\"\"\\n    reactor = self.buildReactor()\\n    state = {}\\n\\n    def beforeStartup():\\n        state[\\'before\\'] = reactor.running\\n\\n    def duringStartup():\\n        state[\\'during\\'] = reactor.running\\n\\n    def afterStartup():\\n        state[\\'after\\'] = reactor.running\\n    reactor.addSystemEventTrigger(\\'before\\', \\'startup\\', beforeStartup)\\n    reactor.addSystemEventTrigger(\\'during\\', \\'startup\\', duringStartup)\\n    reactor.addSystemEventTrigger(\\'after\\', \\'startup\\', afterStartup)\\n    reactor.callWhenRunning(reactor.stop)\\n    self.assertEqual(state, {})\\n    reactor.run()\\n    self.assertEqual(state, {\\'before\\': False, \\'during\\': True, \\'after\\': True})\\n', 'test runningforstartupevents self reactor self buildreactor state def beforestartup state before reactor running def duringstartup state during reactor running def afterstartup state after reactor running reactor addsystemeventtrigger before startup beforestartup reactor addsystemeventtrigger during startup duringstartup reactor addsystemeventtrigger after startup afterstartup reactor callwhenrunning reactor stop self assertequal state reactor run self assertequal state before false during true after true', 'the reactor is not running when c before c startup triggers are called and is running when c during and c after c startup triggers are called'), ('test_signalHandlersInstalledDuringStartup', 123, 'def test_signalHandlersInstalledDuringStartup(self):\\n    \"\"\"\\n        Signal handlers are installed in responsed to the C{\"during\"}\\n        C{\"startup\"}.\\n        \"\"\"\\n    reactor = self.buildReactor()\\n    phase = [None]\\n\\n    def beforeStartup():\\n        phase[0] = \\'before\\'\\n\\n    def afterStartup():\\n        phase[0] = \\'after\\'\\n    reactor.addSystemEventTrigger(\\'before\\', \\'startup\\', beforeStartup)\\n    reactor.addSystemEventTrigger(\\'after\\', \\'startup\\', afterStartup)\\n    sawPhase = [None]\\n\\n    def fakeSignal(signum, action):\\n        sawPhase[0] = phase[0]\\n    self.patch(signal, \\'signal\\', fakeSignal)\\n    reactor.callWhenRunning(reactor.stop)\\n    self.assertEqual(phase[0], None)\\n    self.assertEqual(sawPhase[0], None)\\n    reactor.run()\\n    self.assertEqual(sawPhase[0], \\'before\\')\\n    self.assertEqual(phase[0], \\'after\\')\\n', 'test signalhandlersinstalledduringstartup self reactor self buildreactor phase none def beforestartup phase 0 before def afterstartup phase 0 after reactor addsystemeventtrigger before startup beforestartup reactor addsystemeventtrigger after startup afterstartup sawphase none def fakesignal signum action sawphase 0 phase 0 self patch signal signal fakesignal reactor callwhenrunning reactor stop self assertequal phase 0 none self assertequal sawphase 0 none reactor run self assertequal sawphase 0 before self assertequal phase 0 after', 'signal handlers are installed in responsed to the c during c startup'), ('test_stopShutDownEvents', 149, 'def test_stopShutDownEvents(self):\\n    \"\"\"\\n        C{reactor.stop()} fires all three phases of shutdown event triggers\\n        before it makes C{reactor.run()} return.\\n        \"\"\"\\n    reactor = self.buildReactor()\\n    events = []\\n    reactor.addSystemEventTrigger(\\'before\\', \\'shutdown\\', lambda : events.\\n        append((\\'before\\', \\'shutdown\\')))\\n    reactor.addSystemEventTrigger(\\'during\\', \\'shutdown\\', lambda : events.\\n        append((\\'during\\', \\'shutdown\\')))\\n    reactor.addSystemEventTrigger(\\'after\\', \\'shutdown\\', lambda : events.\\n        append((\\'after\\', \\'shutdown\\')))\\n    reactor.callWhenRunning(reactor.stop)\\n    reactor.run()\\n    self.assertEquals(events, [(\\'before\\', \\'shutdown\\'), (\\'during\\',\\n        \\'shutdown\\'), (\\'after\\', \\'shutdown\\')])\\n', 'test stopshutdownevents self reactor self buildreactor events reactor addsystemeventtrigger before shutdown lambda events append before shutdown reactor addsystemeventtrigger during shutdown lambda events append during shutdown reactor addsystemeventtrigger after shutdown lambda events append after shutdown reactor callwhenrunning reactor stop reactor run self assertequals events before shutdown during shutdown after shutdown', 'c reactor stop fires all three phases of shutdown event triggers before it makes c reactor run return'), ('test_shutdownFiresTriggersAsynchronously', 172, 'def test_shutdownFiresTriggersAsynchronously(self):\\n    \"\"\"\\n        C{\"before\"} C{\"shutdown\"} triggers are not run synchronously from\\n        L{reactor.stop}.\\n        \"\"\"\\n    reactor = self.buildReactor()\\n    events = []\\n    reactor.addSystemEventTrigger(\\'before\\', \\'shutdown\\', events.append,\\n        \\'before shutdown\\')\\n\\n    def stopIt():\\n        reactor.stop()\\n        events.append(\\'stopped\\')\\n    reactor.callWhenRunning(stopIt)\\n    self.assertEqual(events, [])\\n    reactor.run()\\n    self.assertEqual(events, [\\'stopped\\', \\'before shutdown\\'])\\n', 'test shutdownfirestriggersasynchronously self reactor self buildreactor events reactor addsystemeventtrigger before shutdown events append before shutdown def stopit reactor stop events append stopped reactor callwhenrunning stopit self assertequal events reactor run self assertequal events stopped before shutdown', 'c before c shutdown triggers are not run synchronously from l reactor stop'), ('test_shutdownDisconnectsCleanly', 190, 'def test_shutdownDisconnectsCleanly(self):\\n    \"\"\"\\n        A L{IFileDescriptor.connectionLost} implementation which raises an\\n        exception does not prevent the remaining L{IFileDescriptor}s from\\n        having their C{connectionLost} method called.\\n        \"\"\"\\n    lostOK = [False]\\n\\n\\n    class ProblematicFileDescriptor(FileDescriptor):\\n\\n        def connectionLost(self, reason):\\n            raise RuntimeError(\\'simulated connectionLost error\\')\\n\\n\\n    class OKFileDescriptor(FileDescriptor):\\n\\n        def connectionLost(self, reason):\\n            lostOK[0] = True\\n    reactor = self.buildReactor()\\n    fds = iter([ProblematicFileDescriptor(), OKFileDescriptor()])\\n    reactor.removeAll = lambda : fds\\n    reactor.callWhenRunning(reactor.stop)\\n    self.runReactor(reactor)\\n    self.assertEqual(len(self.flushLoggedErrors(RuntimeError)), 1)\\n    self.assertTrue(lostOK[0])\\n', 'test shutdowndisconnectscleanly self lostok false class problematicfiledescriptor filedescriptor def connectionlost self reason raise runtimeerror simulated connectionlost error class okfiledescriptor filedescriptor def connectionlost self reason lostok 0 true reactor self buildreactor fds iter problematicfiledescriptor okfiledescriptor reactor removeall lambda fds reactor callwhenrunning reactor stop self runreactor reactor self assertequal len self flushloggederrors runtimeerror 1 self asserttrue lostok 0', 'a l ifiledescriptor connectionlost implementation which raises an exception does not prevent the remaining l ifiledescriptor s from having their c connectionlost method called'), ('test_multipleRun', 222, 'def test_multipleRun(self):\\n    \"\"\"\\n        C{reactor.run()} raises L{ReactorAlreadyRunning} when called when\\n        the reactor is already running.\\n        \"\"\"\\n    events = []\\n\\n    def reentrantRun():\\n        self.assertRaises(ReactorAlreadyRunning, reactor.run)\\n        events.append(\\'tested\\')\\n    reactor = self.buildReactor()\\n    reactor.callWhenRunning(reentrantRun)\\n    reactor.callWhenRunning(reactor.stop)\\n    reactor.run()\\n    self.assertEqual(events, [\\'tested\\'])\\n', 'test multiplerun self events def reentrantrun self assertraises reactoralreadyrunning reactor run events append tested reactor self buildreactor reactor callwhenrunning reentrantrun reactor callwhenrunning reactor stop reactor run self assertequal events tested', 'c reactor run raises l reactoralreadyrunning when called when the reactor is already running'), ('test_runWithAsynchronousBeforeStartupTrigger', 238, 'def test_runWithAsynchronousBeforeStartupTrigger(self):\\n    \"\"\"\\n        When there is a C{\\'before\\'} C{\\'startup\\'} trigger which returns an\\n        unfired L{Deferred}, C{reactor.run()} starts the reactor and does not\\n        return until after C{reactor.stop()} is called\\n        \"\"\"\\n    events = []\\n\\n    def trigger():\\n        events.append(\\'trigger\\')\\n        d = Deferred()\\n        d.addCallback(callback)\\n        reactor.callLater(0, d.callback, None)\\n        return d\\n\\n    def callback(ignored):\\n        events.append(\\'callback\\')\\n        reactor.stop()\\n    reactor = self.buildReactor()\\n    reactor.addSystemEventTrigger(\\'before\\', \\'startup\\', trigger)\\n    reactor.run()\\n    self.assertEqual(events, [\\'trigger\\', \\'callback\\'])\\n', 'test runwithasynchronousbeforestartuptrigger self events def trigger events append trigger deferred addcallback callback reactor calllater 0 callback none return def callback ignored events append callback reactor stop reactor self buildreactor reactor addsystemeventtrigger before startup trigger reactor run self assertequal events trigger callback', 'when there is a c before c startup trigger which returns an unfired l deferred c reactor run starts the reactor and does not return until after c reactor stop is called'), ('test_iterate', 260, 'def test_iterate(self):\\n    \"\"\"\\n        C{reactor.iterate()} does not block.\\n        \"\"\"\\n    reactor = self.buildReactor()\\n    t = reactor.callLater(5, reactor.crash)\\n    start = time.time()\\n    reactor.iterate(0)\\n    elapsed = time.time() - start\\n    self.failUnless(elapsed < 2)\\n    t.cancel()\\n', 'test iterate self reactor self buildreactor reactor calllater 5 reactor crash start time time reactor iterate 0 elapsed time time start self failunless elapsed 2 cancel', 'c reactor iterate does not block'), ('test_crash', 275, 'def test_crash(self):\\n    \"\"\"\\n        C{reactor.crash()} stops the reactor and does not fire shutdown\\n        triggers.\\n        \"\"\"\\n    reactor = self.buildReactor()\\n    events = []\\n    reactor.addSystemEventTrigger(\\'before\\', \\'shutdown\\', lambda : events.\\n        append((\\'before\\', \\'shutdown\\')))\\n    reactor.callWhenRunning(reactor.callLater, 0, reactor.crash)\\n    reactor.run()\\n    self.assertFalse(reactor.running)\\n    self.assertFalse(events,\\n        \\'Shutdown triggers invoked but they should not have been.\\')\\n', 'test crash self reactor self buildreactor events reactor addsystemeventtrigger before shutdown lambda events append before shutdown reactor callwhenrunning reactor calllater 0 reactor crash reactor run self assertfalse reactor running self assertfalse events shutdown triggers invoked but they should not have been', 'c reactor crash stops the reactor and does not fire shutdown triggers'), ('test_runAfterCrash', 293, 'def test_runAfterCrash(self):\\n    \"\"\"\\n        C{reactor.run()} restarts the reactor after it has been stopped by\\n        C{reactor.crash()}.\\n        \"\"\"\\n    events = []\\n\\n    def crash():\\n        events.append(\\'crash\\')\\n        reactor.crash()\\n    reactor = self.buildReactor()\\n    reactor.callWhenRunning(crash)\\n    reactor.run()\\n\\n    def stop():\\n        events.append((\\'stop\\', reactor.running))\\n        reactor.stop()\\n    reactor.callWhenRunning(stop)\\n    reactor.run()\\n    self.assertEqual(events, [\\'crash\\', (\\'stop\\', True)])\\n', 'test runaftercrash self events def crash events append crash reactor crash reactor self buildreactor reactor callwhenrunning crash reactor run def stop events append stop reactor running reactor stop reactor callwhenrunning stop reactor run self assertequal events crash stop true', 'c reactor run restarts the reactor after it has been stopped by c reactor crash'), ('test_runAfterStop', 313, 'def test_runAfterStop(self):\\n    \"\"\"\\n        C{reactor.run()} raises L{ReactorNotRestartable} when called when\\n        the reactor is being run after getting stopped priorly.\\n        \"\"\"\\n    events = []\\n\\n    def restart():\\n        self.assertRaises(ReactorNotRestartable, reactor.run)\\n        events.append(\\'tested\\')\\n    reactor = self.buildReactor()\\n    reactor.callWhenRunning(reactor.stop)\\n    reactor.addSystemEventTrigger(\\'after\\', \\'shutdown\\', restart)\\n    reactor.run()\\n    self.assertEqual(events, [\\'tested\\'])\\n', 'test runafterstop self events def restart self assertraises reactornotrestartable reactor run events append tested reactor self buildreactor reactor callwhenrunning reactor stop reactor addsystemeventtrigger after shutdown restart reactor run self assertequal events tested', 'c reactor run raises l reactornotrestartable when called when the reactor is being run after getting stopped priorly')] [('get_empty_dataset', 108, 'def get_empty_dataset(ds_id):\\n    return DataSet(ds_id)\\n', 'get empty dataset ds id return dataset ds id', ''), ('get_dataset', 113, 'def get_dataset(ds_id, table_id, columns):\\n    dataset = get_empty_dataset(ds_id)\\n    dataset.add_table(table_id, columns)\\n    return dataset\\n', 'get dataset ds id table id columns dataset get empty dataset ds id dataset add table table id columns return dataset', ''), ('get_hdu_type_dataset', 120, \"def get_hdu_type_dataset(dsId, columns, hduname='EVENTS'):\\n    dataset = DataSet(dsId)\\n    dataset.add_table(hduname, columns)\\n    dataset.tables['GTI'] = DsHelper.get_empty_gti_table()\\n    return dataset\\n\", 'get hdu type dataset dsid columns hduname events dataset dataset dsid dataset add table hduname columns dataset tables gti dshelper get empty gti table return dataset', ''), ('get_dataset_applying_gtis', 131, \"def get_dataset_applying_gtis(dsId, header, header_comments, ds_columns,\\n    ds_columns_errors, ev_list, ev_list_err, gti_start, gti_end,\\n    filter_start=None, filter_end=None, hduname='EVENTS', column=CONFIG.\\n    TIME_COLUMN):\\n    columns = [column]\\n    for column_name in ds_columns:\\n        columns.extend([column_name])\\n    additional_columns = DsHelper.get_additional_column_names(ds_columns,\\n        column)\\n    dataset = get_hdu_type_dataset(dsId, columns, hduname)\\n    dataset.tables[hduname].set_header_info(header, header_comments)\\n    must_filter = not (filter_start is None or filter_end is None)\\n    DsHelper.update_dataset_filtering_by_gti(dataset.tables[hduname],\\n        dataset.tables['GTI'], ev_list, ev_list_err, ds_columns,\\n        ds_columns_errors, gti_start, gti_end, additional_columns, column,\\n        filter_start, filter_end, must_filter)\\n    return dataset\\n\", 'get dataset applying gtis dsid header header comments ds columns ds columns errors ev list ev list err gti start gti end filter start none filter end none hduname events column config time column columns column for column name in ds columns columns extend column name additional columns dshelper get additional column names ds columns column dataset get hdu type dataset dsid columns hduname dataset tables hduname set header info header header comments must filter not filter start is none or filter end is none dshelper update dataset filtering by gti dataset tables hduname dataset tables gti ev list ev list err ds columns ds columns errors gti start gti end additional columns column filter start filter end must filter return dataset', ''), ('get_gti_dataset_from_stingray_gti', 160, \"def get_gti_dataset_from_stingray_gti(st_gtis):\\n    dataset = get_empty_dataset('GTI_DS')\\n    gti_table = DsHelper.get_gti_table_from_stingray_gti(st_gtis)\\n    dataset.tables['GTI'] = gti_table\\n    return dataset\\n\", 'get gti dataset from stingray gti st gtis dataset get empty dataset gti ds gti table dshelper get gti table from stingray gti st gtis dataset tables gti gti table return dataset', ''), ('get_lightcurve_dataset_from_stingray_lcurve', 168, \"def get_lightcurve_dataset_from_stingray_lcurve(lcurve, header,\\n    header_comments, hduname='RATE', column=CONFIG.TIME_COLUMN):\\n    lc_columns = [column, hduname]\\n    dataset = get_hdu_type_dataset('LIGHTCURVE', lc_columns, hduname)\\n    hdu_table = dataset.tables[hduname]\\n    hdu_table.set_header_info(header, header_comments)\\n    hdu_table.columns[lc_columns[0]].add_values(lcurve['time'])\\n    hdu_table.columns[lc_columns[1]].add_values(lcurve['counts'], lcurve[\\n        'counts_err'])\\n    dataset.tables['GTI'] = DsHelper.get_gti_table_from_stingray_gti(lcurve\\n        ['gti'])\\n    return dataset\\n\", 'get lightcurve dataset from stingray lcurve lcurve header header comments hduname rate column config time column lc columns column hduname dataset get hdu type dataset lightcurve lc columns hduname hdu table dataset tables hduname hdu table set header info header header comments hdu table columns lc columns 0 add values lcurve time hdu table columns lc columns 1 add values lcurve counts lcurve counts err dataset tables gti dshelper get gti table from stingray gti lcurve gti return dataset', ''), ('get_lightcurve_dataset_from_stingray_Lightcurve', 185, \"def get_lightcurve_dataset_from_stingray_Lightcurve(lcurve, header=None,\\n    header_comments=None, hduname='RATE', column=CONFIG.TIME_COLUMN):\\n    from astropy.io.fits import Header\\n    dataset = get_hdu_type_dataset('LIGHTCURVE', [column, hduname], hduname)\\n    hdu_table = dataset.tables[hduname]\\n    if header is None:\\n        if not hasattr(lcurve, 'header'):\\n            logging.warn('Light curve has no header')\\n            lcurve.header = Header()\\n        header = Header.fromstring(lcurve.header)\\n        header = dict()\\n        for header_column in header:\\n            header[header_column] = str(header[header_column])\\n            header_comments[header_column] = str(header.comments[header_column]\\n                )\\n    hdu_table.set_header_info(header, header_comments)\\n    hdu_table.columns[column].add_values(lcurve.time)\\n    hdu_table.columns[hduname].add_values(lcurve.counts, lcurve.counts_err)\\n    dataset.tables['GTI'] = DsHelper.get_gti_table_from_stingray_gti(lcurve.gti\\n        )\\n    return dataset\\n\", 'get lightcurve dataset from stingray lightcurve lcurve header none header comments none hduname rate column config time column from astropy io fits import header dataset get hdu type dataset lightcurve column hduname hduname hdu table dataset tables hduname if header is none if not hasattr lcurve header logging warn light curve has no header lcurve header header header header fromstring lcurve header header dict for header column in header header header column str header header column header comments header column str header comments header column hdu table set header info header header comments hdu table columns column add values lcurve time hdu table columns hduname add values lcurve counts lcurve counts err dataset tables gti dshelper get gti table from stingray gti lcurve gti return dataset', ''), ('get_eventlist_dataset_from_stingray_Eventlist', 216, \"def get_eventlist_dataset_from_stingray_Eventlist(evlist, header=None,\\n    header_comments=None, hduname='EVENTS', column=CONFIG.TIME_COLUMN):\\n    from astropy.io.fits import Header\\n    evt_columns = [column, 'PI']\\n    if hasattr(evlist, 'energy'):\\n        evt_columns = [column, 'PI', 'E']\\n    dataset = get_hdu_type_dataset('EVENTS', evt_columns, hduname)\\n    hdu_table = dataset.tables[hduname]\\n    if header is None:\\n        if not hasattr(evlist, 'header'):\\n            logging.warn('Event list has no header')\\n            evlist.header = Header()\\n        header = Header.fromstring(evlist.header)\\n        header = dict()\\n        for header_column in header:\\n            header[header_column] = str(header[header_column])\\n            header_comments[header_column] = str(header.comments[header_column]\\n                )\\n    hdu_table.set_header_info(header, header_comments)\\n    hdu_table.columns[column].add_values(evlist.time)\\n    if hasattr(evlist, 'energy'):\\n        if evlist.energy is not None and len(evlist.energy) == len(evlist.time\\n            ):\\n            hdu_table.columns['E'].add_values(evlist.energy)\\n        else:\\n            logging.warn(\\n                'Event list energies differs from event counts, setted all energies as 0'\\n                )\\n            hdu_table.columns['E'].add_values(np.zeros_like(evlist.time))\\n    if hasattr(evlist, 'pi') and evlist.pi is not None and len(evlist.pi\\n        ) == len(evlist.time):\\n        hdu_table.columns['PI'].add_values(evlist.pi)\\n    else:\\n        logging.warn('Event list has no PI values, using np.zeros_like')\\n        hdu_table.columns['PI'].add_values(np.zeros_like(evlist.time))\\n    dataset.tables['GTI'] = DsHelper.get_gti_table_from_stingray_gti(evlist.gti\\n        )\\n    return dataset\\n\", 'get eventlist dataset from stingray eventlist evlist header none header comments none hduname events column config time column from astropy io fits import header evt columns column pi if hasattr evlist energy evt columns column pi dataset get hdu type dataset events evt columns hduname hdu table dataset tables hduname if header is none if not hasattr evlist header logging warn event list has no header evlist header header header header fromstring evlist header header dict for header column in header header header column str header header column header comments header column str header comments header column hdu table set header info header header comments hdu table columns column add values evlist time if hasattr evlist energy if evlist energy is not none and len evlist energy len evlist time hdu table columns add values evlist energy else logging warn event list energies differs from event counts setted all energies as 0 hdu table columns add values np zeros like evlist time if hasattr evlist pi and evlist pi is not none and len evlist pi len evlist time hdu table columns pi add values evlist pi else logging warn event list has no pi values using np zeros like hdu table columns pi add values np zeros like evlist time dataset tables gti dshelper get gti table from stingray gti evlist gti return dataset', ''), ('__init__', 14, 'def __init__(self, id):\\n    self.id = id + str(randint(0, 99999))\\n    self.tables = dict()\\n', 'init self id self id id str randint 0 99999 self tables dict', ''), ('add_table', 18, 'def add_table(self, table_id, column_names):\\n    self.tables[table_id] = Table(table_id)\\n    self.tables[table_id].add_columns(column_names)\\n', 'add table self table id column names self tables table id table table id self tables table id add columns column names', ''), ('get_schema', 22, 'def get_schema(self):\\n    schema = dict()\\n    for table_id in self.tables:\\n        schema[table_id] = self.tables[table_id].get_schema()\\n    return schema\\n', 'get schema self schema dict for table id in self tables schema table id self tables table id get schema return schema', ''), ('get_header', 28, 'def get_header(self):\\n    header = dict()\\n    for table_id in self.tables:\\n        header[table_id] = self.tables[table_id].get_header()\\n    return header\\n', 'get header self header dict for table id in self tables header table id self tables table id get header return header', ''), ('clone', 34, 'def clone(self, with_values=True):\\n    dataset = DataSet(self.id)\\n    for table_id in self.tables:\\n        table = self.tables[table_id].clone(with_values)\\n        dataset.tables[table_id] = table\\n    return dataset\\n', 'clone self with values true dataset dataset self id for table id in self tables table self tables table id clone with values dataset tables table id table return dataset', ''), ('apply_filters', 43, \"def apply_filters(self, filters):\\n    if not filters or not len(filters):\\n        return self\\n    filtered_dataset = self.clone()\\n    time_filter = FltHelper.get_time_filter(filters)\\n    if time_filter:\\n        filtered_dataset = self.apply_time_filter(time_filter, time_filter[\\n            'table'])\\n    for filter in filters:\\n        table_id = filter['table']\\n        if table_id not in ['EVENTS', 'RATE'] or filter['column'\\n            ] != CONFIG.TIME_COLUMN:\\n            if table_id in filtered_dataset.tables:\\n                filtered_dataset.tables[table_id] = filtered_dataset.tables[\\n                    table_id].apply_filter(filter)\\n            else:\\n                logging.error('dataset.apply_filters wrong table_id: %s' %\\n                    table_id)\\n    return filtered_dataset\\n\", 'apply filters self filters if not filters or not len filters return self filtered dataset self clone time filter flthelper get time filter filters if time filter filtered dataset self apply time filter time filter time filter table for filter in filters table id filter table if table id not in events rate or filter column config time column if table id in filtered dataset tables filtered dataset tables table id filtered dataset tables table id apply filter filter else logging error dataset apply filters wrong table id table id return filtered dataset', ''), ('join', 64, 'def join(self, dataset):\\n    joined_dataset = self.clone()\\n    for table_id in joined_dataset.tables:\\n        if table_id in dataset.tables:\\n            table = joined_dataset.tables[table_id].join(dataset.tables[\\n                table_id])\\n            joined_dataset.tables[table_id] = table\\n    return joined_dataset\\n', 'join self dataset joined dataset self clone for table id in joined dataset tables if table id in dataset tables table joined dataset tables table id join dataset tables table id joined dataset tables table id table return joined dataset', ''), ('apply_time_filter', 74, \"def apply_time_filter(self, filter, hduname='EVENTS', column=CONFIG.TIME_COLUMN\\n    ):\\n    if 'GTI' not in self.tables:\\n        logging.warn('dataset.apply_time_filter: Dataset GTIs missed')\\n        return self\\n    if len(self.tables['GTI'].columns['START'].values) == 0:\\n        logging.warn('dataset.apply_time_filter: Dataset no valid GTIs')\\n        return self\\n    columns_values = dict()\\n    columns_error_values = dict()\\n    for column_name in self.tables[hduname].columns:\\n        if column_name != column:\\n            columns_values[column_name] = self.tables[hduname].columns[\\n                column_name].values\\n            columns_error_values[column_name] = self.tables[hduname].columns[\\n                column_name].error_values\\n    ev_list = self.tables[hduname].columns[column].values\\n    ev_list_err = self.tables[hduname].columns[column].error_values\\n    gti_start = self.tables['GTI'].columns['START'].values\\n    gti_end = self.tables['GTI'].columns['STOP'].values\\n    dataset = get_dataset_applying_gtis(self.id, self.tables[hduname].\\n        header, self.tables[hduname].header_comments, columns_values,\\n        columns_error_values, ev_list, ev_list_err, gti_start, gti_end,\\n        filter['from'], filter['to'], hduname, column)\\n    return dataset\\n\", 'apply time filter self filter hduname events column config time column if gti not in self tables logging warn dataset apply time filter dataset gtis missed return self if len self tables gti columns start values 0 logging warn dataset apply time filter dataset no valid gtis return self columns values dict columns error values dict for column name in self tables hduname columns if column name column columns values column name self tables hduname columns column name values columns error values column name self tables hduname columns column name error values ev list self tables hduname columns column values ev list err self tables hduname columns column error values gti start self tables gti columns start values gti end self tables gti columns stop values dataset get dataset applying gtis self id self tables hduname header self tables hduname header comments columns values columns error values ev list ev list err gti start gti end filter from filter to hduname column return dataset', '')] [('save', 28, 'def save(self, commit=True):\\n    obj = super().save(commit=False)\\n    if not obj.id:\\n        obj.status = Member.STATUS_ACTIVE\\n    if commit:\\n        obj.save()\\n    return obj\\n', 'save self commit true obj super save commit false if not obj id obj status member status active if commit obj save return obj', ''), ('clean_birthday', 38, \"def clean_birthday(self):\\n    if self.instance.id and not self.request.user.has_perm('team.change_member'\\n        ):\\n        return self.instance.birthday\\n    else:\\n        return self.cleaned_data.get('birthday')\\n\", 'clean birthday self if self instance id and not self request user has perm team change member return self instance birthday else return self cleaned data get birthday', ''), ('clean', 44, \"def clean(self):\\n    cleaned_data = self.cleaned_data\\n    try:\\n        slug = CustomSlug.objects.get(first_name=cleaned_data.get(\\n            'first_name', ''), last_name=cleaned_data.get('last_name', ''),\\n            birthday=cleaned_data.get('birthday', '')).slug\\n    except CustomSlug.DoesNotExist:\\n        slug = slugify('%s-%s-%i' % (cleaned_data.get('first_name', ''),\\n            cleaned_data.get('last_name', ''), cleaned_data.get('birthday',\\n            timezone.now()).year), only_ascii=True)\\n    try:\\n        if 'status' not in self.cleaned_data or self.cleaned_data.get('status'\\n            ) == Member.STATUS_ACTIVE:\\n            member = Member.objects.exclude(id=self.instance.id).filter(status\\n                =Member.STATUS_ACTIVE).get(team__distance__competition_id=\\n                self.instance.team.distance.competition_id, slug=slug,\\n                team__distance_id=self.instance.team.distance_id)\\n            self._errors.update({'first_name': [_(\\n                'Member with this First Name and Last Name is already registered in other team - {0}.'\\n                ).format(member.team)]})\\n    except:\\n        pass\\n    return cleaned_data\\n\", 'clean self cleaned data self cleaned data try slug customslug objects get first name cleaned data get first name last name cleaned data get last name birthday cleaned data get birthday slug except customslug doesnotexist slug slugify cleaned data get first name cleaned data get last name cleaned data get birthday timezone now year only ascii true try if status not in self cleaned data or self cleaned data get status member status active member member objects exclude id self instance id filter status member status active get team distance competition id self instance team distance competition id slug slug team distance id self instance team distance id self errors update first name member with this first name and last name is already registered in other team 0 format member team except pass return cleaned data', ''), ('clean_first_name', 68, \"def clean_first_name(self):\\n    if self.instance.id and not self.request.user.has_perm('team.change_member'\\n        ):\\n        return self.instance.first_name\\n    else:\\n        return self.cleaned_data.get('first_name').strip().title()\\n\", 'clean first name self if self instance id and not self request user has perm team change member return self instance first name else return self cleaned data get first name strip title', ''), ('clean_gender', 74, \"def clean_gender(self):\\n    if self.instance.id and not self.request.user.has_perm('team.change_member'\\n        ):\\n        return self.instance.gender\\n    else:\\n        return self.cleaned_data.get('gender')\\n\", 'clean gender self if self instance id and not self request user has perm team change member return self instance gender else return self cleaned data get gender', ''), ('clean_last_name', 80, \"def clean_last_name(self):\\n    if self.instance.id and not self.request.user.has_perm('team.change_member'\\n        ):\\n        return self.instance.last_name\\n    else:\\n        return self.cleaned_data.get('last_name').strip().title()\\n\", 'clean last name self if self instance id and not self request user has perm team change member return self instance last name else return self cleaned data get last name strip title', ''), ('clean_country', 86, \"def clean_country(self):\\n    if self.instance.id and not self.request.user.has_perm('team.change_member'\\n        ):\\n        return self.instance.country\\n    else:\\n        return self.cleaned_data.get('country')\\n\", 'clean country self if self instance id and not self request user has perm team change member return self instance country else return self cleaned data get country', ''), ('__init__', 92, \"def __init__(self, *args, **kwargs):\\n    super().__init__(*args, **kwargs)\\n    self.fields['country'].initial = 'LV'\\n    self.fields['country'].required = True\\n    self.fields['birthday'].required = True\\n    self.fields['first_name'].required = True\\n    self.fields['last_name'].required = True\\n    self.fields['country'].required = True\\n    self.fields['gender'].required = True\\n    if self.instance.id and not self.request.user.has_perm('team.change_member'\\n        ):\\n        self.fields['first_name'].widget.attrs['readonly'] = True\\n        self.fields['last_name'].widget.attrs['readonly'] = True\\n        self.fields['country'].widget.attrs['readonly'] = True\\n        self.fields['birthday'].widget.attrs['readonly'] = True\\n        self.fields['gender'].widget.attrs['readonly'] = True\\n    self.helper = FormHelper()\\n    self.helper.form_tag = False\\n    self.helper.include_media = False\\n    self.helper.template = 'team/form/member_inline.html'\\n    self.helper.layout = Layout()\\n\", 'init self args kwargs super init args kwargs self fields country initial lv self fields country required true self fields birthday required true self fields first name required true self fields last name required true self fields country required true self fields gender required true if self instance id and not self request user has perm team change member self fields first name widget attrs readonly true self fields last name widget attrs readonly true self fields country widget attrs readonly true self fields birthday widget attrs readonly true self fields gender widget attrs readonly true self helper formhelper self helper form tag false self helper include media false self helper template team form member inline html self helper layout layout', ''), ('clean_distance', 125, \"def clean_distance(self):\\n    distance = self.cleaned_data.get('distance')\\n    if self.instance.id:\\n        return self.instance.distance\\n    else:\\n        return distance\\n\", 'clean distance self distance self cleaned data get distance if self instance id return self instance distance else return distance', ''), ('clean_title', 132, \"def clean_title(self):\\n    title = self.cleaned_data.get('title')\\n    try:\\n        Team.objects.exclude(id=self.instance.id).get(slug=slugify(title),\\n            distance=self.cleaned_data.get('distance'))\\n    except Team.DoesNotExist:\\n        return title\\n    except AttributeError:\\n        return title\\n    raise forms.ValidationError(_('Team with such title already exist.'))\\n\", 'clean title self title self cleaned data get title try team objects exclude id self instance id get slug slugify title distance self cleaned data get distance except team doesnotexist return title except attributeerror return title raise forms validationerror team with such title already exist', ''), ('save', 142, 'def save(self, commit=True):\\n    obj = super().save(commit=False)\\n    if not obj.id:\\n        obj.owner = self.request.user\\n        obj.created_by = self.request.user\\n    else:\\n        obj.modified_by = self.request.user\\n    if commit:\\n        obj.save()\\n    return obj\\n', 'save self commit true obj super save commit false if not obj id obj owner self request user obj created by self request user else obj modified by self request user if commit obj save return obj', ''), ('__init__', 155, \"def __init__(self, *args, **kwargs):\\n    super().__init__(*args, **kwargs)\\n    self.fields['email'].required = True\\n    self.fields['is_w'].required = False\\n    self.fields['is_w'].label = 'Sieviešu komanda?'\\n    self.fields['country'].initial = 'LV'\\n    distances = Distance.objects.filter(can_have_teams=True,\\n        competition__is_in_menu=True).exclude(competition__competition_date__lt\\n        =timezone.now()).order_by('competition_id', 'id')\\n    self.fields['distance'].choices = [('', '------')] + [(str(distance.id),\\n        '{0} - {1}'.format(str(distance.competition), str(distance))) for\\n        distance in distances]\\n    if self.instance.distance_id:\\n        try:\\n            if self.instance.distance not in distances:\\n                distance = self.instance.distance\\n                self.fields['distance'].choices.append((str(distance.id),\\n                    '{0} - {1}'.format(str(distance.competition), str(\\n                    distance))))\\n        except Distance.DoesNotExist:\\n            pass\\n    if self.instance.id:\\n        self.fields['distance'].widget.attrs['readonly'] = True\\n\", 'init self args kwargs super init args kwargs self fields email required true self fields is required false self fields is label sievie komanda self fields country initial lv distances distance objects filter can have teams true competition is in menu true exclude competition competition date lt timezone now order by competition id id self fields distance choices str distance id 0 1 format str distance competition str distance for distance in distances if self instance distance id try if self instance distance not in distances distance self instance distance self fields distance choices append str distance id 0 1 format str distance competition str distance except distance doesnotexist pass if self instance id self fields distance widget attrs readonly true', '')] [('create', 7, \"def create(kernel):\\n    result = Creature()\\n    result.template = 'object/mobile/shared_verne_bull.iff'\\n    result.attribute_template_id = 9\\n    result.stfName('monster_name', 'verne')\\n    return result\\n\", 'create kernel result creature result template object mobile shared verne bull iff result attribute template id 9 result stfname monster name verne return result', '')] [('get_app_label', 27, 'def get_app_label(app):\\n    \"\"\"\\n    Returns the _internal_ app label for the given app module.\\n    i.e. for <module django.contrib.auth.models> will return \\'auth\\'\\n    \"\"\"\\n    return app.__name__.split(\\'.\\')[-2]\\n', 'get app label app return app name split 2', 'returns the internal app label for the given app module i e for module django contrib auth models will return auth'), ('app_label_to_app_module', 35, 'def app_label_to_app_module(app_label):\\n    \"\"\"\\n    Given the app label, returns the module of the app itself (unlike models.get_app,\\n    which returns the models module)\\n    \"\"\"\\n    app = models.get_app(app_label)\\n    module_name = \\'.\\'.join(app.__name__.split(\\'.\\')[:-1])\\n    try:\\n        module = sys.modules[module_name]\\n    except KeyError:\\n        __import__(module_name, {}, {}, [\\'\\'])\\n        module = sys.modules[module_name]\\n    return module\\n', 'app label to app module app label app models get app app label module name join app name split 1 try module sys modules module name except keyerror import module name module sys modules module name return module', 'given the app label returns the module of the app itself unlike models get app which returns the models module'), ('flatten', 51, \"def flatten(*stack):\\n    stack = deque(stack)\\n    while stack:\\n        try:\\n            x = next(stack[0])\\n        except TypeError:\\n            stack[0] = iter(stack[0])\\n            x = next(stack[0])\\n        except StopIteration:\\n            stack.popleft()\\n            continue\\n        if hasattr(x, '__iter__') and not isinstance(x, str):\\n            stack.appendleft(x)\\n        else:\\n            yield x\\n\", 'flatten stack stack deque stack while stack try next stack 0 except typeerror stack 0 iter stack 0 next stack 0 except stopiteration stack popleft continue if hasattr iter and not isinstance str stack appendleft else yield', ''), ('_dfs', 71, 'def _dfs(start, get_children, path):\\n    if (start, get_children) in dependency_cache:\\n        return dependency_cache[start, get_children]\\n    results = []\\n    if start in path:\\n        raise exceptions.CircularDependency(path[path.index(start):] + [start])\\n    path.append(start)\\n    results.append(start)\\n    children = sorted(get_children(start), key=lambda x: str(x))\\n    for n in children:\\n        results = _dfs(n, get_children, path) + results\\n    path.pop()\\n    results = list(SortedSet(results))\\n    dependency_cache[start, get_children] = results\\n    return results\\n', 'dfs start get children path if start get children in dependency cache return dependency cache start get children results if start in path raise exceptions circulardependency path path index start start path append start results append start children sorted get children start key lambda str for in children results dfs get children path results path pop results list sortedset results dependency cache start get children results return results', ''), ('dfs', 93, 'def dfs(start, get_children):\\n    return _dfs(start, get_children, [])\\n', 'dfs start get children return dfs start get children', ''), ('depends', 97, 'def depends(start, get_children):\\n    return dfs(start, get_children)\\n', 'depends start get children return dfs start get children', ''), ('__init__', 11, 'def __init__(self, data=tuple()):\\n    self.extend(data)\\n', 'init self data tuple self extend data', ''), ('__str__', 14, \"def __str__(self):\\n    return 'SortedSet(%s)' % list(self)\\n\", 'str self return sortedset list self', ''), ('add', 17, 'def add(self, value):\\n    self[value] = True\\n', 'add self value self value true', ''), ('remove', 20, 'def remove(self, value):\\n    del self[value]\\n', 'remove self value del self value', ''), ('extend', 23, 'def extend(self, iterable):\\n    [self.add(k) for k in iterable]\\n', 'extend self iterable self add for in iterable', '')] [('__init__', 21, 'def __init__(self, args, copy_thread=1):\\n    \"\"\"Initializer.  copy_thread arg shows if the copy process is separate\\n        from main Playback thread or not.  copy_thread=0 means copying happens\\n        in same process.\\n        \"\"\"\\n    Replicator.__init__(self, args)\\n    if not copy_thread:\\n        raise Exception(\\'Combined copy not supported\\')\\n    if len(self.args) != 3:\\n        self.log.error(\\'londiste copy requires table name\\')\\n        sys.exit(1)\\n    self.copy_table_name = self.args[2]\\n    sfx = self.get_copy_suffix(self.copy_table_name)\\n    self.old_consumer_name = self.consumer_name\\n    self.pidfile += sfx\\n    self.consumer_name += sfx\\n    self.copy_thread = 1\\n    self.main_worker = False\\n', 'init self args copy thread 1 replicator init self args if not copy thread raise exception combined copy not supported if len self args 3 self log error londiste copy requires table name sys exit 1 self copy table name self args 2 sfx self get copy suffix self copy table name self old consumer name self consumer name self pidfile sfx self consumer name sfx self copy thread 1 self main worker false', 'initializer copy thread arg shows if the copy process is separate from main playback thread or not copy thread 0 means copying happens in same process'), ('get_copy_suffix', 44, \"def get_copy_suffix(self, tblname):\\n    return '.copy.%s' % tblname\\n\", 'get copy suffix self tblname return copy tblname', ''), ('reload_table_stat', 47, \"def reload_table_stat(self, dst_curs, tblname):\\n    self.load_table_state(dst_curs)\\n    if tblname not in self.table_map:\\n        self.log.warning('Table %s removed from replication', tblname)\\n        sys.exit(1)\\n    t = self.table_map[tblname]\\n    return t\\n\", 'reload table stat self dst curs tblname self load table state dst curs if tblname not in self table map self log warning table removed from replication tblname sys exit 1 self table map tblname return', ''), ('do_copy', 55, 'def do_copy(self, tbl_stat, src_db, dst_db):\\n    \"\"\"Entry point into copying logic.\"\"\"\\n    dst_db.commit()\\n    src_curs = src_db.cursor()\\n    dst_curs = dst_db.cursor()\\n    while 1:\\n        if tbl_stat.copy_role == \\'wait-copy\\':\\n            self.log.info(\\'waiting for first partition to initialize copy\\')\\n        elif tbl_stat.max_parallel_copies_reached():\\n            self.log.info(\\'number of max parallel copies (%s) reached\\',\\n                tbl_stat.max_parallel_copy)\\n        else:\\n            break\\n        time.sleep(10)\\n        tbl_stat = self.reload_table_stat(dst_curs, tbl_stat.name)\\n        dst_db.commit()\\n    while 1:\\n        pmap = self.get_state_map(src_db.cursor())\\n        src_db.commit()\\n        if tbl_stat.name not in pmap:\\n            raise Exception(\\'table %s not available on provider\\' % tbl_stat\\n                .name)\\n        pt = pmap[tbl_stat.name]\\n        if pt.state == TABLE_OK:\\n            break\\n        self.log.warning(\\'table %s not in sync yet on provider, waiting\\',\\n            tbl_stat.name)\\n        time.sleep(10)\\n    src_real_table = pt.dest_table\\n    cmode = 1\\n    if tbl_stat.copy_role == \\'lead\\':\\n        cmode = 2\\n    elif tbl_stat.copy_role:\\n        cmode = 0\\n    oldiso = src_db.isolation_level\\n    src_db.set_isolation_level(skytools.I_REPEATABLE_READ)\\n    src_db.commit()\\n    self.sync_database_encodings(src_db, dst_db)\\n    self.log.info(\\'Starting full copy of %s\\', tbl_stat.name)\\n    if cmode > 0:\\n        self.drop_fkeys(dst_db, tbl_stat.dest_table)\\n    if cmode > 0:\\n        q = \\'lock table \\' + skytools.quote_fqident(tbl_stat.dest_table)\\n        dst_curs.execute(q)\\n    src_struct = TableStruct(src_curs, src_real_table)\\n    dst_struct = TableStruct(dst_curs, tbl_stat.dest_table)\\n    dlist = dst_struct.get_column_list()\\n    slist = src_struct.get_column_list()\\n    common_cols = []\\n    for c in slist:\\n        if c not in dlist:\\n            self.log.warning(\\'Table %s column %s does not exist on subscriber\\',\\n                tbl_stat.name, c)\\n        else:\\n            common_cols.append(c)\\n    for c in dlist:\\n        if c not in slist:\\n            self.log.warning(\\'Table %s column %s does not exist on provider\\',\\n                tbl_stat.name, c)\\n    if cmode > 0:\\n        objs = T_CONSTRAINT | T_INDEX | T_RULE | T_PARENT\\n        dst_struct.drop(dst_curs, objs, log=self.log)\\n        if tbl_stat.table_attrs.get(\\'skip_truncate\\'):\\n            self.log.info(\\'%s: skipping truncate\\', tbl_stat.name)\\n        else:\\n            self.log.info(\\'%s: truncating\\', tbl_stat.name)\\n            q = \\'truncate \\'\\n            if dst_db.server_version >= 80400:\\n                q += \\'only \\'\\n            q += skytools.quote_fqident(tbl_stat.dest_table)\\n            dst_curs.execute(q)\\n        if cmode == 2 and tbl_stat.dropped_ddl is None:\\n            ddl = dst_struct.get_create_sql(objs)\\n            if ddl:\\n                q = \\'select * from londiste.local_set_table_struct(%s, %s, %s)\\'\\n                self.exec_cmd(dst_curs, q, [self.queue_name, tbl_stat.name,\\n                    ddl])\\n            else:\\n                ddl = None\\n            dst_db.commit()\\n            tbl_stat.dropped_ddl = ddl\\n    self.log.info(\\'%s: start copy\\', tbl_stat.name)\\n    p = tbl_stat.get_plugin()\\n    stats = p.real_copy(src_real_table, src_curs, dst_curs, common_cols)\\n    if stats:\\n        self.log.info(\\'%s: copy finished: %d bytes, %d rows\\', tbl_stat.name,\\n            stats[0], stats[1])\\n    src_curs.execute(\\'select txid_current_snapshot()\\')\\n    snapshot = src_curs.fetchone()[0]\\n    src_db.commit()\\n    src_db.set_isolation_level(oldiso)\\n    src_db.commit()\\n    tbl_stat.change_state(TABLE_CATCHING_UP)\\n    tbl_stat.change_snapshot(snapshot)\\n    self.save_table_state(dst_curs)\\n    if cmode == 1:\\n        dst_struct.create(dst_curs, objs, log=self.log)\\n    elif cmode == 2:\\n        dst_db.commit()\\n        while tbl_stat.copy_role:\\n            self.log.info(\\'waiting for other partitions to finish copy\\')\\n            time.sleep(10)\\n            tbl_stat = self.reload_table_stat(dst_curs, tbl_stat.name)\\n            dst_db.commit()\\n        if tbl_stat.dropped_ddl is not None:\\n            self.looping = 0\\n            for ddl in skytools.parse_statements(tbl_stat.dropped_ddl):\\n                self.log.info(ddl)\\n                dst_curs.execute(ddl)\\n            q = \\'select * from londiste.local_set_table_struct(%s, %s, NULL)\\'\\n            self.exec_cmd(dst_curs, q, [self.queue_name, tbl_stat.name])\\n            tbl_stat.dropped_ddl = None\\n            self.looping = 1\\n        dst_db.commit()\\n    if not self.copy_thread:\\n        tbl_stat.change_state(TABLE_OK)\\n        self.save_table_state(dst_curs)\\n    dst_db.commit()\\n    if tbl_stat.copy_role == \\'wait-replay\\':\\n        return\\n    q = \\'select pgq.force_tick(%s)\\'\\n    src_curs.execute(q, [self.queue_name])\\n    src_db.commit()\\n', 'do copy self tbl stat src db dst db dst db commit src curs src db cursor dst curs dst db cursor while 1 if tbl stat copy role wait copy self log info waiting for first partition to initialize copy elif tbl stat max parallel copies reached self log info number of max parallel copies reached tbl stat max parallel copy else break time sleep 10 tbl stat self reload table stat dst curs tbl stat name dst db commit while 1 pmap self get state map src db cursor src db commit if tbl stat name not in pmap raise exception table not available on provider tbl stat name pt pmap tbl stat name if pt state table ok break self log warning table not in sync yet on provider waiting tbl stat name time sleep 10 src real table pt dest table cmode 1 if tbl stat copy role lead cmode 2 elif tbl stat copy role cmode 0 oldiso src db isolation level src db set isolation level skytools repeatable read src db commit self sync database encodings src db dst db self log info starting full copy of tbl stat name if cmode 0 self drop fkeys dst db tbl stat dest table if cmode 0 lock table skytools quote fqident tbl stat dest table dst curs execute src struct tablestruct src curs src real table dst struct tablestruct dst curs tbl stat dest table dlist dst struct get column list slist src struct get column list common cols for in slist if not in dlist self log warning table column does not exist on subscriber tbl stat name else common cols append for in dlist if not in slist self log warning table column does not exist on provider tbl stat name if cmode 0 objs constraint index rule parent dst struct drop dst curs objs log self log if tbl stat table attrs get skip truncate self log info skipping truncate tbl stat name else self log info truncating tbl stat name truncate if dst db server version 80400 only skytools quote fqident tbl stat dest table dst curs execute if cmode 2 and tbl stat dropped ddl is none ddl dst struct get create sql objs if ddl select from londiste local set table struct self exec cmd dst curs self queue name tbl stat name ddl else ddl none dst db commit tbl stat dropped ddl ddl self log info start copy tbl stat name tbl stat get plugin stats real copy src real table src curs dst curs common cols if stats self log info copy finished bytes rows tbl stat name stats 0 stats 1 src curs execute select txid current snapshot snapshot src curs fetchone 0 src db commit src db set isolation level oldiso src db commit tbl stat change state table catching up tbl stat change snapshot snapshot self save table state dst curs if cmode 1 dst struct create dst curs objs log self log elif cmode 2 dst db commit while tbl stat copy role self log info waiting for other partitions to finish copy time sleep 10 tbl stat self reload table stat dst curs tbl stat name dst db commit if tbl stat dropped ddl is not none self looping 0 for ddl in skytools parse statements tbl stat dropped ddl self log info ddl dst curs execute ddl select from londiste local set table struct null self exec cmd dst curs self queue name tbl stat name tbl stat dropped ddl none self looping 1 dst db commit if not self copy thread tbl stat change state table ok self save table state dst curs dst db commit if tbl stat copy role wait replay return select pgq force tick src curs execute self queue name src db commit', 'entry point into copying logic'), ('work', 224, 'def work(self):\\n    if not self.reg_ok:\\n        self.register_copy_consumer()\\n        self.reg_ok = True\\n    return Replicator.work(self)\\n', 'work self if not self reg ok self register copy consumer self reg ok true return replicator work self', ''), ('register_copy_consumer', 231, 'def register_copy_consumer(self):\\n    dst_db = self.get_database(\\'db\\')\\n    dst_curs = dst_db.cursor()\\n    q = \\'select * from londiste.get_table_list(%s) where table_name = %s\\'\\n    dst_curs.execute(q, [self.queue_name, self.copy_table_name])\\n    rows = dst_curs.fetchall()\\n    attrs = {}\\n    if len(rows) > 0:\\n        v_attrs = rows[0][\\'table_attrs\\']\\n        if v_attrs:\\n            attrs = skytools.db_urldecode(v_attrs)\\n    q = \\'select * from pgq_node.get_consumer_state(%s, %s)\\'\\n    rows = self.exec_cmd(dst_db, q, [self.queue_name, self.old_consumer_name])\\n    state = rows[0]\\n    source_node = state[\\'provider_node\\']\\n    source_location = state[\\'provider_location\\']\\n    if \\'copy_node\\' in attrs:\\n        if attrs[\\'copy_node\\'] == \\'?\\':\\n            source_node, source_location, wname = find_copy_source(self,\\n                self.queue_name, self.copy_table_name, source_node,\\n                source_location)\\n        else:\\n            source_node = attrs[\\'copy_node\\']\\n            q = (\\n                \\'select * from pgq_node.get_queue_locations(%s) where node_name = %s\\'\\n                )\\n            dst_curs.execute(q, [self.queue_name, source_node])\\n            rows = dst_curs.fetchall()\\n            if len(rows):\\n                source_location = rows[0][\\'node_location\\']\\n    self.log.info(\"Using \\'%s\\' as source node\", source_node)\\n    self.register_consumer(source_location)\\n', 'register copy consumer self dst db self get database db dst curs dst db cursor select from londiste get table list where table name dst curs execute self queue name self copy table name rows dst curs fetchall attrs if len rows 0 attrs rows 0 table attrs if attrs attrs skytools db urldecode attrs select from pgq node get consumer state rows self exec cmd dst db self queue name self old consumer name state rows 0 source node state provider node source location state provider location if copy node in attrs if attrs copy node source node source location wname find copy source self self queue name self copy table name source node source location else source node attrs copy node select from pgq node get queue locations where node name dst curs execute self queue name source node rows dst curs fetchall if len rows source location rows 0 node location self log info using as source node source node self register consumer source location', '')] [('handle_match', 36, \"def handle_match(match):\\n    global first\\n    if first:\\n        first = False\\n        return match.group(0)\\n    return ''\\n\", 'handle match match global first if first first false return match group 0 return', '')] [('upgrade', 12, 'def upgrade(migrate_engine):\\n    meta = MetaData(migrate_engine)\\n    sql_update_db = (\\n        \"alter database `changebyus` DEFAULT CHARACTER SET = \\'utf8\\' DEFAULT COLLATE = \\'utf8_general_ci\\'\"\\n        )\\n    migrate_engine.execute(sql_update_db)\\n    meta.reflect(migrate_engine)\\n    tbls = meta.tables.keys()\\n    for item in tbls:\\n        sql_update_table = (\\n            \"alter table `%s` CHARACTER SET \\'utf8\\' COLLATE \\'utf8_general_ci\\'\" %\\n            item)\\n        migrate_engine.execute(sql_update_table)\\n', 'upgrade migrate engine meta metadata migrate engine sql update db alter database changebyus default character set default collate general ci migrate engine execute sql update db meta reflect migrate engine tbls meta tables keys for item in tbls sql update table alter table character set collate general ci item migrate engine execute sql update table', ''), ('downgrade', 32, 'def downgrade(migrate_engine):\\n    meta = MetaData(migrate_engine)\\n    sql_update_db = (\\n        \"alter database `changebyus` DEFAULT CHARACTER SET = \\'latin1\\' DEFAULT COLLATE = \\'latin1_swedish_ci\\'\"\\n        )\\n    migrate_engine.execute(sql_update_db)\\n    meta.reflect(migrate_engine)\\n    tbls = meta.tables.keys()\\n    for item in tbls:\\n        sql_update_table = (\\n            \"alter table `%s` DEFAULT CHARACTER SET \\'latin1\\' DEFAULT COLLATE \\'latin1_swedish_ci\\'\"\\n             % item)\\n        migrate_engine.execute(sql_update_table)\\n', 'downgrade migrate engine meta metadata migrate engine sql update db alter database changebyus default character set default collate swedish ci migrate engine execute sql update db meta reflect migrate engine tbls meta tables keys for item in tbls sql update table alter table default character set default collate swedish ci item migrate engine execute sql update table', '')] [('str', 202, 'def str(ob):\\n    with lowlevel:\\n        return tostring(ob)\\n', 'str ob with lowlevel return tostring ob', ''), ('int', 206, 'def int(ob):\\n    with lowlevel:\\n        return tonumber(ob)\\n', 'int ob with lowlevel return tonumber ob', ''), ('float', 210, 'def float(ob):\\n    with lowlevel:\\n        return tonumber(ob)\\n', 'float ob with lowlevel return tonumber ob', ''), ('len', 214, \"def len(ob):\\n    with lowlevel:\\n        if type(ob) == 'string':\\n            return string.len(ob)\\n        else:\\n            return ob.length\\n\", 'len ob with lowlevel if type ob string return string len ob else return ob length', ''), ('chr', 221, 'def chr(a):\\n    with lowlevel:\\n        return string.char(a)\\n', 'chr with lowlevel return string char', ''), ('ord', 225, 'def ord(a):\\n    with lowlevel:\\n        return string.byte(a)\\n', 'ord with lowlevel return string byte', ''), ('getattr', 229, 'def getattr(ob, name):\\n    with lowlevel:\\n        return ob[name]\\n', 'getattr ob name with lowlevel return ob name', ''), ('isinstance', 233, 'def isinstance(ob, klass):\\n    if ob == None:\\n        return False\\n    elif ob.__class__:\\n        if ob.__class__.__name__ == klass.__name__:\\n            return True\\n        else:\\n            return False\\n    else:\\n        return False\\n', 'isinstance ob klass if ob none return false elif ob class if ob class name klass name return true else return false else return false', ''), ('sum', 244, 'def sum(arr):\\n    a = 0\\n    for b in arr:\\n        a += b\\n    return a\\n', 'sum arr 0 for in arr return', ''), ('range', 405, 'def range(num, stop):\\n    \"\"\"Emulates Python\\'s range function\"\"\"\\n    if stop is not None:\\n        i = num\\n        num = stop\\n    else:\\n        i = 0\\n    arr = []\\n    while i < num:\\n        arr.append(i)\\n        i += 1\\n    return arr\\n', 'range num stop if stop is not none num num stop else 0 arr while num arr append 1 return arr', 'emulates python s range function'), ('__init__', 251, 'def __init__(self, obj, index):\\n    with lowlevel:\\n        self.obj = obj\\n        self.index = index\\n        self.length = string.len(obj)\\n', 'init self obj index with lowlevel self obj obj self index index self length string len obj', ''), ('next', 257, 'def next(self):\\n    with lowlevel:\\n        index = self.index\\n        self.index += 1\\n        return string.sub(self.obj, index + 1, index + 1)\\n', 'next self with lowlevel index self index self index 1 return string sub self obj index 1 index 1', ''), ('__init__', 264, 'def __init__(self, obj, index):\\n    self.obj = obj\\n    self.index = index\\n    self.length = len(obj)\\n', 'init self obj index self obj obj self index index self length len obj', ''), ('next', 269, 'def next(self):\\n    with lowlevel:\\n        index = self.index\\n        self.index += 1\\n        return self.obj[...][index + 1]\\n', 'next self with lowlevel index self index self index 1 return self obj index 1', ''), ('__init__', 281, \"def __init__(self, items, pointer=None, length=0):\\n    with lowlevel:\\n        self.length = length\\n        if type(items) == 'string':\\n            self[...] = string.to_array(items)\\n            self.length = string.len(items)\\n        elif type(items\\n            ) == 'table' and items.__class__ and items.__class__.__name__ == 'list':\\n            print('HIT TABLE!!!')\\n        elif pointer:\\n            self[...] = pointer\\n        else:\\n            self[...] = {}\\n\", 'init self items pointer none length 0 with lowlevel self length length if type items string self string to array items self length string len items elif type items table and items class and items class name list print hit table elif pointer self pointer else self', ''), ('__contains__', 294, 'def __contains__(self, value):\\n    with lowlevel:\\n        for v in self[...]:\\n            if v == value:\\n                return True\\n        return False\\n', 'contains self value with lowlevel for in self if value return true return false', ''), ('__getitem__', 301, 'def __getitem__(self, index):\\n    with lowlevel:\\n        if index < 0:\\n            index = self.length + index\\n        return self[...][index + 1]\\n', 'getitem self index with lowlevel if index 0 index self length index return self index 1', ''), ('__setitem__', 307, 'def __setitem__(self, index, value):\\n    with lowlevel:\\n        if index < 0:\\n            index = self.length + index\\n        self[...][index + 1] = value\\n', 'setitem self index value with lowlevel if index 0 index self length index self index 1 value', ''), ('__getslice__', 313, 'def __getslice__(self, start, stop, step):\\n    if stop == None and step == None:\\n        with lowlevel:\\n            copy = table.shallow_copy(self[...])\\n        return list(pointer=copy, length=self.length)\\n    elif stop < 0:\\n        pass\\n', 'getslice self start stop step if stop none and step none with lowlevel copy table shallow copy self return list pointer copy length self length elif stop 0 pass', ''), ('__iter__', 321, 'def __iter__(self):\\n    return __iterator_list(self, 0)\\n', 'iter self return iterator list self 0', ''), ('__add__', 324, 'def __add__(self, other):\\n    with lowlevel:\\n        ptr = table.shallow_copy(self[...])\\n    copy = list(pointer=ptr, length=self.length)\\n    for item in other:\\n        copy.append(item)\\n    return copy\\n', 'add self other with lowlevel ptr table shallow copy self copy list pointer ptr length self length for item in other copy append item return copy', ''), ('append', 332, 'def append(self, item):\\n    with lowlevel:\\n        self.length += 1\\n        self[...][self.length] = item\\n', 'append self item with lowlevel self length 1 self self length item', ''), ('index', 337, 'def index(self, obj):\\n    with lowlevel:\\n        i = 0\\n        while i < self.length:\\n            if self[...][i + 1] == obj:\\n                return i\\n            i += 1\\n', 'index self obj with lowlevel 0 while self length if self 1 obj return 1', ''), ('__init__', 419, 'def __init__(self, object, pointer=None):\\n    with lowlevel:\\n        self[...] = {}\\n        if pointer:\\n            self[...] = pointer\\n        elif object:\\n            for d in object:\\n                self[...][d.key] = d.value\\n', 'init self object pointer none with lowlevel self if pointer self pointer elif object for in object self key value', ''), ('__getitem__', 429, 'def __getitem__(self, key):\\n    with lowlevel:\\n        return self[...][key]\\n', 'getitem self key with lowlevel return self key', ''), ('__setitem__', 433, 'def __setitem__(self, key, value):\\n    with lowlevel:\\n        self[...][key] = value\\n', 'setitem self key value with lowlevel self key value', ''), ('keys', 437, 'def keys(self):\\n    with lowlevel:\\n        ptr = []\\n        i = 1\\n        for k, v in pairs(self[...]):\\n            ptr[i] = k\\n            i = i + 1\\n    return list(pointer=ptr, length=i - 1)\\n', 'keys self with lowlevel ptr 1 for in pairs self ptr 1 return list pointer ptr length 1', ''), ('__iter__', 446, 'def __iter__(self):\\n    return self.keys().__iter__()\\n', 'iter self return self keys iter', ''), ('items', 449, 'def items(self):\\n    with lowlevel:\\n        ptr = []\\n        i = 1\\n        for k, v in pairs(self[...]):\\n            p = [k, v]\\n            item = list.__call__([], {pointer: p, length: 2})\\n            ptr[i] = item\\n            i = i + 1\\n    return list(pointer=ptr, length=i - 1)\\n', 'items self with lowlevel ptr 1 for in pairs self item list call pointer length 2 ptr item 1 return list pointer ptr length 1', '')] [('get_repos', 29, \"def get_repos(self, pid=None, tid=None):\\n    projects_index = Projects()\\n    if pid:\\n        repos = projects_index.get_projects().get(pid)\\n    elif tid:\\n        repos = projects_index.get_tags().get(tid)\\n    else:\\n        abort(404, detail='A tag ID or project ID must be passed as parameter')\\n    if repos is None:\\n        abort(404, detail='Project ID or Tag ID has not been found')\\n    return repos\\n\", 'get repos self pid none tid none projects index projects if pid repos projects index get projects get pid elif tid repos projects index get tags get tid else abort 404 detail tag id or project id must be passed as parameter if repos is none abort 404 detail project id or tag id has not been found return repos', ''), ('get_projects', 44, \"def get_projects(self):\\n    projects_index = Projects()\\n    projects = projects_index.get_projects()\\n    projects = OrderedDict(sorted(projects.items(), key=lambda t: t[0]))\\n    tags = projects_index.get_tags()\\n    return {'projects': projects, 'tags': tags.keys()}\\n\", 'get projects self projects index projects projects projects index get projects projects ordereddict sorted projects items key lambda 0 tags projects index get tags return projects projects tags tags keys', ''), ('projects', 53, \"@expose('json')\\ndef projects(self):\\n    return self.get_projects()\\n\", 'projects self return self get projects', ''), ('repos', 57, \"@expose('json')\\ndef repos(self, pid=None, tid=None):\\n    return self.get_repos(pid, tid)['repos']\\n\", 'repos self pid none tid none return self get repos pid tid repos', '')] [('test_adding_documents_converts_timestamps_to_utc', 8, \"def test_adding_documents_converts_timestamps_to_utc(self):\\n    stub_document = {'_timestamp': d(2014, 1, 1)}\\n    data = SimpleData([stub_document])\\n    assert_that(data.data(), has_length(1))\\n    assert_that(data.data()[0], has_entry('_timestamp', d_tz(2014, 1, 1)))\\n\", 'test adding documents converts timestamps to utc self stub document timestamp 2014 1 1 data simpledata stub document assert that data data has length 1 assert that data data 0 has entry timestamp tz 2014 1 1', ''), ('test_returned_data_should_be_immutable', 17, 'def test_returned_data_should_be_immutable(self):\\n    stub_doc = {\\'_timestamp\\': d(2014, 1, 1)}\\n    data = SimpleData([stub_doc])\\n    another_data = data.data()\\n    try:\\n        another_data.append({\\'even_more_nonsense\\': True})\\n        assert_that(False, \\'expected an exception\\')\\n    except AttributeError as e:\\n        assert_that(str(e), \"\\'tuple\\' object has no attribute append\")\\n', 'test returned data should be immutable self stub doc timestamp 2014 1 1 data simpledata stub doc another data data data try another data append even more nonsense true assert that false expected an exception except attributeerror as assert that str tuple object has no attribute append', '')] [('findBetween', 17, \"def findBetween(s, first, last):\\n    try:\\n        start = s.index(first) + len(first)\\n        end = s.index(last, start)\\n        return s[start:end]\\n    except ValueError:\\n        return ''\\n\", 'findbetween first last try start index first len first end index last start return start end except valueerror return', ''), ('drawCenteredText', 26, \"def drawCenteredText(startY, text, draw, fnt, panelSize):\\n    MAX_W, MAX_H = panelSize[0], panelSize[1]\\n    current_h, pad = startY, 10\\n    if text is not None:\\n        para = textwrap.wrap(text, width=12)\\n        print('para:')\\n        pprint(para)\\n        for line in para:\\n            w, h = draw.textsize(line, font=fnt)\\n            draw.text(((MAX_W - w) / 2, current_h), line, font=fnt, fill=(0,\\n                0, 0, 255))\\n            current_h += h + pad\\n    return current_h\\n\", 'drawcenteredtext starty text draw fnt panelsize max max panelsize 0 panelsize 1 current pad starty 10 if text is not none para textwrap wrap text width 12 print para pprint para for line in para draw textsize line font fnt draw text max 2 current line font fnt fill 0 0 0 255 current pad return current', ''), ('isCorrectOrder', 46, \"def isCorrectOrder(txtLine1, txtLine2, nameorder):\\n    print('comparing nameorder ' + str(nameorder) + ' ' + txtLine2['name'])\\n    for name in nameorder:\\n        if name == txtLine2['name']:\\n            return False\\n        if name == txtLine1['name']:\\n            return True\\n    return True\\n\", 'iscorrectorder nameorder print comparing nameorder str nameorder name for name in nameorder if name name return false if name name return true return true', ''), ('pickNestedFile', 58, 'def pickNestedFile(directory, bad_files, seed=None):\\n    if seed is not None:\\n        random.seed(seed)\\n    file = None\\n    while file is None or file in bad_files:\\n        file = random.choice(os.listdir(directory))\\n    print(\\'Trying \\' + file)\\n    if os.path.isdir(os.path.join(directory, file)) == True:\\n        print(\"It\\'s a directory!\")\\n        return pickNestedFile(directory + \\'/\\' + file, bad_files)\\n    else:\\n        return directory + \\'/\\' + file\\n', 'picknestedfile directory bad files seed none if seed is not none random seed seed file none while file is none or file in bad files file random choice os listdir directory print trying file if os path isdir os path join directory file true print it directory return picknestedfile directory file bad files else return directory file', ''), ('imageFlip', 73, 'def imageFlip(image):\\n    tr = getTransform()\\n    image = image.transpose(tr)\\n    tr = getTransform(20)\\n    if tr is None:\\n        return image\\n    else:\\n        return image.transpose(tr)\\n', 'imageflip image tr gettransform image image transpose tr tr gettransform 20 if tr is none return image else return image transpose tr', ''), ('rollOdds', 83, 'def rollOdds(n, seed=None):\\n    if seed is not None:\\n        random.seed(seed)\\n    n = int(n)\\n    if n < 1:\\n        return false\\n    return random.randint(0, n - 1) == 0\\n', 'rollodds seed none if seed is not none random seed seed int if 1 return false return random randint 0 1 0', ''), ('rollFraction', 92, 'def rollFraction(odds, seed=None):\\n    if seed is not None:\\n        random.seed(seed)\\n    if odds > 1:\\n        return random.random() < 1.0 / float(odds)\\n    else:\\n        return random.random() < odds\\n', 'rollfraction odds seed none if seed is not none random seed seed if odds 1 return random random 1 0 float odds else return random random odds', ''), ('getTransformList', 102, 'def getTransformList(length, nullWeight=10):\\n    list = []\\n    for i in (1, length):\\n        list.append(getTransform(nullWeight))\\n    return list\\n', 'gettransformlist length nullweight 10 list for in 1 length list append gettransform nullweight return list', ''), ('applyTransformList', 109, 'def applyTransformList(list, image):\\n    for transformation in list:\\n        if transformation is not None:\\n            image = image.transpose(Image.ROTATE_180)\\n    return image\\n', 'applytransformlist list image for transformation in list if transformation is not none image image transpose image rotate 180 return image', ''), ('possiblyTransform', 117, 'def possiblyTransform(image, odds, length=2):\\n    if rollOdds(odds):\\n        return applyTransformList(getTransformList(length), image)\\n    else:\\n        return image\\n', 'possiblytransform image odds length 2 if rollodds odds return applytransformlist gettransformlist length image else return image', ''), ('undoTransformList', 126, 'def undoTransformList(list, image):\\n    undoList = []\\n    for transformation in list:\\n        if transformation is not None:\\n            undoList.insert(0, undoTransform_D[transformation])\\n    return applyTransformList(undoList, image)\\n', 'undotransformlist list image undolist for transformation in list if transformation is not none undolist insert 0 undotransform transformation return applytransformlist undolist image', ''), ('getTransform', 135, 'def getTransform(allowNothing=None):\\n    return weightedDictPick(transform_D, int(allowNothing))\\n', 'gettransform allownothing none return weighteddictpick transform int allownothing', ''), ('weightedDictPick', 140, 'def weightedDictPick(weightedDict, increasedNoneWeight=0, seed=None):\\n    if seed is not None:\\n        random.seed(seed)\\n    return weightedDict.get(random.randint(1, len(list(weightedDict.keys())\\n        ) + increasedNoneWeight) - 1, None)\\n', 'weighteddictpick weighteddict increasednoneweight 0 seed none if seed is not none random seed seed return weighteddict get random randint 1 len list weighteddict keys increasednoneweight 1 none', ''), ('genTransformDict', 158, \"def genTransformDict(flip=10, rotate=20):\\n    undoTransform_D = {'Image.FLIP_LEFT_RIGHT': 'Image.FLIP_LEFT_RIGHT',\\n        'Image.FLIP_TOP_BOTTOM': 'Image.FLIP_TOP_BOTTOM',\\n        'Image.ROTATE_180': 'Image.ROTATE_180', 'Image.ROTATE_90':\\n        'Image.ROTATE_270', 'Image.ROTATE_270': 'Image.ROTATE_90'}\\n    transform_D = {}\\n    genProbabilityDict({'Image.FLIP_LEFT_RIGHT': flip,\\n        'Image.FLIP_TOP_BOTTOM': flip, 'Image.ROTATE_90': rotate,\\n        'Image.ROTATE_180': rotate, 'Image.ROTATE_270': rotate}, transform_D, 0\\n        )\\n    return transform_D, undoTransform_D\\n\", 'gentransformdict flip 10 rotate 20 undotransform image flip left right image flip left right image flip top bottom image flip top bottom image rotate 180 image rotate 180 image rotate 90 image rotate 270 image rotate 270 image rotate 90 transform genprobabilitydict image flip left right flip image flip top bottom flip image rotate 90 rotate image rotate 180 rotate image rotate 270 rotate transform 0 return transform undotransform', ''), ('setPanelSizes', 180, 'def setPanelSizes(ps, closeupMultiplier):\\n    panelSize = ps\\n    charHeight = 3 * panelSize[1] / 7\\n    charHeightCloseup = int(charHeight * closeupMultiplier)\\n    smallCharHeight = int(charHeight / closeupMultiplier)\\n    return charHeight, charHeightCloseup, smallCharHeight\\n', 'setpanelsizes ps closeupmultiplier panelsize ps charheight 3 panelsize 1 7 charheightcloseup int charheight closeupmultiplier smallcharheight int charheight closeupmultiplier return charheight charheightcloseup smallcharheight', ''), ('insertLineBreaks', 188, \"def insertLineBreaks(text, maxCharsPerLine):\\n    words = text.split(' ')\\n    newstr = ''\\n    currentCharCount = 0\\n    for word in words:\\n        if currentCharCount + len(word) > maxCharsPerLine:\\n            newstr += '\\\\n'\\n            currentCharCount = 0\\n        else:\\n            newstr += ' '\\n        currentCharCount += len(word) + 1\\n        newstr += word\\n    newstr = newstr.strip()\\n    return newstr\\n\", 'insertlinebreaks text maxcharsperline words text split newstr currentcharcount 0 for word in words if currentcharcount len word maxcharsperline newstr currentcharcount 0 else newstr currentcharcount len word 1 newstr word newstr newstr strip return newstr', ''), ('circle', 204, 'def circle(draw, center, radius):\\n    draw.ellipse((center[0] - radius + 1, center[1] - radius + 1, center[0] +\\n        radius - 1, center[1] + radius - 1), fill=(255, 255, 255), outline=None\\n        )\\n', 'circle draw center radius draw ellipse center 0 radius 1 center 1 radius 1 center 0 radius 1 center 1 radius 1 fill 255 255 255 outline none', ''), ('triangularInt', 219, 'def triangularInt(low, high, mode, seed=None):\\n    if seed is not None:\\n        random.seed(seed)\\n    return int(random.triangular(low, high, mode))\\n', 'triangularint low high mode seed none if seed is not none random seed seed return int random triangular low high mode', ''), ('genProbabilityDict', 227, 'def genProbabilityDict(probabilityTable, outputDict=None, noneWeight=0):\\n    if outputDict is None:\\n        outputDict = {}\\n    counter = 0\\n    for entry in probabilityTable:\\n        weight = int(probabilityTable[entry])\\n        for i in range(counter, counter + weight):\\n            outputDict[i] = entry\\n        counter += weight\\n    for i in range(counter, counter + noneWeight):\\n        outuptDict[i] = None\\n    return outputDict\\n', 'genprobabilitydict probabilitytable outputdict none noneweight 0 if outputdict is none outputdict counter 0 for entry in probabilitytable weight int probabilitytable entry for in range counter counter weight outputdict entry counter weight for in range counter counter noneweight outuptdict none return outputdict', ''), ('decomposeNumericSwitchList', 247, 'def decomposeNumericSwitchList(number, base, omitZero=True):\\n    list = {}\\n    maxpower = int(math.log(number, base))\\n    for power in range(maxpower, -1, -1):\\n        component = base ** power\\n        coefficient = number / component\\n        if coefficient != 0 or omitZero is False:\\n            list[power] = coefficient\\n        number %= component\\n    return list\\n', 'decomposenumericswitchlist number base omitzero true list maxpower int math log number base for power in range maxpower 1 1 component base power coefficient number component if coefficient 0 or omitzero is false list power coefficient number component return list', ''), ('decomposeBinarySwitches', 260, 'def decomposeBinarySwitches(number):\\n    return uniqueSumOfPowersList(number, 2)\\n', 'decomposebinaryswitches number return uniquesumofpowerslist number 2', ''), ('decomposeNumericComponents', 264, 'def decomposeNumericComponents(number, base):\\n    componentList = {}\\n    exponentList = decomposeNumericSwitchList(number, base, True)\\n    for exponent in list(exponentList.keys()):\\n        componentList[base ** exponent] = exponentList[exponent]\\n    return componentList\\n', 'decomposenumericcomponents number base componentlist exponentlist decomposenumericswitchlist number base true for exponent in list exponentlist keys componentlist base exponent exponentlist exponent return componentlist', ''), ('uniqueSumOfPowersList', 275, 'def uniqueSumOfPowersList(number, base):\\n    complist = []\\n    components = decomposeNumericComponents(number, base)\\n    for component in list(components.keys()):\\n        complist.append(component * components[component])\\n    return complist\\n', 'uniquesumofpowerslist number base complist components decomposenumericcomponents number base for component in list components keys complist append component components component return complist', ''), ('quitline', 283, \"def quitline(line):\\n    quitmessage = ['(Quit:', 'has joined (', 'has left IRC (',\\n        'has changed mode:', 'You have joined', 'set the topic']\\n    for msg in quitmessage:\\n        if msg in line:\\n            return True\\n    return False\\n\", 'quitline line quitmessage quit has joined has left irc has changed mode you have joined set the topic for msg in quitmessage if msg in line return true return false', ''), ('soloURL', 298, \"def soloURL(line):\\n    webendings = ['com', 'net', 'org', 'bat', 'gif', 'png', 'jpg', 'bmp',\\n        'htm', 'asp', 'gov', 'mil', 'mp4', 'mp3', 'm4v', 'mkv', '.uk',\\n        '.co', '.tk', '.pw', '.es', '.us', '.py', '.pl', '.nl', '.ru',\\n        '.fr', '.ca']\\n    if len(line.split(' ')) > 1:\\n        return False\\n    if len(line) < 11:\\n        return False\\n    if line[:4].lower() == 'http':\\n        return True\\n    if line[-3:].lower() in webendings:\\n        return True\\n    return True\\n\", 'solourl line webendings com net org bat gif png jpg bmp htm asp gov mil mkv uk co tk pw es us py pl nl ru fr ca if len line split 1 return false if len line 11 return false if line 4 lower http return true if line 3 lower in webendings return true return true', ''), ('pickfileIndex', 341, \"def pickfileIndex(inputfolder, bad_files, seed=None):\\n    if seed is not None:\\n        random.seed(seed)\\n    file = None\\n    directoryList = os.listdir(inputfolder)\\n    while file is None or file in bad_files:\\n        location = random.randint(0, len(directoryList) - 1)\\n        file = directoryList[location]\\n    if os.path.isdir(os.path.join(inputfolder, file)) is True:\\n        return pickfileIndex(inputfolder + '/' + file, bad_files)\\n    else:\\n        return inputfolder, directoryList, location\\n\", 'pickfileindex inputfolder bad files seed none if seed is not none random seed seed file none directorylist os listdir inputfolder while file is none or file in bad files location random randint 0 len directorylist 1 file directorylist location if os path isdir os path join inputfolder file is true return pickfileindex inputfolder file bad files else return inputfolder directorylist location', ''), ('rollColor', 356, 'def rollColor(avgR, avgG, avgB, darkness):\\n    return triangularInt(0, 256, avgR), triangularInt(0, 256, avgG\\n        ), triangularInt(0, 256, avgB), triangularInt(0, 256, darkness)\\n', 'rollcolor avgr avgg avgb darkness return triangularint 0 256 avgr triangularint 0 256 avgg triangularint 0 256 avgb triangularint 0 256 darkness', ''), ('anonWord', 368, 'def anonWord(wordIn, nameList, joiner=\\'\\', recheck=True, debugprint=False):\\n    masterNameList = {}\\n    if debugprint is True:\\n        print(\\'considering: \\' + wordIn)\\n    if len(wordIn) < 4:\\n        return wordIn\\n    if recheck is False:\\n        if nameList.get(wordIn.lower(), None) is not None:\\n            print(\\'removing a name \\' + wordIn)\\n            return nameList[wordIn.lower()][1:]\\n        return wordIn\\n    if joiner != \\'\\':\\n        print(\"Splitting into words delimited by \\'\" + joiner + \"\\'\")\\n        parts = wordIn.split(joiner)\\n        recheck = True\\n    else:\\n        parts = re.findall(\\'\\\\\\\\w+|[^\\\\\\\\w\\\\\\\\s]\\', wordIn, re.UNICODE)\\n        recheck = False\\n    newparts = []\\n    for part in parts:\\n        newparts.append(anonWord(part, nameList, recheck=recheck))\\n    return joiner.join(newparts)\\n', 'anonword wordin namelist joiner recheck true debugprint false masternamelist if debugprint is true print considering wordin if len wordin 4 return wordin if recheck is false if namelist get wordin lower none is not none print removing name wordin return namelist wordin lower 1 return wordin if joiner print splitting into words delimited by joiner parts wordin split joiner recheck true else parts re findall wordin re unicode recheck false newparts for part in parts newparts append anonword part namelist recheck recheck return joiner join newparts', ''), ('cleanupline', 398, \"def cleanupline(linein, namelist, ignored_users=[], params={'bot': False,\\n    'debug': False}):\\n    if params['bot'] is True and '> ~' in linein:\\n        return None\\n    linein = linein.strip()\\n    linein = linein.strip('\\\\n')\\n    if params['debug'] is True:\\n        print('line: ' + linein)\\n    isme = False\\n    if linein[:2] == '* ' and quitline(linein) is False:\\n        isme = True\\n        linein = linein[2:]\\n        linein = '<' + linein[:linein.index(' ')] + '> *' + linein[linein.\\n            index(' ') + 1:] + '*'\\n        if params['debug'] is True:\\n            print('new /me linein ' + linein)\\n    if linein.count('<') < 1 or linein.count('>') < 1:\\n        print('Found malformed line!')\\n        return None\\n    name = findBetween(linein, '<', '>').lower()\\n    if name not in namelist:\\n        namelist.append(name)\\n    if name.lower() in ignored_users:\\n        print('Skipping ignored user ' + name)\\n        return None\\n    return {'name': name, 'text': linein[linein.index('>') + 2:], 'meline':\\n        isme}\\n\", 'cleanupline linein namelist ignored users params bot false debug false if params bot is true and in linein return none linein linein strip linein linein strip if params debug is true print line linein isme false if linein 2 and quitline linein is false isme true linein linein 2 linein linein linein index linein linein index 1 if params debug is true print new me linein linein if linein count 1 or linein count 1 print found malformed line return none name findbetween linein lower if name not in namelist namelist append name if name lower in ignored users print skipping ignored user name return none return name name text linein linein index 2 meline isme', '')] [] [('__init__', 32, 'def __init__(self, args=None, **kwargs):\\n    description = \"\"\"Takes two dictionary and compares them semantically (by looking at the\\nstructure, not the textual representation. If the dictionaries do not\\nhave the same name, it looks for the destination file by searching the\\nequivalent place in the destination case. If more than two files are\\nspecified then the last name is assumed to be a directory and all the\\nequivalents to the other files are searched there.\\n        \"\"\"\\n    PyFoamApplication.__init__(self, args=args, description=description,\\n        usage=\\'%prog [options] <source> <destination-case>\\', nr=2, exactNr=\\n        False, interspersed=True, **kwargs)\\n', 'init self args none kwargs description takes two dictionary and compares them semantically by looking at the structure not the textual representation if the dictionaries do not have the same name it looks for the destination file by searching the equivalent place in the destination case if more than two files are specified then the last name is assumed to be directory and all the equivalents to the other files are searched there pyfoamapplication init self args args description description usage prog options source destination case nr 2 exactnr false interspersed true kwargs', ''), ('addOptions', 53, 'def addOptions(self):\\n    self.parser.add_option(\\'--not-equal\\', action=\\'store_true\\', default=\\n        False, dest=\\'notequal\\', help=\\n        \\'Allow source and destination to have different names\\')\\n    self.parser.add_option(\\'--debug\\', action=\\'store_true\\', default=False,\\n        dest=\\'debug\\', help=\\'Debug the comparing process\\')\\n    self.parser.add_option(\\'--long-field-threshold\\', action=\\'store\\', type=\\n        \\'int\\', default=None, dest=\\'longlist\\', help=\\n        \"Fields that are longer than this won\\'t be parsed, but read into memory (and compared as strings)\"\\n        )\\n    CommonParserOptions.addOptions(self)\\n', 'addoptions self self parser add option not equal action store true default false dest notequal help allow source and destination to have different names self parser add option debug action store true default false dest debug help debug the comparing process self parser add option long field threshold action store type int default none dest longlist help fields that are longer than this won be parsed but read into memory and compared as strings commonparseroptions addoptions self', ''), ('run', 75, \"def run(self):\\n    sFiles = self.parser.getArgs()[0:-1]\\n    dFile = path.abspath(self.parser.getArgs()[-1])\\n    for s in sFiles:\\n        sName = path.abspath(s)\\n        dName = dFile\\n        if len(s) > 1:\\n            print_(f.name + 'Source file', sName, f.reset)\\n        try:\\n            source = ParsedParameterFile(sName, backup=False, debug=self.\\n                opts.debugParser, listLengthUnparsed=self.opts.longlist,\\n                noBody=self.opts.noBody, noHeader=self.opts.noHeader,\\n                boundaryDict=self.opts.boundaryDict, listDict=self.opts.\\n                listDict, listDictWithHeader=self.opts.listDictWithHeader)\\n        except IOError:\\n            e = sys.exc_info()[1]\\n            self.warning('Problem with file', sName, ':', e)\\n            continue\\n        except PyFoamParserError:\\n            e = sys.exc_info()[1]\\n            self.warning('Parser problem with', sName, ':', e)\\n            continue\\n        found = False\\n        if path.isfile(sName) and path.isfile(dName):\\n            found = True\\n        if not found and not self.opts.notequal and path.basename(sName\\n            ) != path.basename(dName):\\n            parts = sName.split(path.sep)\\n            for i in range(len(parts)):\\n                tmp = path.join(*([dName] + parts[-(i + 1):]))\\n                if path.exists(tmp):\\n                    found = True\\n                    dName = tmp\\n                    warning('Found', dName, 'and using this')\\n                    break\\n            if not found:\\n                error('Could not find a file named', path.basename(sName),\\n                    'in', dName)\\n        if path.samefile(sName, dName):\\n            error('Source', sName, 'and destination', dName, 'are the same')\\n        try:\\n            dest = ParsedParameterFile(dName, backup=False, debug=self.opts\\n                .debugParser, listLengthUnparsed=self.opts.longlist, noBody\\n                =self.opts.noBody, noHeader=self.opts.noHeader,\\n                boundaryDict=self.opts.boundaryDict, listDict=self.opts.\\n                listDict, listDictWithHeader=self.opts.listDictWithHeader)\\n        except IOError:\\n            e = sys.exc_info()[1]\\n            self.error('Problem with file', dName, ':', e)\\n        self.pling = False\\n        if (not self.opts.boundaryDict and not self.opts.listDict and not\\n            self.opts.listDictWithHeader):\\n            self.compareDict(source.content, dest.content, 1, path.basename\\n                (sName))\\n        else:\\n            self.compareIterable(source.content, dest.content, 1, path.\\n                basename(sName))\\n        if not self.pling:\\n            print_('\\\\nNo differences found')\\n\", 'run self sfiles self parser getargs 0 1 dfile path abspath self parser getargs 1 for in sfiles sname path abspath dname dfile if len 1 print name source file sname reset try source parsedparameterfile sname backup false debug self opts debugparser listlengthunparsed self opts longlist nobody self opts nobody noheader self opts noheader boundarydict self opts boundarydict listdict self opts listdict listdictwithheader self opts listdictwithheader except ioerror sys exc info 1 self warning problem with file sname continue except pyfoamparsererror sys exc info 1 self warning parser problem with sname continue found false if path isfile sname and path isfile dname found true if not found and not self opts notequal and path basename sname path basename dname parts sname split path sep for in range len parts tmp path join dname parts 1 if path exists tmp found true dname tmp warning found dname and using this break if not found error could not find file named path basename sname in dname if path samefile sname dname error source sname and destination dname are the same try dest parsedparameterfile dname backup false debug self opts debugparser listlengthunparsed self opts longlist nobody self opts nobody noheader self opts noheader boundarydict self opts boundarydict listdict self opts listdict listdictwithheader self opts listdictwithheader except ioerror sys exc info 1 self error problem with file dname self pling false if not self opts boundarydict and not self opts listdict and not self opts listdictwithheader self comparedict source content dest content 1 path basename sname else self compareiterable source content dest content 1 path basename sname if not self pling print nno differences found', ''), ('dictString', 150, \"def dictString(self, path, name):\\n    return '%s[%s]' % (path, name)\\n\", 'dictstring self path name return path name', ''), ('iterString', 153, \"def iterString(self, path, index):\\n    return '%s[%d]' % (path, index)\\n\", 'iterstring self path index return path index', ''), ('compare', 156, \"def compare(self, src, dst, depth, name):\\n    if type(src) != type(dst):\\n        print_(f.diff + '>><<', name, ': Types differ' + f.reset + '\\\\n+' +\\n            f.src + '>>Source:' + f.reset + '\\\\n', makeString(src), '\\\\n' + f\\n            .dst + '<<Destination:' + f.reset + '\\\\n', makeString(dst) + f.reset\\n            )\\n        self.pling = True\\n    elif type(src) in [tuple, list, TupleProxy]:\\n        self.compareIterable(src, dst, depth, name)\\n    elif isinstance(src, (str, float) + integer_types) or src == None:\\n        self.comparePrimitive(src, dst, depth, name)\\n    elif src.__class__ in [Dimension, Tensor, SymmTensor, Vector]:\\n        self.comparePrimitive(src, dst, depth, name)\\n    elif src.__class__ == Field:\\n        self.compareField(src, dst, depth, name)\\n    elif type(src) in [DictProxy, dict]:\\n        self.compareDict(src, dst, depth, name)\\n    else:\\n        warning('Type of', name, '=', type(src), 'unknown')\\n        if self.opts.debug:\\n            try:\\n                print_('Class of', name, '=', src.__class__, 'unknown')\\n            except:\\n                pass\\n\", 'compare self src dst depth name if type src type dst print diff name types differ reset src source reset makestring src dst destination reset makestring dst reset self pling true elif type src in tuple list tupleproxy self compareiterable src dst depth name elif isinstance src str float integer types or src none self compareprimitive src dst depth name elif src class in dimension tensor symmtensor vector self compareprimitive src dst depth name elif src class field self comparefield src dst depth name elif type src in dictproxy dict self comparedict src dst depth name else warning type of name type src unknown if self opts debug try print class of name src class unknown except pass', ''), ('compareField', 178, \"def compareField(self, src, dst, depth, name):\\n    if src != dst:\\n        self.pling = True\\n        print_(f.diff + '>><< Field', name, ': Differs' + f.reset + '\\\\n' +\\n            f.src + '>>Source:' + f.reset + '\\\\n', end=' ')\\n        if src.uniform:\\n            print_(src)\\n        else:\\n            print_('nonuniform - field not printed')\\n        print_(f.dst + '<<Destination:' + f.reset + '\\\\n', end=' ')\\n        if dst.uniform:\\n            print_(dst)\\n        else:\\n            print_('nonuniform - field not printed')\\n\", 'comparefield self src dst depth name if src dst self pling true print diff field name differs reset src source reset end if src uniform print src else print nonuniform field not printed print dst destination reset end if dst uniform print dst else print nonuniform field not printed', ''), ('comparePrimitive', 192, \"def comparePrimitive(self, src, dst, depth, name):\\n    if src != dst:\\n        print_(f.diff + '>><<', name, ': Differs' + f.reset + '\\\\n' + f.src +\\n            '>>Source:' + f.reset + '\\\\n', src, '\\\\n' + f.dst +\\n            '<<Destination:' + f.reset + '\\\\n', dst)\\n        self.pling = True\\n\", 'compareprimitive self src dst depth name if src dst print diff name differs reset src source reset src dst destination reset dst self pling true', ''), ('compareIterable', 197, 'def compareIterable(self, src, dst, depth, name):\\n    nr = min(len(src), len(dst))\\n    for i in range(nr):\\n        if self.opts.debug:\\n            print_(\\'Comparing\\', self.iterString(name, i))\\n        self.compare(src[i], dst[i], depth + 1, self.iterString(name, i))\\n    if nr < len(src):\\n        print_(f.src + \\'>>>>\\', self.iterString(name, nr), \\'to\\', self.\\n            iterString(name, len(src) - 1), \"\"\"missing from destination\\n\"\"\" +\\n            f.reset, makeString(src[nr:]))\\n        self.pling = True\\n    elif nr < len(dst):\\n        print_(f.dst + \\'<<<<\\', self.iterString(name, nr), \\'to\\', self.\\n            iterString(name, len(dst) - 1), \"\"\"missing from source\\n\"\"\" + f.\\n            reset, makeString(dst[nr:]))\\n        self.pling = True\\n', 'compareiterable self src dst depth name nr min len src len dst for in range nr if self opts debug print comparing self iterstring name self compare src dst depth 1 self iterstring name if nr len src print src self iterstring name nr to self iterstring name len src 1 missing from destination reset makestring src nr self pling true elif nr len dst print dst self iterstring name nr to self iterstring name len dst 1 missing from source reset makestring dst nr self pling true', ''), ('compareDict', 212, \"def compareDict(self, src, dst, depth, name):\\n    for n in src:\\n        if not n in dst:\\n            print_(f.src + '>>>>', self.dictString(name, n), \\n                ': Missing from destination\\\\n' + f.reset, makeString(src[n]))\\n            self.pling = True\\n        else:\\n            if self.opts.debug:\\n                print_('Comparing', self.dictString(name, n))\\n            self.compare(src[n], dst[n], depth + 1, self.dictString(name, n))\\n    for n in dst:\\n        if not n in src:\\n            print_(f.dst + '<<<<', self.dictString(name, n), \\n                ': Missing from source\\\\n' + f.reset, makeString(dst[n]))\\n            self.pling = True\\n\", 'comparedict self src dst depth name for in src if not in dst print src self dictstring name missing from destination reset makestring src self pling true else if self opts debug print comparing self dictstring name self compare src dst depth 1 self dictstring name for in dst if not in src print dst self dictstring name missing from source reset makestring dst self pling true', '')] [('_pickRule', 140, 'def _pickRule(tree, name):\\n    rules = tree.findall(\\'rule\\')\\n    elements = []\\n    for r in rules:\\n        if r.get(\\'name\\') == name:\\n            elements.append(r)\\n    if len(elements) == 0:\\n        print(\"Error, no rules found with name \\'%s\\'\" % name)\\n        quit()\\n    sum, tuples = 0, []\\n    for e in elements:\\n        weight = int(e.get(\\'weight\\', 1))\\n        sum = sum + weight\\n        tuples.append((e, weight))\\n    n = random.randint(0, sum - 1)\\n    for item, weight in tuples:\\n        if n < weight:\\n            break\\n        n = n - weight\\n    return item\\n', 'pickrule tree name rules tree findall rule elements for in rules if get name name elements append if len elements 0 print error no rules found with name name quit sum tuples 0 for in elements weight int get weight 1 sum sum weight tuples append weight random randint 0 sum 1 for item weight in tuples if weight break weight return item', ''), ('_parseXform', 166, 'def _parseXform(xform_string):\\n    if xform_string in _xformCache:\\n        return _xformCache[xform_string]\\n    matrix = mu.Matrix.Identity(4)\\n    tokens = xform_string.split(\\' \\')\\n    t = 0\\n    while t < len(tokens) - 1:\\n        command, t = tokens[t], t + 1\\n        if command == \\'tx\\':\\n            x, t = eval(tokens[t]), t + 1\\n            matrix *= mu.Matrix.Translation(mu.Vector((x, 0, 0)))\\n        elif command == \\'ty\\':\\n            y, t = eval(tokens[t]), t + 1\\n            matrix *= mu.Matrix.Translation(mu.Vector((0, y, 0)))\\n        elif command == \\'tz\\':\\n            z, t = eval(tokens[t]), t + 1\\n            matrix *= mu.Matrix.Translation(mu.Vector((0, 0, z)))\\n        elif command == \\'t\\':\\n            x, t = eval(tokens[t]), t + 1\\n            y, t = eval(tokens[t]), t + 1\\n            z, t = eval(tokens[t]), t + 1\\n            matrix *= mu.Matrix.Translation(mu.Vector((x, y, z)))\\n        elif command == \\'rx\\':\\n            theta, t = _radians(eval(tokens[t])), t + 1\\n            matrix *= mu.Matrix.Rotation(theta, 4, \\'X\\')\\n        elif command == \\'ry\\':\\n            theta, t = _radians(eval(tokens[t])), t + 1\\n            matrix *= mu.Matrix.Rotation(theta, 4, \\'Y\\')\\n        elif command == \\'rz\\':\\n            theta, t = _radians(eval(tokens[t])), t + 1\\n            matrix *= mu.Matrix.Rotation(theta, 4, \\'Z\\')\\n        elif command == \\'sx\\':\\n            x, t = eval(tokens[t]), t + 1\\n            matrix *= mu.Matrix.Scale(x, 4, mu.Vector((1.0, 0.0, 0.0)))\\n        elif command == \\'sy\\':\\n            y, t = eval(tokens[t]), t + 1\\n            matrix *= mu.Matrix.Scale(y, 4, mu.Vector((0.0, 1.0, 0.0)))\\n        elif command == \\'sz\\':\\n            z, t = eval(tokens[t]), t + 1\\n            matrix *= mu.Matrix.Scale(z, 4, mu.Vector((0.0, 0.0, 1.0)))\\n        elif command == \\'sa\\':\\n            v, t = eval(tokens[t]), t + 1\\n            matrix *= mu.Matrix.Scale(v, 4)\\n        elif command == \\'s\\':\\n            x, t = eval(tokens[t]), t + 1\\n            y, t = eval(tokens[t]), t + 1\\n            z, t = eval(tokens[t]), t + 1\\n            mx = mu.Matrix.Scale(x, 4, mu.Vector((1.0, 0.0, 0.0)))\\n            my = mu.Matrix.Scale(y, 4, mu.Vector((0.0, 1.0, 0.0)))\\n            mz = mu.Matrix.Scale(z, 4, mu.Vector((0.0, 0.0, 1.0)))\\n            mxyz = mx * my * mz\\n            matrix *= mxyz\\n        else:\\n            print(\\n                \"unrecognized transformation: \\'%s\\' at position %d in \\'%s\\'\" %\\n                (command, t, xform_string))\\n            quit()\\n    _xformCache[xform_string] = matrix\\n    return matrix\\n', 'parsexform xform string if xform string in xformcache return xformcache xform string matrix mu matrix identity 4 tokens xform string split 0 while len tokens 1 command tokens 1 if command tx eval tokens 1 matrix mu matrix translation mu vector 0 0 elif command ty eval tokens 1 matrix mu matrix translation mu vector 0 0 elif command tz eval tokens 1 matrix mu matrix translation mu vector 0 0 elif command eval tokens 1 eval tokens 1 eval tokens 1 matrix mu matrix translation mu vector elif command rx theta radians eval tokens 1 matrix mu matrix rotation theta 4 elif command ry theta radians eval tokens 1 matrix mu matrix rotation theta 4 elif command rz theta radians eval tokens 1 matrix mu matrix rotation theta 4 elif command sx eval tokens 1 matrix mu matrix scale 4 mu vector 1 0 0 0 0 0 elif command sy eval tokens 1 matrix mu matrix scale 4 mu vector 0 0 1 0 0 0 elif command sz eval tokens 1 matrix mu matrix scale 4 mu vector 0 0 0 0 1 0 elif command sa eval tokens 1 matrix mu matrix scale 4 elif command eval tokens 1 eval tokens 1 eval tokens 1 mx mu matrix scale 4 mu vector 1 0 0 0 0 0 my mu matrix scale 4 mu vector 0 0 1 0 0 0 mz mu matrix scale 4 mu vector 0 0 0 0 1 0 mxyz mx my mz matrix mxyz else print unrecognized transformation at position in command xform string quit xformcache xform string matrix return matrix', ''), ('_radians', 234, 'def _radians(d):\\n    return float(d * 3.141 / 180.0)\\n', 'radians return float 3 141 180 0', ''), ('__init__', 12, \"def __init__(self, rules, maxObjects):\\n    self._tree = fromstring(rules)\\n    self._maxDepth = int(self._tree.get('max_depth'))\\n    self._progressCount = 0\\n    self._maxObjects = maxObjects\\n\", 'init self rules maxobjects self tree fromstring rules self maxdepth int self tree get max depth self progresscount 0 self maxobjects maxobjects', ''), ('evaluate', 22, \"def evaluate(self, seed=0):\\n    random.seed(seed)\\n    rule = _pickRule(self._tree, 'entry')\\n    entry = rule, 0, mu.Matrix.Identity(4)\\n    shapes = self._evaluate(entry)\\n    return shapes\\n\", 'evaluate self seed 0 random seed seed rule pickrule self tree entry entry rule 0 mu matrix identity 4 shapes self evaluate entry return shapes', ''), ('_evaluate', 29, \"def _evaluate(self, entry):\\n    stack = [entry]\\n    shapes = []\\n    while len(stack) > 0:\\n        if len(shapes) > self._maxObjects:\\n            print('max objects reached')\\n            break\\n        if len(shapes) > self._progressCount + 1000:\\n            print(len(shapes), 'curve segments so far')\\n            print(self._maxObjects)\\n            self._progressCount = len(shapes)\\n        rule, depth, matrix = stack.pop()\\n        local_max_depth = self._maxDepth\\n        if 'max_depth' in rule.attrib:\\n            local_max_depth = int(rule.get('max_depth'))\\n        if len(stack) >= self._maxDepth:\\n            shapes.append(None)\\n            continue\\n        if depth >= local_max_depth:\\n            if 'successor' in rule.attrib:\\n                successor = rule.get('successor')\\n                rule = _pickRule(self._tree, successor)\\n                stack.append((rule, 0, matrix))\\n            shapes.append(None)\\n            continue\\n        for statement in rule:\\n            tstr = statement.get('transforms', '')\\n            if not tstr:\\n                tstr = ''\\n                for t in ['tx', 'ty', 'tz', 'rx', 'ry', 'rz', 'sa', 'sx',\\n                    'sy', 'sz']:\\n                    tvalue = statement.get(t)\\n                    if tvalue:\\n                        n = eval(tvalue)\\n                        tstr += '{} {:f} '.format(t, n)\\n            xform = _parseXform(tstr)\\n            count = int(statement.get('count', 1))\\n            for n in range(count):\\n                matrix *= xform\\n                if statement.tag == 'call':\\n                    rule = _pickRule(self._tree, statement.get('rule'))\\n                    cloned_matrix = matrix.copy()\\n                    entry = rule, depth + 1, cloned_matrix\\n                    stack.append(entry)\\n                elif statement.tag == 'instance':\\n                    name = statement.get('shape')\\n                    shape = name, matrix\\n                    shapes.append(shape)\\n                else:\\n                    print('malformed xml')\\n                    quit()\\n    print('\\\\nGenerated %d shapes.' % len(shapes))\\n    return shapes\\n\", 'evaluate self entry stack entry shapes while len stack 0 if len shapes self maxobjects print max objects reached break if len shapes self progresscount 1000 print len shapes curve segments so far print self maxobjects self progresscount len shapes rule depth matrix stack pop local max depth self maxdepth if max depth in rule attrib local max depth int rule get max depth if len stack self maxdepth shapes append none continue if depth local max depth if successor in rule attrib successor rule get successor rule pickrule self tree successor stack append rule 0 matrix shapes append none continue for statement in rule tstr statement get transforms if not tstr tstr for in tx ty tz rx ry rz sa sx sy sz tvalue statement get if tvalue eval tvalue tstr format xform parsexform tstr count int statement get count 1 for in range count matrix xform if statement tag call rule pickrule self tree statement get rule cloned matrix matrix copy entry rule depth 1 cloned matrix stack append entry elif statement tag instance name statement get shape shape name matrix shapes append shape else print malformed xml quit print ngenerated shapes len shapes return shapes', ''), ('make_tube', 96, 'def make_tube(self, mats, verts):\\n    \"\"\"\\n        takes a list of vertices and a list of matrices\\n        the vertices are to be joined in a ring, copied and transformed by the 1st matrix \\n        and this ring joined to the previous ring.\\n\\n        The ring dosen\\'t have to be planar.\\n        outputs lists of vertices, edges and faces\\n        \"\"\"\\n    edges_out = []\\n    verts_out = []\\n    faces_out = []\\n    vID = 0\\n    if len(mats) > 1:\\n        nring = len(verts[0])\\n        faces_out.append(list(range(nring)))\\n        for i, m in enumerate(mats):\\n            for j, v in enumerate(verts[0]):\\n                vout = mu.Matrix(m) * mu.Vector(v)\\n                verts_out.append(vout.to_tuple())\\n                vID = j + i * nring\\n                if j != 0:\\n                    edges_out.append([vID, vID - 1])\\n                else:\\n                    edges_out.append([vID, vID + nring - 1])\\n                if i != 0:\\n                    edges_out.append([vID, vID - nring])\\n                    if j != 0:\\n                        faces_out.append([vID, vID - nring, vID - nring - 1,\\n                            vID - 1])\\n                    else:\\n                        faces_out.append([vID, vID - nring, vID - 1, vID +\\n                            nring - 1])\\n        f = list(range(vID, vID - nring, -1))\\n        faces_out.append(f)\\n    return verts_out, edges_out, faces_out\\n', 'make tube self mats verts edges out verts out faces out vid 0 if len mats 1 nring len verts 0 faces out append list range nring for in enumerate mats for in enumerate verts 0 vout mu matrix mu vector verts out append vout to tuple vid nring if 0 edges out append vid vid 1 else edges out append vid vid nring 1 if 0 edges out append vid vid nring if 0 faces out append vid vid nring vid nring 1 vid 1 else faces out append vid vid nring vid 1 vid nring 1 list range vid vid nring 1 faces out append return verts out edges out faces out', 'takes a list of vertices and a list of matrices the vertices are to be joined in a ring copied and transformed by the 1st matrix and this ring joined to the previous ring')] [] [('__init__', 23, 'def __init__(self, n_cv=1, cv_test_size=0.5, random_state=57):\\n    self.n_cv = n_cv\\n    self.cv_test_size = cv_test_size\\n    self.random_state = random_state\\n', 'init self cv 1 cv test size 0 5 random state 57 self cv cv self cv test size cv test size self random state random state', ''), ('get_cv', 28, 'def get_cv(self, X, y):\\n    unique_event_ids = np.unique(y[:, (0)])\\n    event_cv = ShuffleSplit(n_splits=self.n_cv, test_size=self.cv_test_size,\\n        random_state=self.random_state)\\n    for train_event_is, test_event_is in event_cv.split(unique_event_ids):\\n        train_is = np.where(np.in1d(y[:, (0)], unique_event_ids[\\n            train_event_is]))[0]\\n        test_is = np.where(np.in1d(y[:, (0)], unique_event_ids[test_event_is])\\n            )[0]\\n        yield train_is, test_is\\n', 'get cv self unique event ids np unique 0 event cv shufflesplit splits self cv test size self cv test size random state self random state for train event is test event is in event cv split unique event ids train is np where np 0 unique event ids train event is 0 test is np where np 0 unique event ids test event is 0 yield train is test is', '')] [('upload', 13, '@abstractmethod\\ndef upload(self, resource_id: str, container: str):\\n    raise NotImplementedError\\n', 'upload self resource id str container str raise notimplementederror', ''), ('download', 17, '@abstractmethod\\ndef download(self) ->Tuple[str, str]:\\n    raise NotImplementedError\\n', 'download self tuple str str raise notimplementederror', ''), ('__init__', 25, 'def __init__(self, read_api: str, write_api: str, client_id: str):\\n    self._read_api = read_api\\n    self._write_api = write_api\\n    self._client_id = client_id\\n', 'init self read api str write api str client id str self read api read api self write api write api self client id client id', ''), ('_upload_url', 30, \"@property\\ndef _upload_url(self) ->str:\\n    return 'http://{host}/api/email/upload/{client_id}'.format(client_id=\\n        self._client_id, host=self._write_api)\\n\", 'upload url self str return http host api email upload client id format client id self client id host self write api', ''), ('_download_url', 36, \"@property\\ndef _download_url(self) ->str:\\n    return 'http://{host}/api/email/download/{client_id}'.format(client_id=\\n        self._client_id, host=self._read_api)\\n\", 'download url self str return http host api email download client id format client id self client id host self read api', ''), ('upload', 42, \"def upload(self, resource_id, container):\\n    payload = {'resource_id': resource_id, 'container_name': container,\\n        'resource_type': self._supported_resource_type}\\n    response = http_post(self._upload_url, json=payload)\\n    response.raise_for_status()\\n\", 'upload self resource id container payload resource id resource id container name container resource type self supported resource type response http post self upload url json payload response raise for status', ''), ('download', 52, 'def download(self):\\n    response = http_get(self._download_url)\\n    response.raise_for_status()\\n    resource_id, resource_container = self._validate(response)\\n    return resource_id, resource_container\\n', 'download self response http get self download url response raise for status resource id resource container self validate response return resource id resource container', ''), ('_validate', 60, \"def _validate(self, response: Response):\\n    try:\\n        payload = response.json()\\n    except ValueError:\\n        payload = {}\\n    resource_id = payload.get('resource_id', '')\\n    resource_container = payload.get('resource_container', '')\\n    resource_type = payload.get('resource_type', '').lower()\\n    if resource_type and resource_type != self._supported_resource_type:\\n        raise ValueError('unsupported resource type: {}'.format(resource_type))\\n    return resource_id, resource_container\\n\", 'validate self response response try payload response json except valueerror payload resource id payload get resource id resource container payload get resource container resource type payload get resource type lower if resource type and resource type self supported resource type raise valueerror unsupported resource type format resource type return resource id resource container', ''), ('__init__', 78, 'def __init__(self, *args, **kwargs):\\n    pass\\n', 'init self args kwargs pass', ''), ('download', 81, \"def download(self) ->Tuple[str, str]:\\n    container, resource_id = 'to-lokole', 'emails.pack'\\n    local_file = path.join(getenv('AZURE_ROOT'), container, resource_id)\\n    if not path.isfile(local_file):\\n        return '', ''\\n    return resource_id, container\\n\", 'download self tuple str str container resource id to lokole emails pack local file path join getenv azure root container resource id if not path isfile local file return return resource id container', ''), ('upload', 88, \"def upload(self, resource_id: str, container: str):\\n    upload_directory = path.join(getenv('AZURE_ROOT'), container)\\n    print('Uploaded to {}'.format(upload_directory))\\n\", 'upload self resource id str container str upload directory path join getenv azure root container print uploaded to format upload directory', '')] [('__init__', 40, 'def __init__(self, msg):\\n    self.msg = msg\\n', 'init self msg self msg msg', ''), ('__str__', 43, 'def __str__(self):\\n    return self.msg\\n', 'str self return self msg', ''), ('__init__', 47, 'def __init__(self, msg):\\n    self.msg = msg\\n', 'init self msg self msg msg', ''), ('__str__', 50, 'def __str__(self):\\n    return self.msg\\n', 'str self return self msg', ''), ('__init__', 54, 'def __init__(self, msg):\\n    self.msg = msg\\n', 'init self msg self msg msg', ''), ('__str__', 57, 'def __str__(self):\\n    return self.msg\\n', 'str self return self msg', ''), ('__init__', 61, 'def __init__(self, msg):\\n    self.msg = msg\\n', 'init self msg self msg msg', ''), ('__str__', 64, 'def __str__(self):\\n    return self.msg\\n', 'str self return self msg', ''), ('__init__', 68, 'def __init__(self, msg):\\n    self.msg = msg\\n', 'init self msg self msg msg', ''), ('__str__', 71, 'def __str__(self):\\n    return self.msg\\n', 'str self return self msg', '')] [('__init__', 45, 'def __init__(self, **kwargs):\\n    \"\"\"Constructor\\n\\n        :params group: if specified all config options apply to that group.\\n\\n        :params **kwargs: the rest of the kwargs are processed as a\\n        set of key/value pairs to be set as configuration override.\\n\\n        \"\"\"\\n    super(ConfPatcher, self).__init__()\\n    self.group = kwargs.pop(\\'group\\', None)\\n    self.args = kwargs\\n', 'init self kwargs super confpatcher self init self group kwargs pop group none self args kwargs', 'constructor'), ('setUp', 58, 'def setUp(self):\\n    super(ConfPatcher, self).setUp()\\n    for k, v in self.args.items():\\n        self.addCleanup(CONF.clear_override, k, self.group)\\n        CONF.set_override(k, v, self.group)\\n', 'setup self super confpatcher self setup for in self args items self addcleanup conf clear override self group conf set override self group', ''), ('__init__', 73, 'def __init__(self, verify=True, *args, **kwargs):\\n    \"\"\"Constructor for ``RealPolicyFixture``.\\n\\n        :param verify: Whether to verify that expected and actual policies\\n            match. True by default.\\n        \"\"\"\\n    super(RealPolicyFixture, self).__init__(*args, **kwargs)\\n    self.verify = verify\\n', 'init self verify true args kwargs super realpolicyfixture self init args kwargs self verify verify', 'constructor for realpolicyfixture'), ('_setUp', 82, \"def _setUp(self):\\n    super(RealPolicyFixture, self)._setUp()\\n    self.policy_dir = self.useFixture(fixtures.TempDir())\\n    self.policy_file = os.path.join(self.policy_dir.path, 'policy.yaml')\\n    policy_rules = yaml.safe_load(fake_policy.policy_data)\\n    self.add_missing_default_rules(policy_rules)\\n    with open(self.policy_file, 'w') as f:\\n        yaml.safe_dump(policy_rules, f)\\n    policy_opts.set_defaults(CONF)\\n    self.useFixture(ConfPatcher(policy_dirs=[], policy_file=self.\\n        policy_file, group='oslo_policy'))\\n    armada.common.policy.reset_policy()\\n    armada.common.policy.setup_policy()\\n    self.addCleanup(armada.common.policy.reset_policy)\\n    if self.verify:\\n        self._install_policy_verification_hook()\\n\", 'setup self super realpolicyfixture self setup self policy dir self usefixture fixtures tempdir self policy file os path join self policy dir path policy yaml policy rules yaml safe load fake policy policy data self add missing default rules policy rules with open self policy file as yaml safe dump policy rules policy opts set defaults conf self usefixture confpatcher policy dirs policy file self policy file group oslo policy armada common policy reset policy armada common policy setup policy self addcleanup armada common policy reset policy if self verify self install policy verification hook', ''), ('_verify_policies_match', 105, 'def _verify_policies_match(self):\\n    \"\"\"Validate that the expected and actual policies are equivalent.\\n        Otherwise an ``AssertionError`` is raised.\\n        \"\"\"\\n    if not set(self.expected_policy_actions) == set(self.actual_policy_actions\\n        ):\\n        error_msg = (\\n            \\'The expected policy actions passed to `self.policy.set_rules` do not match the policy actions that were actually enforced by Armada. Set of expected policies %s should be equal to set of actual policies: %s. There is either a bug with the test or with policy enforcement in the controller.\\'\\n             % (self.expected_policy_actions, self.actual_policy_actions))\\n        raise AssertionError(error_msg)\\n', 'verify policies match self if not set self expected policy actions set self actual policy actions error msg the expected policy actions passed to self policy set rules do not match the policy actions that were actually enforced by armada set of expected policies should be equal to set of actual policies there is either bug with the test or with policy enforcement in the controller self expected policy actions self actual policy actions raise assertionerror error msg', 'validate that the expected and actual policies are equivalent otherwise an assertionerror is raised'), ('_install_policy_verification_hook', 123, 'def _install_policy_verification_hook(self):\\n    \"\"\"Install policy verification hook for validating RBAC.\\n\\n        This function\\'s purpose is to guarantee that policy enforcement is\\n        happening the way we expect it to. It validates that the policies\\n        that are passed to ``self.policy.set_rules`` from within a test that\\n        uses this fixture is a subset of the actual policies that are enforced\\n        by Armada controllers.\\n\\n        The algorithm is as follows:\\n\\n            1) Initialize list of actual policy actions to remember.\\n            2) Initialize list of expected policy actions to remember.\\n            3) Reference a pre-mocked copy of the policy enforcement function\\n               that is ultimately called by Armada for policy enforcement.\\n            4a) Create a hook that stores the actual policy for later.\\n            4b) The hook then calls the *real* policy enforcement function\\n                using the reference from step 3).\\n            5) Mock the policy enforcement function and have it instead call\\n               our hook from step 4a).\\n            6) Add a clean up to undo the mock from step 5).\\n\\n        There is a tight coupling between this function and ``set_rules``\\n        below.\\n\\n        The comparison between ``self.expected_policy_actions`` and\\n        ``self.actual_policy_actions`` is performed during clean up.\\n        \"\"\"\\n    self.actual_policy_actions = []\\n    self.expected_policy_actions = []\\n    _do_enforce_rbac = armada.common.policy._enforce_policy\\n\\n    def enforce_policy_and_remember_actual_rules(action, *a, **k):\\n        self.actual_policy_actions.append(action)\\n        _do_enforce_rbac(action, *a, **k)\\n    mock_do_enforce_rbac = mock.patch.object(armada.common.policy,\\n        \\'_enforce_policy\\', autospec=True).start()\\n    mock_do_enforce_rbac.side_effect = enforce_policy_and_remember_actual_rules\\n    self.addCleanup(mock.patch.stopall)\\n    self.addCleanup(self._verify_policies_match)\\n', 'install policy verification hook self self actual policy actions self expected policy actions do enforce rbac armada common policy enforce policy def enforce policy and remember actual rules action self actual policy actions append action do enforce rbac action mock do enforce rbac mock patch object armada common policy enforce policy autospec true start mock do enforce rbac side effect enforce policy and remember actual rules self addcleanup mock patch stopall self addcleanup self verify policies match', 'install policy verification hook for validating rbac'), ('add_missing_default_rules', 167, 'def add_missing_default_rules(self, rules):\\n    \"\"\"Adds default rules and their values to the given rules dict.\\n\\n        The given rulen dict may have an incomplete set of policy rules.\\n        This method will add the default policy rules and their values to\\n        the dict. It will not override the existing rules.\\n        \"\"\"\\n    for rule in policies.list_rules():\\n        if rule.name not in rules:\\n            rules[rule.name] = rule.check_str\\n', 'add missing default rules self rules for rule in policies list rules if rule name not in rules rules rule name rule check str', 'adds default rules and their values to the given rules dict'), ('set_rules', 178, 'def set_rules(self, rules, overwrite=True):\\n    \"\"\"Set the custom policy rules to override.\\n\\n        :param dict rules: Dictionary keyed with policy actions enforced\\n            by Armada whose values are a custom rule understood by\\n            ``oslo.policy`` library.\\n\\n        This function overrides the default policy rules with the custom rules\\n        specified by ``rules``. The ``rules`` passed here are added to\\n        ``self.expected_policy_actions`` for later comparison with\\n        ``self.actual_policy_actions``.\\n        \"\"\"\\n    if isinstance(rules, dict):\\n        rules = oslo_policy.Rules.from_dict(rules)\\n    self.expected_policy_actions.extend(rules)\\n    policy = armada.common.policy._ENFORCER\\n    policy.set_rules(rules, overwrite=overwrite)\\n', 'set rules self rules overwrite true if isinstance rules dict rules oslo policy rules from dict rules self expected policy actions extend rules policy armada common policy enforcer policy set rules rules overwrite overwrite', 'set the custom policy rules to override')] [('getCommand', 12, \"@staticmethod\\ndef getCommand(name):\\n\\n\\n    class command_faked_installation(HoneyPotCommand):\\n\\n        def call(self):\\n            self.writeln('%s: Segmentation fault' % name)\\n    return command_faked_installation\\n\", 'getcommand name class command faked installation honeypotcommand def call self self writeln segmentation fault name return command faked installation', ''), ('start', 23, \"def start(self):\\n    if len(self.args) > 0 and self.args[0] == 'install':\\n        self.do_install()\\n    else:\\n        self.do_locked()\\n\", 'start self if len self args 0 and self args 0 install self do install else self do locked', ''), ('sleep', 29, 'def sleep(self, time, time2=None):\\n    d = defer.Deferred()\\n    if time2:\\n        time = random.randint(time * 100, time2 * 100) / 100.0\\n    reactor.callLater(time, d.callback, None)\\n    return d\\n', 'sleep self time none defer deferred if time random randint time 100 100 100 0 reactor calllater time callback none return', ''), ('do_install', 36, \"@inlineCallbacks\\ndef do_install(self, *args):\\n    if len(self.args) <= 1:\\n        self.writeln(\\n            '0 upgraded, 0 newly installed, 0 to remove and %s not upgraded.' %\\n            random.randint(200, 300))\\n        self.exit()\\n        return\\n    packages = {}\\n    for y in [re.sub('[^A-Za-z0-9]', '', x) for x in self.args[1:]]:\\n        packages[y] = {'version': '%d.%d-%d' % (random.choice((0, 1)),\\n            random.randint(1, 40), random.randint(1, 10)), 'size': random.\\n            randint(100, 900)}\\n    totalsize = sum([packages[x]['size'] for x in packages])\\n    self.writeln('Reading package lists... Done')\\n    self.writeln('Building dependency tree')\\n    self.writeln('Reading state information... Done')\\n    self.writeln('The following NEW packages will be installed:')\\n    self.writeln('  %s ' % ' '.join(packages))\\n    self.writeln(\\n        '0 upgraded, %d newly installed, 0 to remove and 259 not upgraded.' %\\n        len(packages))\\n    self.writeln('Need to get %s.2kB of archives.' % totalsize)\\n    self.writeln(\\n        'After this operation, %skB of additional disk space will be used.' %\\n        (totalsize * 2.2,))\\n    i = 1\\n    for p in packages:\\n        self.writeln(\\n            'Get:%d http://ftp.debian.org stable/main %s %s [%s.2kB]' % (i,\\n            p, packages[p]['version'], packages[p]['size']))\\n        i += 1\\n        yield self.sleep(1, 2)\\n    self.writeln('Fetched %s.2kB in 1s (4493B/s)' % totalsize)\\n    self.writeln('Reading package fields... Done')\\n    yield self.sleep(1, 2)\\n    self.writeln('Reading package status... Done')\\n    self.writeln(\\n        '(Reading database ... 177887 files and directories currently installed.)'\\n        )\\n    yield self.sleep(1, 2)\\n    for p in packages:\\n        self.writeln('Unpacking %s (from .../archives/%s_%s_i386.deb) ...' %\\n            (p, p, packages[p]['version']))\\n        yield self.sleep(1, 2)\\n    self.writeln('Processing triggers for man-db ...')\\n    yield self.sleep(2)\\n    for p in packages:\\n        self.writeln('Setting up %s (%s) ...' % (p, packages[p]['version']))\\n        self.fs.mkfile('/usr/bin/%s' % p, 0, 0, random.randint(10000, 90000\\n            ), 33188)\\n        self.honeypot.commands['/usr/bin/%s' % p\\n            ] = command_faked_package_class_factory.getCommand(p)\\n        yield self.sleep(2)\\n    self.exit()\\n\", 'do install self args if len self args 1 self writeln 0 upgraded 0 newly installed 0 to remove and not upgraded random randint 200 300 self exit return packages for in re sub za 9 for in self args 1 packages version random choice 0 1 random randint 1 40 random randint 1 10 size random randint 100 900 totalsize sum packages size for in packages self writeln reading package lists done self writeln building dependency tree self writeln reading state information done self writeln the following new packages will be installed self writeln join packages self writeln 0 upgraded newly installed 0 to remove and 259 not upgraded len packages self writeln need to get of archives totalsize self writeln after this operation skb of additional disk space will be used totalsize 2 2 1 for in packages self writeln get http ftp debian org stable main packages version packages size 1 yield self sleep 1 2 self writeln fetched in totalsize self writeln reading package fields done yield self sleep 1 2 self writeln reading package status done self writeln reading database 177887 files and directories currently installed yield self sleep 1 2 for in packages self writeln unpacking from archives deb packages version yield self sleep 1 2 self writeln processing triggers for man db yield self sleep 2 for in packages self writeln setting up packages version self fs mkfile usr bin 0 0 random randint 10000 90000 33188 self honeypot commands usr bin command faked package class factory getcommand yield self sleep 2 self exit', ''), ('do_locked', 92, \"def do_locked(self):\\n    self.writeln(\\n        'E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)'\\n        )\\n    self.writeln('E: Unable to lock the list directory')\\n    self.exit()\\n\", 'do locked self self writeln could not open lock file var lib apt lists lock open 13 permission denied self writeln unable to lock the list directory self exit', '')] [('namelist', 10, 'def namelist(fname):\\n    \"\"\"\\n    return the tarfile nameand the targz file name\\n    \"\"\"\\n    basename = fname.split(\\'.\\')[0]\\n    tarfilename = basename + \\'.tar\\'\\n    targzname = tarfilename + \\'.gz\\'\\n    return tarfilename, targzname\\n', 'namelist fname basename fname split 0 tarfilename basename tar targzname tarfilename gz return tarfilename targzname', 'return the tarfile nameand the targz file name'), ('tarfilelist', 20, 'def tarfilelist(lst, fname):\\n    \"\"\"\\n    tar up all the filenames in lst into base.tar where base = basename for\\n    fname\\n    \"\"\"\\n    outfname, _ = namelist(fname)\\n    f = tarfile.open(outfname, \\'w\\')\\n    for s in lst:\\n        f.add(s)\\n    f.close()\\n    return outfname\\n', 'tarfilelist lst fname outfname namelist fname tarfile open outfname for in lst add close return outfname', 'tar up all the filenames in lst into base tar where base basename for fname'), ('snspectralist', 34, 'def snspectralist(fname, logffname=None):\\n    \"\"\"\\n    List all the spectra files associated with a phosim instance catalog\\n    \"\"\"\\n    x = []\\n    with open(fname, \\'r\\') as f:\\n        for line in f:\\n            if \\'spectra_file\\' in line:\\n                x.append(line.split()[5])\\n    return x\\n', 'snspectralist fname logffname none with open fname as for line in if spectra file in line append line split 5 return', 'list all the spectra files associated with a phosim instance catalog'), ('listFiles', 45, 'def listFiles(logfile, prefix=\\'InstanceCatalogs/phosim_input_\\'):\\n    \"\"\"\\n    Read the log file to get a list of phosim instance catalogs done\\n    \"\"\"\\n    df = pd.read_csv(logfile)\\n    fileList = [(prefix + str(x) + \\'.txt\\') for x in df.obsHistID.values]\\n    return fileList\\n    return x\\n', 'listfiles logfile prefix instancecatalogs phosim input df pd read csv logfile filelist prefix str txt for in df obshistid values return filelist return', 'read the log file to get a list of phosim instance catalogs done'), ('gziptarfile', 56, 'def gziptarfile(fname, prefix=\\'\\'):\\n    \"\"\"\\n    gzip a tarred up file\\n    \"\"\"\\n    tarfilename, targzname = namelist(fname)\\n    targzname = prefix + targzname\\n    with open(tarfilename, \\'rb\\') as f_in, gzip.open(targzname, \\'wb\\') as f_out:\\n        shutil.copyfileobj(f_in, f_out)\\n', 'gziptarfile fname prefix tarfilename targzname namelist fname targzname prefix targzname with open tarfilename rb as in gzip open targzname wb as out shutil copyfileobj in out', 'gzip a tarred up file'), ('cleanup', 64, 'def cleanup(fname):\\n    l = snspectralist(fname)\\n    tarfilename, _ = namelist(fname)\\n    for file in l:\\n        os.remove(file)\\n    os.remove(tarfilename)\\n', 'cleanup fname snspectralist fname tarfilename namelist fname for file in os remove file os remove tarfilename', '')] [('__init__', 15, 'def __init__(self, services=None):\\n    self.registered_services = {}\\n    if services:\\n        for service in services:\\n            self.register_service(service)\\n', 'init self services none self registered services if services for service in services self register service service', ''), ('register_service', 21, 'def register_service(self, service):\\n    \"\"\"Adds a new service to this registry.\"\"\"\\n    self.registered_services[service.name] = service\\n', 'register service self service self registered services service name service', 'adds a new service to this registry'), ('unregister_service', 25, 'def unregister_service(self, service):\\n    \"\"\"Removes a service from this registry.\"\"\"\\n    del self.registered_services[service.name]\\n', 'unregister service self service del self registered services service name', 'removes a service from this registry'), ('process_message_from', 29, 'def process_message_from(self, client, msg):\\n    \"\"\"Delivers a message to the destination service specified in it, which\\n        should be associated with this registry.\\n        \"\"\"\\n    try:\\n        service_name = msg[\\'service\\']\\n        content = msg[\\'message\\']\\n    except KeyError:\\n        snorky_log.warning(\\'Malformed message from client %s: %s\\' % (client\\n            .remote_address, msg))\\n        return\\n    try:\\n        service = self.registered_services[service_name]\\n    except KeyError:\\n        snorky_log.warning(\\n            \\'Message for non existing service \"%s\" from client %s\\' % (\\n            service_name, client.remote_address))\\n        return\\n    service.process_message_from(client, content)\\n', 'process message from self client msg try service name msg service content msg message except keyerror snorky log warning malformed message from client client remote address msg return try service self registered services service name except keyerror snorky log warning message for non existing service from client service name client remote address return service process message from client content', 'delivers a message to the destination service specified in it which should be associated with this registry'), ('process_message_raw', 51, 'def process_message_raw(self, client, msg):\\n    \"\"\"Decodes a message encoded as a JSON character string and sends it to\\n        the destination service.\"\"\"\\n    try:\\n        decoded_msg = make_hashable(json.loads(msg))\\n    except ValueError:\\n        snorky_log.warning(\\'Invalid JSON from client %s: %s\\' % (client.\\n            remote_address, msg))\\n        return\\n    return self.process_message_from(client, decoded_msg)\\n', 'process message raw self client msg try decoded msg make hashable json loads msg except valueerror snorky log warning invalid json from client client remote address msg return return self process message from client decoded msg', 'decodes a message encoded as a json character string and sends it to the destination service'), ('client_connected', 63, 'def client_connected(self, client):\\n    \"\"\"This method must be called every time a new client connects to the\\n        server through a request handler associated with this registry.\\n        \"\"\"\\n    for service in self.registered_services.values():\\n        service.client_connected(client)\\n', 'client connected self client for service in self registered services values service client connected client', 'this method must be called every time a new client connects to the server through a request handler associated with this registry'), ('client_disconnected', 70, 'def client_disconnected(self, client):\\n    \"\"\"This method must be called every time a client disconnects from the\\n        server.\\n        \"\"\"\\n    for service in self.registered_services.values():\\n        service.client_disconnected(client)\\n', 'client disconnected self client for service in self registered services values service client disconnected client', 'this method must be called every time a client disconnects from the server')] [('add_default_join_mode', 10, \"def add_default_join_mode(apps, _):\\n    organization_membership_model = apps.get_model('organizations',\\n        'OrganizationMembership')\\n    facility_membership_model = apps.get_model('organizations',\\n        'FacilityMembership')\\n    organization_membership_model.objects.update(status=Membership.Status.\\n        APPROVED)\\n    facility_membership_model.objects.update(status=Membership.Status.APPROVED)\\n\", 'add default join mode apps organization membership model apps get model organizations organizationmembership facility membership model apps get model organizations facilitymembership organization membership model objects update status membership status approved facility membership model objects update status membership status approved', '')] [('generate', 40, 'def generate(env):\\n    \"\"\"Add Builders and construction variables for ifl to an Environment.\"\"\"\\n    fscan = FortranScan(\\'FORTRANPATH\\')\\n    SCons.Tool.SourceFileScanner.add_scanner(\\'.i\\', fscan)\\n    SCons.Tool.SourceFileScanner.add_scanner(\\'.i90\\', fscan)\\n    if \\'FORTRANFILESUFFIXES\\' not in env:\\n        env[\\'FORTRANFILESUFFIXES\\'] = [\\'.i\\']\\n    else:\\n        env[\\'FORTRANFILESUFFIXES\\'].append(\\'.i\\')\\n    if \\'F90FILESUFFIXES\\' not in env:\\n        env[\\'F90FILESUFFIXES\\'] = [\\'.i90\\']\\n    else:\\n        env[\\'F90FILESUFFIXES\\'].append(\\'.i90\\')\\n    add_all_to_env(env)\\n    env[\\'FORTRAN\\'] = \\'ifl\\'\\n    env[\\'SHFORTRAN\\'] = \\'$FORTRAN\\'\\n    env[\\'FORTRANCOM\\'\\n        ] = \\'$FORTRAN $FORTRANFLAGS $_FORTRANINCFLAGS /c $SOURCES /Fo$TARGET\\'\\n    env[\\'FORTRANPPCOM\\'] = (\\n        \\'$FORTRAN $FORTRANFLAGS $CPPFLAGS $_CPPDEFFLAGS $_FORTRANINCFLAGS /c $SOURCES /Fo$TARGET\\'\\n        )\\n    env[\\'SHFORTRANCOM\\'\\n        ] = \\'$SHFORTRAN $SHFORTRANFLAGS $_FORTRANINCFLAGS /c $SOURCES /Fo$TARGET\\'\\n    env[\\'SHFORTRANPPCOM\\'] = (\\n        \\'$SHFORTRAN $SHFORTRANFLAGS $CPPFLAGS $_CPPDEFFLAGS $_FORTRANINCFLAGS /c $SOURCES /Fo$TARGET\\'\\n        )\\n', 'generate env fscan fortranscan fortranpath scons tool sourcefilescanner add scanner fscan scons tool sourcefilescanner add scanner fscan if fortranfilesuffixes not in env env fortranfilesuffixes else env fortranfilesuffixes append if not in env env else env append add all to env env env fortran ifl env shfortran fortran env fortrancom fortran fortranflags fortranincflags sources fo target env fortranppcom fortran fortranflags cppflags cppdefflags fortranincflags sources fo target env shfortrancom shfortran shfortranflags fortranincflags sources fo target env shfortranppcom shfortran shfortranflags cppflags cppdefflags fortranincflags sources fo target', 'add builders and construction variables for ifl to an environment'), ('exists', 65, \"def exists(env):\\n    return env.Detect('ifl')\\n\", 'exists env return env detect ifl', '')] [('features', 13, \"@pytest.fixture\\ndef features():\\n    feat1 = {'type': 'Feature', 'geometry': {'type': 'Point', 'coordinates':\\n        [1.0, 2.0]}, 'properties': {'prop1': 'value1', 'prop2': 2.0}}\\n    feat2 = {'type': 'Feature', 'geometry': {'type': 'LineString',\\n        'coordinates': [[3.0, 4.0], [5.0, 6.0]]}, 'properties': {'prop1':\\n        'lineval', 'prop2': 3.0}}\\n    fc = {'type': 'FeatureCollection', 'features': [feat1, feat2]}\\n    return fc\\n\", 'features type feature geometry type point coordinates 1 0 2 0 properties 2 0 type feature geometry type linestring coordinates 3 0 4 0 5 0 6 0 properties lineval 3 0 fc type featurecollection features return fc', ''), ('test_parse_string', 59, 'def test_parse_string(features):\\n    contents = json.dumps(features)\\n    assert geojsonio.make_geojson(contents) == contents\\n', 'test parse string features contents json dumps features assert geojsonio make geojson contents contents', ''), ('test_parse_single_feature_geo', 65, \"def test_parse_single_feature_geo(features):\\n    feature = features['features'][0]\\n    o = GeoDict(feature)\\n    expected = {'type': 'FeatureCollection', 'features': [feature]}\\n    contents = geojsonio.make_geojson(o)\\n    assert contents == json.dumps(expected)\\n\", 'test parse single feature geo features feature features features 0 geodict feature expected type featurecollection features feature contents geojsonio make geojson assert contents json dumps expected', ''), ('test_parse_list_features_geo', 78, \"def test_parse_list_features_geo(features):\\n    os = [GeoDict(f) for f in features['features']]\\n    contents = geojsonio.make_geojson(os)\\n    dict_contents = json.loads(contents)\\n    assert features == dict_contents\\n\", 'test parse list features geo features os geodict for in features features contents geojsonio make geojson os dict contents json loads contents assert features dict contents', ''), ('test_parse_fail_non_feature', 86, 'def test_parse_fail_non_feature():\\n    with pytest.raises(ValueError):\\n        geojsonio.make_geojson(5)\\n', 'test parse fail non feature with pytest raises valueerror geojsonio make geojson 5', ''), ('test_parse_fail_list_non_feature', 91, \"def test_parse_fail_list_non_feature(features):\\n    os = [GeoDict(features['features'][0]), 5]\\n    with pytest.raises(ValueError):\\n        geojsonio.make_geojson(os)\\n\", 'test parse fail list non feature features os geodict features features 0 5 with pytest raises valueerror geojsonio make geojson os', ''), ('test_data_url', 97, \"def test_data_url(features):\\n    feature = features['features'][0]\\n    header = 'http://geojson.io/#data=data:application/json,'\\n    length = len(header)\\n    result = geojsonio.data_url(json.dumps(feature))\\n    payload = json.loads(urllib.parse.unquote(result[length:]))\\n    assert result[:length] == header\\n    assert payload == feature\\n\", 'test data url features feature features features 0 header http geojson io data data application json length len header result geojsonio data url json dumps feature payload json loads urllib parse unquote result length assert result length header assert payload feature', ''), ('test_gist_url', 109, \"def test_gist_url():\\n    expected = 'http://geojson.io/#id=gist:/abcd'\\n    assert expected == geojsonio.gist_url('abcd')\\n\", 'test gist url expected http geojson io id gist abcd assert expected geojsonio gist url abcd', ''), ('test_factory_data', 115, 'def test_factory_data(features):\\n    contents = json.dumps(features)\\n    size = len(contents)\\n    url = geojsonio.make_url(contents, size_for_gist=size + 1)\\n    assert url == geojsonio.data_url(contents)\\n', 'test factory data features contents json dumps features size len contents url geojsonio make url contents size for gist size 1 assert url geojsonio data url contents', ''), ('test_factory_gist', 123, \"def test_factory_gist(features):\\n    contents = json.dumps(features)\\n    size = len(contents)\\n    with mock.patch.object(github3.GitHub, 'create_gist') as MockInstance:\\n\\n\\n        class Dummy(object):\\n            id = 'abc123'\\n        MockInstance.return_value = Dummy()\\n        url = geojsonio.make_url(contents, size_for_gist=size - 1)\\n    assert url == geojsonio.gist_url(Dummy.id)\\n\", 'test factory gist features contents json dumps features size len contents with mock patch object github create gist as mockinstance class dummy object id mockinstance return value dummy url geojsonio make url contents size for gist size 1 assert url geojsonio gist url dummy id', ''), ('test_factory_force_gist', 136, \"def test_factory_force_gist(features):\\n    contents = json.dumps(features)\\n    size = len(contents)\\n    with mock.patch.object(github3.GitHub, 'create_gist') as MockInstance:\\n\\n\\n        class Dummy(object):\\n            id = 'abc123'\\n        MockInstance.return_value = Dummy()\\n        url = geojsonio.make_url(contents, size_for_gist=size + 1,\\n            force_gist=True)\\n    assert url == geojsonio.gist_url(Dummy.id)\\n\", 'test factory force gist features contents json dumps features size len contents with mock patch object github create gist as mockinstance class dummy object id mockinstance return value dummy url geojsonio make url contents size for gist size 1 force gist true assert url geojsonio gist url dummy id', ''), ('__init__', 51, 'def __init__(self, d):\\n    self.d = d\\n', 'init self self', ''), ('__geo_interface__', 54, '@property\\ndef __geo_interface__(self):\\n    return self.d\\n', 'geo interface self return self', '')] [('S_diff', 27, 'def S_diff(lst):\\n    \"\"\"Given a list of int or float, calculate S_diff and S_point\"\"\"\\n    S_avg = sum(lst) / len(lst)\\n    S_dist = [(i - S_avg) for i in lst]\\n    S_cum = []\\n    S_cum.append(0)\\n    for i in range(0, len(S_dist)):\\n        S_cum.append(S_cum[i] + S_dist[i])\\n    return [nlargest(1, range(0, len(S_cum)), key=lambda i: S_cum[i]), max(\\n        S_cum) - min(S_cum)]\\n', 'diff lst avg sum lst len lst dist avg for in lst cum cum append 0 for in range 0 len dist cum append cum dist return nlargest 1 range 0 len cum key lambda cum max cum min cum', 'given a list of int or float calculate s diff and s point'), ('bootstrap', 39, 'def bootstrap(lst, obs, rep=1000):\\n    \"\"\"Given a list of int or float (lst) and an observation value(obs). calcualte the chance (pvalue) \\n\\tof getting this observation through bootstrapping.\"\"\"\\n    shuffled_diff = []\\n    count = 0\\n    tmp = lst\\n    for i in range(0, rep):\\n        shuffle(tmp)\\n        shuffled_diff.append(S_diff(tmp))\\n    for i in sorted(shuffled_diff):\\n        if i >= obs:\\n            count += 1\\n    if count / rep < 0.5:\\n        return count / rep\\n    else:\\n        return 1 - count / rep\\n', 'bootstrap lst obs rep 1000 shuffled diff count 0 tmp lst for in range 0 rep shuffle tmp shuffled diff append diff tmp for in sorted shuffled diff if obs count 1 if count rep 0 5 return count rep else return 1 count rep', 'given a list of int or float lst and an observation value obs calcualte the chance pvalue of getting this observation through bootstrapping')] [('get_current_branch', 6, 'def get_current_branch():\\n    current_branch = local(\\n        \"git branch --no-color | grep \\'^\\\\\\\\* \\' | grep -v \\'no branch\\' | sed \\'s/^* //g\\'\"\\n        , capture=True)\\n    return current_branch\\n', 'get current branch current branch local git branch no color grep grep no branch sed capture true return current branch', ''), ('change_branch', 10, \"def change_branch(branch='master'):\\n    return local('git checkout %s' % branch)\\n\", 'change branch branch master return local git checkout branch', ''), ('get_latest_version', 13, 'def get_latest_version():\\n    return local(\"git tag -l | tail -n 1 | sed -e \\'s/^v//g\\'\", capture=True)\\n', 'get latest version return local git tag tail 1 sed capture true', ''), ('make_archive', 16, \"def make_archive(version='HEAD'):\\n    filename = '%s-%s.tar.gz' % (PROJECT_NAME, version)\\n    local('git archive --format=tar --prefix=%s/ %s | gzip >%s' % (\\n        PROJECT_NAME, version, filename))\\n    return filename\\n\", 'make archive version head filename tar gz project name version local git archive format tar prefix gzip project name version filename return filename', ''), ('tag', 23, \"def tag(docker_index, image_version):\\n    sudo('docker tag -f %s %s/%s:%s' % (PROJECT_NAME, docker_index,\\n        PROJECT_NAME, image_version))\\n    sudo('docker tag -f %s %s/%s:latest' % (PROJECT_NAME, docker_index,\\n        PROJECT_NAME))\\n\", 'tag docker index image version sudo docker tag project name docker index project name image version sudo docker tag latest project name docker index project name', ''), ('push', 30, \"def push(docker_index, image_version):\\n    sudo('docker push %s/%s:%s' % (docker_index, PROJECT_NAME, image_version))\\n    sudo('docker push %s/%s:latest' % (docker_index, PROJECT_NAME))\\n\", 'push docker index image version sudo docker push docker index project name image version sudo docker push latest docker index project name', ''), ('build', 37, \"def build(docker_index='docker.timbaobjects.com', version='HEAD'):\\n    current_branch = get_current_branch()\\n    image_version = get_latest_version()\\n    change_branch('master')\\n    archive = make_archive(version)\\n    run('rm -rf %s' % PROJECT_DIR)\\n    run('mkdir -p %s' % PROJECT_DIR)\\n    put(archive, PROJECT_DIR)\\n    with cd(PROJECT_DIR):\\n        run('tar xzf %s' % archive)\\n    with cd('%s/%s' % (PROJECT_DIR, PROJECT_NAME)):\\n        sudo('docker build --rm=false -t %s .' % PROJECT_NAME)\\n    tag(docker_index, image_version)\\n    push(docker_index, image_version)\\n    change_branch(current_branch)\\n\", 'build docker index docker timbaobjects com version head current branch get current branch image version get latest version change branch master archive make archive version run rm rf project dir run mkdir project dir put archive project dir with cd project dir run tar xzf archive with cd project dir project name sudo docker build rm false project name tag docker index image version push docker index image version change branch current branch', '')] [] [('testWithStatement', 19, \"def testWithStatement(self):\\n    with patch('tests._testwith.something', sentinel.Something2):\\n        self.assertEqual(something, sentinel.Something2, 'unpatched')\\n    self.assertEqual(something, sentinel.Something)\\n\", 'testwithstatement self with patch tests testwith something sentinel self assertequal something sentinel unpatched self assertequal something sentinel something', ''), ('testWithStatementException', 24, \"def testWithStatementException(self):\\n    try:\\n        with patch('tests._testwith.something', sentinel.Something2):\\n            self.assertEqual(something, sentinel.Something2, 'unpatched')\\n            raise Exception('pow')\\n    except Exception:\\n        pass\\n    else:\\n        self.fail('patch swallowed exception')\\n    self.assertEqual(something, sentinel.Something)\\n\", 'testwithstatementexception self try with patch tests testwith something sentinel self assertequal something sentinel unpatched raise exception pow except exception pass else self fail patch swallowed exception self assertequal something sentinel something', ''), ('testWithStatementAs', 36, \"def testWithStatementAs(self):\\n    with patch('tests._testwith.something') as mock_something:\\n        self.assertEqual(something, mock_something, 'unpatched')\\n        self.assertTrue(isinstance(mock_something, Mock), 'patching wrong type'\\n            )\\n    self.assertEqual(something, sentinel.Something)\\n\", 'testwithstatementas self with patch tests testwith something as mock something self assertequal something mock something unpatched self asserttrue isinstance mock something mock patching wrong type self assertequal something sentinel something', ''), ('testPatchObjectWithStatementAs', 43, \"def testPatchObjectWithStatementAs(self):\\n    mock = Mock()\\n    original = mock.something\\n    with patch.object(mock, 'something'):\\n        self.assertNotEqual(mock.something, original, 'unpatched')\\n    self.assertEqual(mock.something, original)\\n\", 'testpatchobjectwithstatementas self mock mock original mock something with patch object mock something self assertnotequal mock something original unpatched self assertequal mock something original', ''), ('testWithStatementNested', 51, \"def testWithStatementNested(self):\\n    with catch_warnings(record=True):\\n        with nested(patch('tests._testwith.something'), patch(\\n            'tests._testwith.something_else')) as (mock_something,\\n            mock_something_else):\\n            self.assertEqual(something, mock_something, 'unpatched')\\n            self.assertEqual(something_else, mock_something_else, 'unpatched')\\n    self.assertEqual(something, sentinel.Something)\\n    self.assertEqual(something_else, sentinel.SomethingElse)\\n\", 'testwithstatementnested self with catch warnings record true with nested patch tests testwith something patch tests testwith something else as mock something mock something else self assertequal something mock something unpatched self assertequal something else mock something else unpatched self assertequal something sentinel something self assertequal something else sentinel somethingelse', ''), ('testWithStatementSpecified', 62, \"def testWithStatementSpecified(self):\\n    with patch('tests._testwith.something', sentinel.Patched\\n        ) as mock_something:\\n        self.assertEqual(something, mock_something, 'unpatched')\\n        self.assertEqual(mock_something, sentinel.Patched, 'wrong patch')\\n    self.assertEqual(something, sentinel.Something)\\n\", 'testwithstatementspecified self with patch tests testwith something sentinel patched as mock something self assertequal something mock something unpatched self assertequal mock something sentinel patched wrong patch self assertequal something sentinel something', ''), ('testContextManagerMocking', 69, 'def testContextManagerMocking(self):\\n    mock = Mock()\\n    mock.__enter__ = Mock()\\n    mock.__exit__ = Mock()\\n    mock.__exit__.return_value = False\\n    with mock as m:\\n        self.assertEqual(m, mock.__enter__.return_value)\\n    mock.__enter__.assert_called_with()\\n    mock.__exit__.assert_called_with(None, None, None)\\n', 'testcontextmanagermocking self mock mock mock enter mock mock exit mock mock exit return value false with mock as self assertequal mock enter return value mock enter assert called with mock exit assert called with none none none', ''), ('testContextManagerWithMagicMock', 81, \"def testContextManagerWithMagicMock(self):\\n    mock = MagicMock()\\n    with self.assertRaises(TypeError):\\n        with mock:\\n            'foo' + 3\\n    mock.__enter__.assert_called_with()\\n    self.assertTrue(mock.__exit__.called)\\n\", 'testcontextmanagerwithmagicmock self mock magicmock with self assertraises typeerror with mock foo 3 mock enter assert called with self asserttrue mock exit called', ''), ('testWithStatementSameAttribute', 91, \"def testWithStatementSameAttribute(self):\\n    with patch('tests._testwith.something', sentinel.Patched\\n        ) as mock_something:\\n        self.assertEqual(something, mock_something, 'unpatched')\\n        with patch('tests._testwith.something') as mock_again:\\n            self.assertEqual(something, mock_again, 'unpatched')\\n        self.assertEqual(something, mock_something,\\n            'restored with wrong instance')\\n    self.assertEqual(something, sentinel.Something, 'not restored')\\n\", 'testwithstatementsameattribute self with patch tests testwith something sentinel patched as mock something self assertequal something mock something unpatched with patch tests testwith something as mock again self assertequal something mock again unpatched self assertequal something mock something restored with wrong instance self assertequal something sentinel something not restored', ''), ('testWithStatementImbricated', 102, \"def testWithStatementImbricated(self):\\n    with patch('tests._testwith.something') as mock_something:\\n        self.assertEqual(something, mock_something, 'unpatched')\\n        with patch('tests._testwith.something_else') as mock_something_else:\\n            self.assertEqual(something_else, mock_something_else, 'unpatched')\\n    self.assertEqual(something, sentinel.Something)\\n    self.assertEqual(something_else, sentinel.SomethingElse)\\n\", 'testwithstatementimbricated self with patch tests testwith something as mock something self assertequal something mock something unpatched with patch tests testwith something else as mock something else self assertequal something else mock something else unpatched self assertequal something sentinel something self assertequal something else sentinel somethingelse', ''), ('testDictContextManager', 112, \"def testDictContextManager(self):\\n    foo = {}\\n    with patch.dict(foo, {'a': 'b'}):\\n        self.assertEqual(foo, {'a': 'b'})\\n    self.assertEqual(foo, {})\\n    with self.assertRaises(NameError):\\n        with patch.dict(foo, {'a': 'b'}):\\n            self.assertEqual(foo, {'a': 'b'})\\n            raise NameError('Konrad')\\n    self.assertEqual(foo, {})\\n\", 'testdictcontextmanager self foo with patch dict foo self assertequal foo self assertequal foo with self assertraises nameerror with patch dict foo self assertequal foo raise nameerror konrad self assertequal foo', '')] [('test', 14, 'def test(self, file):\\n    function = load_solution_function(file)\\n    prefix = get_random_string(self.dictionary, 1, 5) + \\',\\'\\n    correct_argument = \\'{}{},{}\\'.format(prefix, get_random_string(self.\\n        dictionary, 1, 5), get_random_string(self.dictionary, 1, 5))\\n    result = function(correct_argument)\\n    if result != prefix:\\n        raise AdventureVerificationError(_(\\n            \"Your function didn\\'t return the expected string \\'{}\\' when executed with \\'{}\\'. It returned \\'{}\\'.\"\\n            .format(prefix, correct_argument, result)))\\n', 'test self file function load solution function file prefix get random string self dictionary 1 5 correct argument format prefix get random string self dictionary 1 5 get random string self dictionary 1 5 result function correct argument if result prefix raise adventureverificationerror your function didn return the expected string when executed with it returned format prefix correct argument result', '')] [('af', 4, \"def af():\\n    max_name_width = 0\\n    max_af_width = 0\\n    for pin_entry in pins_af.PINS_AF:\\n        max_name_width = max(max_name_width, len(pin_entry[0]))\\n        for af_entry in pin_entry[1:]:\\n            max_af_width = max(max_af_width, len(af_entry[1]))\\n    for pin_entry in pins_af.PINS_AF:\\n        pin_name = pin_entry[0]\\n        print('%-*s ' % (max_name_width, pin_name), end='')\\n        for af_entry in pin_entry[1:]:\\n            print('%2d: %-*s ' % (af_entry[0], max_af_width, af_entry[1]),\\n                end='')\\n        print('')\\n\", 'af max name width 0 max af width 0 for pin entry in pins af pins af max name width max max name width len pin entry 0 for af entry in pin entry 1 max af width max max af width len af entry 1 for pin entry in pins af pins af pin name pin entry 0 print max name width pin name end for af entry in pin entry 1 print af entry 0 max af width af entry 1 end print', ''), ('pins', 18, \"def pins():\\n    mode_str = {pyb.Pin.IN: 'IN', pyb.Pin.OUT_PP: 'OUT_PP', pyb.Pin.OUT_OD:\\n        'OUT_OD', pyb.Pin.AF_PP: 'AF_PP', pyb.Pin.AF_OD: 'AF_OD', pyb.Pin.\\n        ANALOG: 'ANALOG'}\\n    pull_str = {pyb.Pin.PULL_NONE: '', pyb.Pin.PULL_UP: 'PULL_UP', pyb.Pin.\\n        PULL_DOWN: 'PULL_DOWN'}\\n    width = [0, 0, 0, 0]\\n    rows = []\\n    for pin_entry in pins_af.PINS_AF:\\n        row = []\\n        pin_name = pin_entry[0]\\n        pin = pyb.Pin(pin_name)\\n        pin_mode = pin.mode()\\n        row.append(pin_name)\\n        row.append(mode_str[pin_mode])\\n        row.append(pull_str[pin.pull()])\\n        if pin_mode == pyb.Pin.AF_PP or pin_mode == pyb.Pin.AF_OD:\\n            pin_af = pin.af()\\n            for af_entry in pin_entry[1:]:\\n                if pin_af == af_entry[0]:\\n                    af_str = '%d: %s' % (pin_af, af_entry[1])\\n                    break\\n            else:\\n                af_str = '%d' % pin_af\\n        else:\\n            af_str = ''\\n        row.append(af_str)\\n        for col in range(len(width)):\\n            width[col] = max(width[col], len(row[col]))\\n        rows.append(row)\\n    for row in rows:\\n        for col in range(len(width)):\\n            print('%-*s ' % (width[col], row[col]), end='')\\n        print('')\\n\", 'pins mode str pyb pin in in pyb pin out pp out pp pyb pin out od out od pyb pin af pp af pp pyb pin af od af od pyb pin analog analog pull str pyb pin pull none pyb pin pull up pull up pyb pin pull down pull down width 0 0 0 0 rows for pin entry in pins af pins af row pin name pin entry 0 pin pyb pin pin name pin mode pin mode row append pin name row append mode str pin mode row append pull str pin pull if pin mode pyb pin af pp or pin mode pyb pin af od pin af pin af for af entry in pin entry 1 if pin af af entry 0 af str pin af af entry 1 break else af str pin af else af str row append af str for col in range len width width col max width col len row col rows append row for row in rows for col in range len width print width col row col end print', '')] [('__init__', 25, 'def __init__(self, plugin_administrator, config=None, recursive=True):\\n    \"\"\"\\n        recursive flag: If True recursively analyze included files\\n        default flags should be edited above. Otherwise the scheduler cannot overwrite them.\\n        \"\"\"\\n    self.config = config\\n    super().__init__(plugin_administrator, config=config, recursive=\\n        recursive, no_multithread=True, plugin_path=__file__)\\n', 'init self plugin administrator config none recursive true self config config super init plugin administrator config config recursive recursive no multithread true plugin path file', 'recursive flag'), ('process_object', 35, 'def process_object(self, file_object):\\n    \"\"\"\\n        This function must be implemented by the plugin.\\n        Analysis result must be a dict stored in file_object.processed_analysis[self.NAME]\\n        If you want to propagate results to parent objects store a list of strings \\'summary\\' entry of your result dict\\n        \"\"\"\\n    if self.NAME not in file_object.processed_analysis:\\n        file_object.processed_analysis[self.NAME] = {}\\n    file_object.processed_analysis[self.NAME][\\'summary\\'] = []\\n    for passwd_regex in [\\n        b\\'[a-zA-Z][a-zA-Z0-9_-]{2,15}:[^:]?:\\\\\\\\d+:\\\\\\\\d*:[^:]*:[^:]*:[^\\\\n ]*\\',\\n        b\\'[a-zA-Z][a-zA-Z0-9_-]{2,15}:\\\\\\\\$[^\\\\\\\\$]+\\\\\\\\$[^\\\\\\\\$]+\\\\\\\\$[a-zA-Z0-9\\\\\\\\./]{16,128}\\'\\n        ]:\\n        passwd_entries = re.findall(passwd_regex, file_object.binary)\\n        if passwd_entries:\\n            result = self._generate_analysis_entry(passwd_entries)\\n            file_object.processed_analysis[self.NAME].update(result)\\n            file_object.processed_analysis[self.NAME][\\'summary\\'] += list(result\\n                .keys())\\n    return file_object\\n', 'process object self file object if self name not in file object processed analysis file object processed analysis self name file object processed analysis self name summary for passwd regex in za za 9 2 15 za za 9 2 15 za 9 16 128 passwd entries re findall passwd regex file object binary if passwd entries result self generate analysis entry passwd entries file object processed analysis self name update result file object processed analysis self name summary list result keys return file object', 'this function must be implemented by the plugin analysis result must be a dict stored in file object processed analysis self name if you want to propagate results to parent objects store a list of strings summary entry of your result dict'), ('_generate_analysis_entry', 56, \"def _generate_analysis_entry(self, passwd_entries):\\n    result = {}\\n    for entry in [e.split(b':') for e in passwd_entries]:\\n        key = entry[0].decode(encoding='utf_8', errors='replace')\\n        result[key] = {'entry': b':'.join(entry).decode(encoding='utf_8',\\n            errors='replace')}\\n        try:\\n            if entry[1][0] == ord('$'):\\n                result[key]['password-hash'] = entry[1].decode(encoding=\\n                    'utf_8', errors='replace')\\n                cracked_pw = self._crack_hash(entry, result, key)\\n                result[key]['cracked'] = True if cracked_pw else False\\n        except Exception as e:\\n            logging.error('Invalid Format: {} - {}'.format(sys.exc_info()[0\\n                ].__name__, e))\\n    return result\\n\", 'generate analysis entry self passwd entries result for entry in split for in passwd entries key entry 0 decode encoding utf 8 errors replace result key entry join entry decode encoding utf 8 errors replace try if entry 1 0 ord result key password hash entry 1 decode encoding utf 8 errors replace cracked pw self crack hash entry result key result key cracked true if cracked pw else false except exception as logging error invalid format format sys exc info 0 name return result', ''), ('_crack_hash', 70, \"def _crack_hash(self, passwd_entry, result_dict, key):\\n    with NamedTemporaryFile() as fp:\\n        fp.write(b':'.join(passwd_entry[:2]))\\n        fp.seek(0)\\n        result_dict[key]['log'] = execute_shell_command('john --wordlist={} {}'\\n            .format(self.wordlist_path, fp.name))\\n        output = execute_shell_command('john --show {}'.format(fp.name)).split(\\n            '\\\\n')\\n    if len(output) > 2:\\n        with suppress(KeyError):\\n            result_dict[key]['password'] = output[0].split(':')[1]\\n            return True\\n    return False\\n\", 'crack hash self passwd entry result dict key with namedtemporaryfile as fp fp write join passwd entry 2 fp seek 0 result dict key log execute shell command john wordlist format self wordlist path fp name output execute shell command john show format fp name split if len output 2 with suppress keyerror result dict key password output 0 split 1 return true return false', '')] [] [('__init__', 22, 'def __init__(self, logger):\\n    self.logger = logger\\n    self.client = None\\n    self.registered = False\\n    self.active = True\\n', 'init self logger self logger logger self client none self registered false self active true', ''), ('main', 28, 'def main(self, args):\\n    import aetros.const\\n    parser = argparse.ArgumentParser(formatter_class=argparse.\\n        RawTextHelpFormatter, prog=aetros.const.__prog__ + \\' authenticate\\',\\n        description=\\n        \\'Authenticates the machine with a new pair of SSH keys with a user account.\\'\\n        )\\n    parsed_args = parser.parse_args(args)\\n    home_config = read_home_config()\\n    host = home_config[\\'host\\']\\n    installed_key = get_ssh_key_for_host(host)\\n    key_exists_and_valid = False\\n    if installed_key:\\n        try:\\n            create_ssh_stream(home_config, exit_on_failure=False)\\n            key_exists_and_valid = True\\n        except Exception:\\n            pass\\n    if key_exists_and_valid:\\n        choice = six.moves.input(\\n            \\'You have already configured a valid SSH (ssk_key: \\' +\\n            installed_key + \\') for \\' + host +\\n            \"\"\".\\nWant to create a new key? The old won\\'t be removed. (y/N): \"\"\"\\n            ).lower()\\n        if choice != \\'y\\' and choice != \\'yes\\':\\n            print(\\'Aborted.\\')\\n            sys.exit(1)\\n    ssh_key = paramiko.RSAKey.generate(4096)\\n    ssh_key_private = ssh_key.key.private_bytes(serialization.Encoding.PEM,\\n        serialization.PrivateFormat.TraditionalOpenSSL, serialization.\\n        NoEncryption()).decode()\\n    ssh_key_public = \\'rsa \\' + ssh_key.get_base64()\\n    string_key = ssh_key.__str__()\\n    if not isinstance(string_key, six.binary_type):\\n        string_key = string_key.encode(\\'utf-8\\')\\n    md5 = hashlib.md5(string_key)\\n    fingerprint = md5.hexdigest()\\n    fingerprint = \\':\\'.join(a + b for a, b in zip(fingerprint[::2],\\n        fingerprint[1::2]))\\n    try:\\n        token = api.http_request(\\'machine-token\\', None, {\\'host\\': socket.\\n            getfqdn(), \\'key\\': ssh_key_public})\\n    except requests.exceptions.SSLError:\\n        sys.exit(1)\\n    print(\\n        \"Open following link and login to confirm this machine\\'s SSH key in your account.\"\\n        )\\n    print(\\'Public Key Fingerprint: MD5:\\' + fingerprint)\\n    print(\\'\\\\n   \\' + home_config[\\'url\\'] + \\'/confirm-machine/\\' + token)\\n    print(\\'\\\\nWaiting for confirmation ...\\')\\n    key_prefix = home_config[\\'host\\'] + \\'_\\'\\n    while True:\\n        time.sleep(3)\\n        response = api.http_request(\\'machine-token/authorized?id=\\' + token,\\n            method=\\'post\\')\\n        if response[\\'status\\'] == \\'confirmed\\':\\n            print(\\'\\\\n\\' + response[\\'username\\'] +\\n                \\' confirmed the public key. Test with \"aetros id\" or \"ssh git@\\'\\n                 + host + \\'\".\\')\\n            private_key_path = os.path.expanduser(\\'~/.ssh/\\' + key_prefix +\\n                response[\\'username\\'] + \\'_rsa\\')\\n            public_key_path = os.path.expanduser(\\'~/.ssh/\\' + key_prefix +\\n                response[\\'username\\'] + \\'_rsa.pub\\')\\n            if not os.path.exists(os.path.dirname(private_key_path)):\\n                os.makedirs(os.path.dirname(private_key_path))\\n            with open(private_key_path, \\'w\\') as f:\\n                f.write(ssh_key_private)\\n            with open(public_key_path, \\'w\\') as f:\\n                f.write(ssh_key_public)\\n            os.chmod(private_key_path, 384)\\n            os.chmod(public_key_path, 384)\\n            ssh_config_path = os.path.expanduser(\\'~/.ssh/config\\')\\n            if not os.path.exists(os.path.dirname(ssh_config_path)):\\n                os.makedirs(os.path.dirname(ssh_config_path))\\n            host_section = \\'host \\' + host + \\'\\\\n\\'\\n            identity_section = (\\'    IdentityFile ~/.ssh/\\' + key_prefix +\\n                response[\\'username\\'] + \\'_rsa\\\\n\\')\\n            if os.path.exists(ssh_config_path):\\n                import re\\n                regex = re.compile(\\'^host\\\\\\\\s+\\' + re.escape(host) + \\'\\\\\\\\s*\\', \\n                    re.IGNORECASE | re.MULTILINE)\\n                with open(ssh_config_path, \\'r+\\') as f:\\n                    config = f.read()\\n                    if regex.match(config):\\n                        config = regex.sub(host_section + identity_section,\\n                            config, 1)\\n                    else:\\n                        config = host_section + identity_section + config\\n                    f.seek(0)\\n                    f.write(config)\\n            else:\\n                with open(ssh_config_path, \\'w\\') as f:\\n                    f.write(host_section + identity_section)\\n            print(\\'Private key \\' + private_key_path +\\n                \\' installed in ~/.ssh/config for \\' + host + \\'.\\\\n\\')\\n            user = api.user()\\n            print(\\'Key installed of account %s (%s).\\' % (user[\\'username\\'],\\n                user[\\'name\\']))\\n            sys.exit(0)\\n        if response[\\'status\\'] == \\'expired\\':\\n            print(\\'Token expired.\\')\\n            sys.exit(1)\\n', 'main self args import aetros const parser argparse argumentparser formatter class argparse rawtexthelpformatter prog aetros const prog authenticate description authenticates the machine with new pair of ssh keys with user account parsed args parser parse args args home config read home config host home config host installed key get ssh key for host host key exists and valid false if installed key try create ssh stream home config exit on failure false key exists and valid true except exception pass if key exists and valid choice six moves input you have already configured valid ssh ssk key installed key for host want to create new key the old won be removed lower if choice and choice yes print aborted sys exit 1 ssh key paramiko rsakey generate 4096 ssh key private ssh key key private bytes serialization encoding pem serialization privateformat traditionalopenssl serialization noencryption decode ssh key public rsa ssh key get string key ssh key str if not isinstance string key six binary type string key string key encode utf 8 hashlib string key fingerprint hexdigest fingerprint join for in zip fingerprint 2 fingerprint 1 2 try token api http request machine token none host socket getfqdn key ssh key public except requests exceptions sslerror sys exit 1 print open following link and login to confirm this machine ssh key in your account print public key fingerprint fingerprint print home config url confirm machine token print nwaiting for confirmation key prefix home config host while true time sleep 3 response api http request machine token authorized id token method post if response status confirmed print response username confirmed the public key test with aetros id or ssh git host private key path os path expanduser ssh key prefix response username rsa public key path os path expanduser ssh key prefix response username rsa pub if not os path exists os path dirname private key path os makedirs os path dirname private key path with open private key path as write ssh key private with open public key path as write ssh key public os chmod private key path 384 os chmod public key path 384 ssh config path os path expanduser ssh config if not os path exists os path dirname ssh config path os makedirs os path dirname ssh config path host section host host identity section identityfile ssh key prefix response username rsa if os path exists ssh config path import re regex re compile host re escape host re ignorecase re multiline with open ssh config path as config read if regex match config config regex sub host section identity section config 1 else config host section identity section config seek 0 write config else with open ssh config path as write host section identity section print private key private key path installed in ssh config for host user api user print key installed of account user username user name sys exit 0 if response status expired print token expired sys exit 1', '')] [('sidebar_groups', 7, \"@register.inclusion_tag('groups/_sidebar_list.html')\\ndef sidebar_groups(user, exclude_group=None):\\n    groups = models.Group.objects.order_by('-score')\\n    if exclude_group:\\n        groups = groups.exclude(id=exclude_group.id)\\n    return {'groups': groups[:8], 'user': user}\\n\", 'sidebar groups user exclude group none groups models group objects order by score if exclude group groups groups exclude id exclude group id return groups groups 8 user user', '')] [('parse_args', 11, \"def parse_args():\\n    args = sys.argv[1:]\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument('-a', '--addr', default='127.0.0.1', help=\\n        'Gearman host address.')\\n    parser.add_argument('-p', '--port', default=4730, type=int, help=\\n        'Gearman port number.')\\n    return parser.parse_args(args)\\n\", 'parse args args sys argv 1 parser argparse argumentparser parser add argument addr default 127 0 0 1 help gearman host address parser add argument port default 4730 type int help gearman port number return parser parse args args', '')] [('__init__', 31, \"def __init__(self, connection=None):\\n    self.connection = connection\\n    if self.connection and hasattr(self.connection, 'region'):\\n        self.region = connection.region\\n    else:\\n        self.region = None\\n\", 'init self connection none self connection connection if self connection and hasattr self connection region self region connection region else self region none', ''), ('startElement', 38, 'def startElement(self, name, attrs, connection):\\n    return None\\n', 'startelement self name attrs connection return none', ''), ('endElement', 41, 'def endElement(self, name, value, connection):\\n    setattr(self, name, value)\\n', 'endelement self name value connection setattr self name value', ''), ('__init__', 56, 'def __init__(self, connection=None):\\n    super(TaggedEC2Object, self).__init__(connection)\\n    self.tags = TagSet()\\n', 'init self connection none super self init connection self tags tagset', ''), ('startElement', 60, \"def startElement(self, name, attrs, connection):\\n    if name == 'tagSet':\\n        return self.tags\\n    else:\\n        return None\\n\", 'startelement self name attrs connection if name tagset return self tags else return none', ''), ('add_tag', 66, 'def add_tag(self, key, value=\\'\\', dry_run=False):\\n    \"\"\"\\n        Add a tag to this object.  Tags are stored by AWS and can be used\\n        to organize and filter resources.  Adding a tag involves a round-trip\\n        to the EC2 service.\\n\\n        :type key: str\\n        :param key: The key or name of the tag being stored.\\n\\n        :type value: str\\n        :param value: An optional value that can be stored with the tag.\\n                      If you want only the tag name and no value, the\\n                      value should be the empty string.\\n        \"\"\"\\n    self.add_tags({key: value}, dry_run)\\n', 'add tag self key value dry run false self add tags key value dry run', 'add a tag to this object tags are stored by aws and can be used to organize and filter resources adding a tag involves a round trip to the ec2 service'), ('add_tags', 82, 'def add_tags(self, tags, dry_run=False):\\n    \"\"\"\\n        Add tags to this object.  Tags are stored by AWS and can be used\\n        to organize and filter resources.  Adding tags involves a round-trip\\n        to the EC2 service.\\n\\n        :type tags: dict\\n        :param tags: A dictionary of key-value pairs for the tags being stored.\\n                     If for some tags you want only the name and no value, the\\n                     corresponding value for that tag name should be an empty\\n                     string.\\n        \"\"\"\\n    status = self.connection.create_tags([self.id], tags, dry_run=dry_run)\\n    if self.tags is None:\\n        self.tags = TagSet()\\n    self.tags.update(tags)\\n', 'add tags self tags dry run false status self connection create tags self id tags dry run dry run if self tags is none self tags tagset self tags update tags', 'add tags to this object tags are stored by aws and can be used to organize and filter resources adding tags involves a round trip to the ec2 service'), ('remove_tag', 103, 'def remove_tag(self, key, value=None, dry_run=False):\\n    \"\"\"\\n        Remove a tag from this object.  Removing a tag involves a round-trip\\n        to the EC2 service.\\n\\n        :type key: str\\n        :param key: The key or name of the tag being stored.\\n\\n        :type value: str\\n        :param value: An optional value that can be stored with the tag.\\n                      If a value is provided, it must match the value currently\\n                      stored in EC2.  If not, the tag will not be removed.  If\\n                      a value of None is provided, the tag will be\\n                      unconditionally deleted.\\n                      NOTE: There is an important distinction between a value\\n                      of \\'\\' and a value of None.\\n        \"\"\"\\n    self.remove_tags({key: value}, dry_run)\\n', 'remove tag self key value none dry run false self remove tags key value dry run', 'remove a tag from this object removing a tag involves a round trip to the ec2 service'), ('remove_tags', 122, 'def remove_tags(self, tags, dry_run=False):\\n    \"\"\"\\n        Removes tags from this object.  Removing tags involves a round-trip\\n        to the EC2 service.\\n\\n        :type tags: dict\\n        :param tags: A dictionary of key-value pairs for the tags being removed.\\n                     For each key, the provided value must match the value\\n                     currently stored in EC2.  If not, that particular tag will\\n                     not be removed.  However, if a value of None is provided,\\n                     the tag will be unconditionally deleted.\\n                     NOTE: There is an important distinction between a value of\\n                     \\'\\' and a value of None.\\n        \"\"\"\\n    status = self.connection.delete_tags([self.id], tags, dry_run=dry_run)\\n    for key, value in tags.items():\\n        if key in self.tags:\\n            if value is None or value == self.tags[key]:\\n                del self.tags[key]\\n', 'remove tags self tags dry run false status self connection delete tags self id tags dry run dry run for key value in tags items if key in self tags if value is none or value self tags key del self tags key', 'removes tags from this object removing tags involves a round trip to the ec2 service')] [('get_context_data', 21, \"def get_context_data(self, **kwargs):\\n    context = super(IndexView, self).get_context_data(**kwargs)\\n    surveys = Survey.objects.filter(is_published=True)\\n    if not self.request.user.is_authenticated():\\n        surveys = surveys.filter(need_logged_user=False)\\n    context['surveys'] = surveys\\n    return context\\n\", 'get context data self kwargs context super indexview self get context data kwargs surveys survey objects filter is published true if not self request user is authenticated surveys surveys filter need logged user false context surveys surveys return context', '')] [('write_to_rom', 7, 'def write_to_rom(self, rom):\\n    last_offset = rom.size - 1\\n    if rom.is_unallocated((last_offset, last_offset)):\\n        rom.mark_allocated((last_offset, last_offset))\\n        rom[last_offset] = 197\\n', 'write to rom self rom last offset rom size 1 if rom is unallocated last offset last offset rom mark allocated last offset last offset rom last offset 197', '')] [('global_settings', 7, 'def global_settings(request):\\n    \"\"\"Adds settings to the context.\"\"\"\\n    return {\\'settings\\': settings}\\n', 'global settings request return settings settings', 'adds settings to the context'), ('i18n', 12, \"def i18n(request):\\n    return {'LANG': settings.LANGUAGE_URL_MAP.get(translation.get_language(\\n        )) or translation.get_language(), 'DIR': 'rtl' if translation.\\n        get_language_bidi() else 'ltr'}\\n\", 'request return lang settings language url map get translation get language or translation get language dir rtl if translation get language bidi else ltr', ''), ('aaq_languages', 18, 'def aaq_languages(request):\\n    \"\"\"Adds the list of AAQ languages to the context.\"\"\"\\n    return {\\'AAQ_LANGUAGES\\': QuestionLocale.objects.locales_list()}\\n', 'aaq languages request return aaq languages questionlocale objects locales list', 'adds the list of aaq languages to the context')] [('__init__', 18, 'def __init__(self):\\n    super(Formatter, self).__init__()\\n    self._origin = 0, 0\\n', 'init self super formatter self init self origin 0 0', ''), ('format', 22, 'def format(self, frame):\\n    \"\"\"\\n        Convert image to a string and return it.\\n        \"\"\"\\n    raise NotImplementedError()\\n', 'format self frame raise notimplementederror', 'convert image to a string and return it'), ('dimensions', 28, '@staticmethod\\ndef dimensions():\\n    \"\"\"\\n        Return a hint to the maximum image size suitable for this\\n        formatter.\\n        \"\"\"\\n    raise NotImplementedError()\\n', 'dimensions raise notimplementederror', 'return a hint to the maximum image size suitable for this formatter'), ('move_cursor', 36, 'def move_cursor(self, pos_x, pos_y):\\n    \"\"\"\\n        Return a the command to move the cursor to a position as a string.\\n        \"\"\"\\n    raise NotImplementedError()\\n', 'move cursor self pos pos raise notimplementederror', 'return a the command to move the cursor to a position as a string'), ('save_cursor', 42, '@staticmethod\\ndef save_cursor():\\n    \"\"\"\\n        Return a string containing a command to save the cursor\\n        position.\\n        \"\"\"\\n    raise NotImplementedError()\\n', 'save cursor raise notimplementederror', 'return a string containing a command to save the cursor position'), ('restore_cursor', 50, '@staticmethod\\ndef restore_cursor():\\n    \"\"\"\\n        Return a string containing a command to restore the cursor\\n        position.\\n        \"\"\"\\n    raise NotImplementedError()\\n', 'restore cursor raise notimplementederror', 'return a string containing a command to restore the cursor position'), ('clear_screen', 58, '@staticmethod\\ndef clear_screen():\\n    \"\"\"\\n        Return a string containing a command to clear the drawing\\n        area.\\n        \"\"\"\\n    raise NotImplementedError()\\n', 'clear screen raise notimplementederror', 'return a string containing a command to clear the drawing area'), ('color_code', 67, '@classmethod\\ndef color_code(cls, r, g, b, a=255):\\n    \"\"\"\\n        Convert the given color into the colorspace used by the\\n        formatter.\\n        \"\"\"\\n    raise NotImplementedError()\\n', 'color code cls 255 raise notimplementederror', 'convert the given color into the colorspace used by the formatter')] [] [('schema1', 9, '@pytest.fixture()\\ndef schema1(self):\\n    for stmt in [\\n        \"\"\"CREATE TABLE test1 (\\n                id INTEGER PRIMARY KEY,\\n                test1_id INTEGER REFERENCES test1\\n            );\"\"\"\\n        ]:\\n        self.database.execute(stmt)\\n    return SqliteSchema.create_from_conn(self.database.connection)\\n', 'self for stmt in create table id integer primary key id integer references self database execute stmt return sqliteschema create from conn self database connection', ''), ('test_one_table_self_referencing', 21, \"def test_one_table_self_referencing(self, schema1):\\n    table = schema1.tables[0]\\n    rows = [(table, (1, 1))]\\n    self.database.insert_rows(rows)\\n    extraction_model_data = [{'subject': [{'tables': [{'table': 'test1'}]}]\\n        }, {'relations': [{'defaults': Relation.DEFAULT_EVERYTHING}]}]\\n    self.check_launch(schema1, extraction_model_data, rows)\\n\", 'test one table self referencing self table tables 0 rows table 1 1 self database insert rows rows extraction model data subject tables table relations defaults relation default everything self check launch extraction model data rows', ''), ('schema2', 33, '@pytest.fixture()\\ndef schema2(self):\\n    for stmt in [\\n        \"\"\"CREATE TABLE managers (\\n                id INTEGER PRIMARY KEY,\\n                staff_id INTEGER NOT NULL REFERENCES staff\\n            );\"\"\"\\n        ,\\n        \"\"\"CREATE TABLE staff (\\n                id INTEGER PRIMARY KEY,\\n                manager_id INTEGER REFERENCES managers\\n            );\"\"\"\\n        ]:\\n        self.database.execute(stmt)\\n    return SqliteSchema.create_from_conn(self.database.connection)\\n', 'self for stmt in create table managers id integer primary key staff id integer not null references staff create table staff id integer primary key manager id integer references managers self database execute stmt return sqliteschema create from conn self database connection', ''), ('test_staff_manager', 49, \"def test_staff_manager(self, schema2):\\n    managers = schema2.tables[0]\\n    staff = schema2.tables[1]\\n    rows = [(staff, (1, None)), (managers, (1, 1)), (staff, (2, 1)), (staff,\\n        (3, 1))]\\n    self.database.insert_rows(rows)\\n    table = {'table': 'staff', 'column': 'id', 'values': 2}\\n    extraction_model_data = [{'subject': [{'tables': [table]}]}, {\\n        'relations': [{'defaults': Relation.DEFAULT_EVERYTHING}]}]\\n    self.check_launch(schema2, extraction_model_data, rows)\\n\", 'test staff manager self managers tables 0 staff tables 1 rows staff 1 none managers 1 1 staff 2 1 staff 3 1 self database insert rows rows table table staff column id values 2 extraction model data subject tables table relations defaults relation default everything self check launch extraction model data rows', ''), ('schema3', 68, '@pytest.fixture()\\ndef schema3(self):\\n    for stmt in [\\n        \"\"\"CREATE TABLE managers (\\n                id INTEGER PRIMARY KEY,\\n                alt_staff_id INTEGER NOT NULL REFERENCES staff(alt_id)\\n            );\"\"\"\\n        ,\\n        \"\"\"CREATE TABLE staff (\\n                id INTEGER PRIMARY KEY,\\n                alt_id INTEGER UNIQUE,\\n                manager_id INTEGER REFERENCES managers\\n            );\"\"\"\\n        ]:\\n        self.database.execute(stmt)\\n    return SqliteSchema.create_from_conn(self.database.connection)\\n', 'self for stmt in create table managers id integer primary key alt staff id integer not null references staff alt id create table staff id integer primary key alt id integer unique manager id integer references managers self database execute stmt return sqliteschema create from conn self database connection', ''), ('test_alt_staff_manager', 86, \"def test_alt_staff_manager(self, schema3):\\n    managers = schema3.tables[0]\\n    staff = schema3.tables[1]\\n    rows = [(staff, (1, 1, None)), (managers, (1, 1)), (staff, (2, 2, 1)),\\n        (staff, (3, 3, 1))]\\n    self.database.insert_rows(rows)\\n    table = {'table': 'staff', 'column': 'id', 'values': 2}\\n    extraction_model_data = [{'subject': [{'tables': [table]}]}, {\\n        'relations': [{'defaults': Relation.DEFAULT_EVERYTHING}]}]\\n    self.check_launch(schema3, extraction_model_data, rows)\\n\", 'test alt staff manager self managers tables 0 staff tables 1 rows staff 1 1 none managers 1 1 staff 2 2 1 staff 3 3 1 self database insert rows rows table table staff column id values 2 extraction model data subject tables table relations defaults relation default everything self check launch extraction model data rows', '')] [('radio', 13, '@view_config(route_name=\\'radio\\', renderer=\\'prettyjson\\')\\ndef radio(request):\\n    \"\"\"get all information about all the programs.\\n    How: scrap the website and look for the javascript links.\\n    \"\"\"\\n    menudata = get_radio_program_data()\\n    result = {\\'@context\\': \\'http://www.w3.org/ns/hydra/context.jsonld\\',\\n        \\'@id\\': request.route_url(\\'programs\\'), \\'@type\\': \\'Radio\\', \\'parent\\': {}}\\n    results = []\\n    for item in menudata:\\n        data = {\\'@id\\': request.route_url(\\'radioplaylist\\', playlist_id=item.\\n            get(\\'id\\')), \\'@type\\': \\'Radio Playlist\\', \\'title\\': safe_encode(\\n            item.get(\\'title\\')), \\'description\\': \\'\\'}\\n        if data not in results:\\n            results.append(data)\\n    result[\\'member\\'] = sorted(results, key=lambda x: x.get(\\'title\\', \\'\\').lower()\\n        )\\n    return result\\n', 'radio request menudata get radio program data result context http www org ns hydra context jsonld id request route url programs type radio parent results for item in menudata data id request route url radioplaylist playlist id item get id type radio playlist title safe encode item get title description if data not in results results append data result member sorted results key lambda get title lower return result', 'get all information about all the programs how'), ('radio_program_type_list', 44, \"@view_config(route_name='radio-program-type-list', renderer='prettyjson')\\ndef radio_program_type_list(request):\\n    result = {'@context': 'http://www.w3.org/ns/hydra/context.jsonld',\\n        '@id': request.route_url('radio-program-type-list'), '@type':\\n        'RadioTypeList', 'parent': request.route_url('home'), 'member': []}\\n    member = []\\n    categorydict = get_radio_program_types()\\n    for categoryname, categoryvalues in categorydict.items():\\n        item = {'@id': request.route_url('radio-playlist-per-type',\\n            playlist_id=categoryvalues.get('submenu', {}).get('hash', '')),\\n            '@type': 'Radio-Type-Playlist', 'parent': request.route_url(\\n            'radio-program-type-list'), 'title': categoryname}\\n        member.append(item)\\n    result['member'] = sorted(member, key=lambda x: x.get('title', ''))\\n    return result\\n\", 'radio program type list request result context http www org ns hydra context jsonld id request route url radio program type list type radiotypelist parent request route url home member member categorydict get radio program types for categoryname categoryvalues in categorydict items item id request route url radio playlist per type playlist id categoryvalues get submenu get hash type radio type playlist parent request route url radio program type list title categoryname member append item result member sorted member key lambda get title return result', ''), ('radio_stations', 71, \"@view_config(route_name='radio-stations', renderer='prettyjson')\\ndef radio_stations(request):\\n    result = {'@context': 'http://www.w3.org/ns/hydra/context.jsonld',\\n        '@id': request.route_url('radio-stations'), '@type':\\n        'RadioStationList', 'parent': request.route_url('home'), 'member': []}\\n    member = []\\n    categorydict = get_radio_stations()\\n    for categoryname, categoryvalues in categorydict.items():\\n        item = {'@id': request.route_url('radio-station-program-list',\\n            station_id=categoryvalues.get('submenu', {}).get('hash', '')),\\n            '@type': 'Radio Station Program list', 'parent': request.\\n            route_url('radio-program-type-list'), 'title': categoryname}\\n        member.append(item)\\n    result['member'] = sorted(member, key=lambda x: x.get('title', ''))\\n    return result\\n\", 'radio stations request result context http www org ns hydra context jsonld id request route url radio stations type radiostationlist parent request route url home member member categorydict get radio stations for categoryname categoryvalues in categorydict items item id request route url radio station program list station id categoryvalues get submenu get hash type radio station program list parent request route url radio program type list title categoryname member append item result member sorted member key lambda get title return result', ''), ('radioplaylist', 98, '@view_config(route_name=\\'radioplaylist\\', renderer=\\'prettyjson\\')\\ndef radioplaylist(request):\\n    \"\"\" get all the information about the given program.\\n    \"\"\"\\n    playlist_id = request.matchdict[\\'playlist_id\\']\\n    result = {\\'@context\\': \\'http://www.w3.org/ns/hydra/context.jsonld\\',\\n        \\'@id\\': request.route_url(\\'radioplaylist\\', playlist_id=playlist_id),\\n        \\'@type\\': \\'Radio Playlist\\', \\'parent\\': request.route_url(\\'radio\\')}\\n    results = []\\n    radio_programs = get_radio_programs(playlist_id)\\n    for radio_program in radio_programs:\\n        item = {\\'@id\\': \\'\\', \\'@type\\': \\'Radio Program\\', \\'@context\\':\\n            \\'http://www.w3.org/ns/hydra/context.jsonld\\', \\'title\\':\\n            radio_program.get(\\'title\\', \\'\\'), \\'date\\': radio_program.get(\\n            \\'date\\', \\'\\'), \\'duration\\': radio_program.get(\\'duration\\', \\'\\'),\\n            \\'url\\': radio_program.get(\\'url\\', \\'\\')}\\n        results.append(item)\\n    result[\\'member\\'] = sorted(results, key=lambda x: x.get(\\'date\\', \\'\\'),\\n        reverse=True)\\n    return result\\n', 'radioplaylist request playlist id request matchdict playlist id result context http www org ns hydra context jsonld id request route url radioplaylist playlist id playlist id type radio playlist parent request route url radio results radio programs get radio programs playlist id for radio program in radio programs item id type radio program context http www org ns hydra context jsonld title radio program get title date radio program get date duration radio program get duration url radio program get url results append item result member sorted results key lambda get date reverse true return result', 'get all the information about the given program'), ('radio_programs_per_type', 128, \"@view_config(route_name='radio-playlist-per-type', renderer='prettyjson')\\ndef radio_programs_per_type(request):\\n    playlist_id = request.matchdict['playlist_id']\\n    result = {'@context': 'http://www.w3.org/ns/hydra/context.jsonld',\\n        '@id': request.route_url('radioplaylist', playlist_id=playlist_id),\\n        '@type': 'Radio Program Type List', 'parent': {}}\\n    menudata = get_radio_program_data_per_type(playlist_id)\\n    results = []\\n    for item in menudata:\\n        data = {'@id': request.route_url('radioplaylist', playlist_id=item.\\n            get('id')), '@type': 'Radio Playlist', 'title': safe_encode(\\n            item.get('title')), 'description': ''}\\n        if data not in results:\\n            results.append(data)\\n    result['member'] = sorted(results, key=lambda x: x.get('title', ''))\\n    return result\\n\", 'radio programs per type request playlist id request matchdict playlist id result context http www org ns hydra context jsonld id request route url radioplaylist playlist id playlist id type radio program type list parent menudata get radio program data per type playlist id results for item in menudata data id request route url radioplaylist playlist id item get id type radio playlist title safe encode item get title description if data not in results results append data result member sorted results key lambda get title return result', ''), ('radio_programs_per_station', 153, \"@view_config(route_name='radio-station-program-list', renderer='prettyjson')\\ndef radio_programs_per_station(request):\\n    station_id = request.matchdict['station_id']\\n    result = {'@context': 'http://www.w3.org/ns/hydra/context.jsonld',\\n        '@id': request.route_url('radio-station-program-list', station_id=\\n        station_id), '@type': 'Radio Station Program List', 'parent': {}}\\n    menudata = get_radio_program_data_per_station(station_id)\\n    results = []\\n    for item in menudata:\\n        data = {'@id': request.route_url('radioplaylist', playlist_id=item.\\n            get('id')), '@type': 'Radio Playlist', 'title': safe_encode(\\n            item.get('title')), 'description': ''}\\n        if data not in results:\\n            results.append(data)\\n    result['member'] = sorted(results, key=lambda x: x.get('title', ''))\\n    return result\\n\", 'radio programs per station request station id request matchdict station id result context http www org ns hydra context jsonld id request route url radio station program list station id station id type radio station program list parent menudata get radio program data per station station id results for item in menudata data id request route url radioplaylist playlist id item get id type radio playlist title safe encode item get title description if data not in results results append data result member sorted results key lambda get title return result', '')] [('__init__', 11, 'def __init__(self, pandevice_object):\\n    self.pandevice_object = pandevice_object\\n    super(PaloAltoServiceGroup, self).__init__(name=pandevice_object.name)\\n', 'init self pandevice object self pandevice object pandevice object super paloaltoservicegroup self init name pandevice object name', '')] [('forwards', 8, \"def forwards(self, orm):\\n    db.create_table('feed_import_oauthtoken', (('id', orm[\\n        'feed_import.oauthtoken:id']), ('user', orm[\\n        'feed_import.oauthtoken:user']), ('request_token', orm[\\n        'feed_import.oauthtoken:request_token']), ('request_token_secret',\\n        orm['feed_import.oauthtoken:request_token_secret']), (\\n        'access_token', orm['feed_import.oauthtoken:access_token']), (\\n        'access_token_secret', orm[\\n        'feed_import.oauthtoken:access_token_secret'])))\\n    db.send_create_signal('feed_import', ['OAuthToken'])\\n\", 'forwards self orm db create table feed import oauthtoken id orm feed import oauthtoken id user orm feed import oauthtoken user request token orm feed import oauthtoken request token request token secret orm feed import oauthtoken request token secret access token orm feed import oauthtoken access token access token secret orm feed import oauthtoken access token secret db send create signal feed import oauthtoken', ''), ('backwards', 23, \"def backwards(self, orm):\\n    db.delete_table('feed_import_oauthtoken')\\n\", 'backwards self orm db delete table feed import oauthtoken', '')] [('check_permission', 20, 'def check_permission(user, institution, post, comment):\\n    \"\"\"Check the user permission to delete comment.\"\"\"\\n    is_not_post_author = post.author != user.key\\n    is_not_admin = institution.admin != user.key\\n    is_not_comment_author = comment.get(\\'author_key\\') != user.key.urlsafe()\\n    Utils._assert(is_not_post_author and is_not_admin and\\n        is_not_comment_author, \\'User not allowed to remove comment\\',\\n        NotAuthorizedException)\\n', 'check permission user institution post comment is not post author post author user key is not admin institution admin user key is not comment author comment get author key user key urlsafe utils assert is not post author and is not admin and is not comment author user not allowed to remove comment notauthorizedexception', 'check the user permission to delete comment'), ('get', 34, '@json_response\\n@login_required\\ndef get(self, user, post_key):\\n    \"\"\"Handle Get Comments requests.\"\"\"\\n    post = ndb.Key(urlsafe=post_key).get()\\n    post.make_comments()\\n    self.response.write(json.dumps(post.comments))\\n', 'get self user post key post ndb key urlsafe post key get post make comments self response write json dumps post comments', 'handle get comments requests'), ('post', 42, '@json_response\\n@login_required\\ndef post(self, user, post_key):\\n    \"\"\"Handle Post Comments requests.\"\"\"\\n    body = json.loads(self.request.body)\\n    comment_data = body[\\'commentData\\']\\n    post = ndb.Key(urlsafe=post_key).get()\\n    Utils._assert(post.state == \\'deleted\\', \\'This post has been deleted\\',\\n        EntityException)\\n    comment = Comment.create(comment_data, user)\\n    post.add_comment(comment)\\n    entity_type = \\'COMMENT\\'\\n    user_is_the_post_author = post.author == user.key\\n    if not user_is_the_post_author:\\n        params = {\\'receiver_key\\': post.author.urlsafe(), \\'sender_key\\': user\\n            .key.urlsafe(), \\'entity_key\\': post.key.urlsafe(), \\'entity_type\\':\\n            entity_type, \\'current_institution\\': user.current_institution.\\n            urlsafe(), \\'sender_institution_key\\': post.institution.urlsafe()}\\n        enqueue_task(\\'post-notification\\', params)\\n    self.response.write(json.dumps(Utils.toJson(comment)))\\n', 'post self user post key body json loads self request body comment data body commentdata post ndb key urlsafe post key get utils assert post state deleted this post has been deleted entityexception comment comment create comment data user post add comment comment entity type comment user is the post author post author user key if not user is the post author params receiver key post author urlsafe sender key user key urlsafe entity key post key urlsafe entity type entity type current institution user current institution urlsafe sender institution key post institution urlsafe enqueue task post notification params self response write json dumps utils tojson comment', 'handle post comments requests'), ('delete', 71, '@json_response\\n@login_required\\n@ndb.transactional(xg=True)\\ndef delete(self, user, post_key, comment_id):\\n    \"\"\"Handle Delete Comments requests.\"\"\"\\n    post = ndb.Key(urlsafe=post_key).get()\\n    institution = post.institution.get()\\n    Utils._assert(institution.state == \\'inactive\\',\\n        \\'The institution has been deleted\\', NotAuthorizedException)\\n    Utils._assert(post.state == \\'deleted\\',\\n        \\'Can not delete comment in deleted post\\', NotAuthorizedException)\\n    comment = post.get_comment(comment_id)\\n    has_activity = len(comment.get(\\'replies\\')) > 0 or len(comment.get(\\'likes\\')\\n        ) > 0\\n    Utils._assert(has_activity, \"Comment with activity can\\'t be removed\",\\n        NotAuthorizedException)\\n    Utils._assert(has_activity, \"Comment with activity can\\'t be removed\",\\n        NotAuthorizedException)\\n    check_permission(user, institution, post, comment)\\n    post.remove_comment(comment)\\n    self.response.write(json.dumps(comment))\\n', 'delete self user post key comment id post ndb key urlsafe post key get institution post institution get utils assert institution state inactive the institution has been deleted notauthorizedexception utils assert post state deleted can not delete comment in deleted post notauthorizedexception comment post get comment comment id has activity len comment get replies 0 or len comment get likes 0 utils assert has activity comment with activity can be removed notauthorizedexception utils assert has activity comment with activity can be removed notauthorizedexception check permission user institution post comment post remove comment comment self response write json dumps comment', 'handle delete comments requests')] [('client', 13, \"@pytest.fixture(scope='module')\\ndef client():\\n    return ibis.bigquery.connect('ibis-gbq', 'testing')\\n\", 'client return ibis bigquery connect ibis gbq testing', ''), ('alltypes', 18, \"@pytest.fixture(scope='module')\\ndef alltypes(client):\\n    t = client.table('functional_alltypes')\\n    expr = t[t.bigint_col.isin([10, 20])].limit(10)\\n    return expr\\n\", 'alltypes client client table functional alltypes expr bigint col isin 10 20 limit 10 return expr', ''), ('df', 25, \"@pytest.fixture(scope='module')\\ndef df(alltypes):\\n    return alltypes.execute()\\n\", 'df alltypes return alltypes execute', ''), ('test_udf', 30, \"def test_udf(client, alltypes, df):\\n\\n    @udf(input_type=[dt.double, dt.double], output_type=dt.double)\\n    def my_add(a, b):\\n        return a + b\\n    expr = my_add(alltypes.double_col, alltypes.double_col)\\n    result = expr.execute()\\n    assert not result.empty\\n    expected = (df.double_col + df.double_col).rename('tmp')\\n    tm.assert_series_equal(result.value_counts().sort_index(), expected.\\n        value_counts().sort_index())\\n\", 'test udf client alltypes df udf input type dt double dt double output type dt double def my add return expr my add alltypes double col alltypes double col result expr execute assert not result empty expected df double col df double col rename tmp tm assert series equal result value counts sort index expected value counts sort index', ''), ('test_udf_with_struct', 46, 'def test_udf_with_struct(client, alltypes, df):\\n\\n    @udf(input_type=[dt.double, dt.double], output_type=dt.Struct.\\n        from_tuples([(\\'width\\', dt.double), (\\'height\\', dt.double)]))\\n    def my_struct_thing(a, b):\\n\\n\\n        class Rectangle:\\n\\n            def __init__(self, width, height):\\n                self.width = width\\n                self.height = height\\n        return Rectangle(a, b)\\n    assert my_struct_thing.js == \"\"\"CREATE TEMPORARY FUNCTION my_struct_thing(a FLOAT64, b FLOAT64)\\nRETURNS STRUCT<width FLOAT64, height FLOAT64>\\nLANGUAGE js AS \"\"\\\\\"\\n\\'use strict\\';\\nfunction my_struct_thing(a, b) {\\n    class Rectangle {\\n        constructor(width, height) {\\n            this.width = width;\\n            this.height = height;\\n        }\\n    }\\n    return (new Rectangle(a, b));\\n}\\nreturn my_struct_thing(a, b);\\n\"\"\\\\\";\"\"\"\\n    expr = my_struct_thing(alltypes.double_col, alltypes.double_col)\\n    result = expr.execute()\\n    assert not result.empty\\n    expected = pd.Series([{\\'width\\': c, \\'height\\': c} for c in df.double_col],\\n        name=\\'tmp\\')\\n    tm.assert_series_equal(result, expected)\\n', 'test udf with struct client alltypes df udf input type dt double dt double output type dt struct from tuples width dt double height dt double def my struct thing class rectangle def init self width height self width width self height height return rectangle assert my struct thing js create temporary function my struct thing returns struct width height language js as use strict function my struct thing class rectangle constructor width height this width width this height height return new rectangle return my struct thing expr my struct thing alltypes double col alltypes double col result expr execute assert not result empty expected pd series width height for in df double col name tmp tm assert series equal result expected', ''), ('test_udf_compose', 89, \"def test_udf_compose(client, alltypes, df):\\n\\n    @udf([dt.double], dt.double)\\n    def add_one(x):\\n        return x + 1.0\\n\\n    @udf([dt.double], dt.double)\\n    def times_two(x):\\n        return x * 2.0\\n    t = alltypes\\n    expr = times_two(add_one(t.double_col))\\n    result = expr.execute()\\n    expected = ((df.double_col + 1.0) * 2.0).rename('tmp')\\n    tm.assert_series_equal(result, expected)\\n\", 'test udf compose client alltypes df udf dt double dt double def add one return 1 0 udf dt double dt double def times two return 2 0 alltypes expr times two add one double col result expr execute expected df double col 1 0 2 0 rename tmp tm assert series equal result expected', '')] [('test_artifact_get_path', 7, 'def test_artifact_get_path(self):\\n    link = [\\'</test/artifact/test>; rel=\"self\"; title=\"{0}\"\\'.format(\\n        LinkTitle.ARTIFACT),\\n        \\'</test/artifact/test/_meta>; rel=\"related\"; title=\"{0}\"\\'.format(\\n        LinkTitle.METADATA)]\\n    self.route_tester.artifact().route_params(bucket_name=\\'test\\', path=\\'test\\'\\n        ).expect(200, \\'hello world\\', headers={\\'Link\\': link}).get(headers=\\n        self.auth)\\n', 'test artifact get path self link test artifact test rel self title 0 format linktitle artifact test artifact test meta rel related title 0 format linktitle metadata self route tester artifact route params bucket name test path test expect 200 hello world headers link link get headers self auth', ''), ('test_artifact_no_permissions', 18, 'def test_artifact_no_permissions(self):\\n    \"\"\"\\n            This tests to make sure that if we did not give permissions to\\n            a particular path, it will still give a 401 Unauthorized.\\n        \"\"\"\\n    self.route_tester.artifact().route_params(bucket_name=\\'test\\', path=\\n        \\'dir/test\\').expect(401, self.RESPONSE_401).get(headers=self.auth)\\n', 'test artifact no permissions self self route tester artifact route params bucket name test path dir test expect 401 self response 401 get headers self auth', 'this tests to make sure that if we did not give permissions to a particular path it will still give a 401 unauthorized'), ('test_artifact_get_none', 29, \"def test_artifact_get_none(self):\\n    self.route_tester.artifact().route_params(bucket_name='test', path='nada'\\n        ).expect(404, self.RESPONSE_404).get(headers=self.auth)\\n\", 'test artifact get none self self route tester artifact route params bucket name test path nada expect 404 self response 404 get headers self auth', ''), ('test_no_permission_file', 36, \"def test_no_permission_file(self):\\n    self.route_tester.artifact().route_params(bucket_name='test', path=\\n        'billy-bob-thorton').expect(401, self.RESPONSE_401).get(headers={\\n        'Authorization': 'bkdjfaojdklfjakdjHELLOWORLDlajdfjkadjok'})\\n\", 'test no permission file self self route tester artifact route params bucket name test path billy bob thorton expect 401 self response 401 get headers authorization bkdjfaojdklfjakdjhelloworldlajdfjkadjok', ''), ('artifact_get_list', 43, 'def artifact_get_list(self, path):\\n    self.route_tester.artifact().route_params(bucket_name=\\'test\\', path=path\\n        ).expect(204, headers={\\'Link\\':\\n        \\'</test/artifact/dir/dir2/dir3/dir4/test5>; rel=\"item\"; title=\"{0}\"\\'\\n        .format(LinkTitle.ARTIFACT)}).get(headers=self.auth)\\n', 'artifact get list self path self route tester artifact route params bucket name test path path expect 204 headers link test artifact dir rel item title 0 format linktitle artifact get headers self auth', ''), ('test_artifact_get_artifact_list', 54, \"def test_artifact_get_artifact_list(self):\\n    self.artifact_get_list('dir/dir2/dir3/dir4/')\\n\", 'test artifact get artifact list self self artifact get list dir', ''), ('test_artifact_get_artifact_list_no_trailing_slash', 57, \"def test_artifact_get_artifact_list_no_trailing_slash(self):\\n    self.artifact_get_list('dir/dir2/dir3/dir4')\\n\", 'test artifact get artifact list no trailing slash self self artifact get list dir', ''), ('test_artifact_get_artifact_list_all', 60, 'def test_artifact_get_artifact_list_all(self):\\n    self.route_tester.artifact().route_params(bucket_name=\\'test\\', path=\\'\\'\\n        ).expect(204, headers={\\'Link\\': [\\n        \\'</test/artifact/empty>; rel=\"item\"; title=\"{0}\"\\'.format(LinkTitle.\\n        ARTIFACT), \\'</test/artifact/test>; rel=\"item\"; title=\"{0}\"\\'.format(\\n        LinkTitle.ARTIFACT),\\n        \\'</test/artifact/dir/>; rel=\"collection\"; title=\"{0}\"\\'.format(\\n        LinkTitle.ARTIFACT_LIST),\\n        \\'</test/artifact/this/>; rel=\"collection\"; title=\"{0}\"\\'.format(\\n        LinkTitle.ARTIFACT_LIST)]}).get(headers=self.auth)\\n', 'test artifact get artifact list all self self route tester artifact route params bucket name test path expect 204 headers link test artifact empty rel item title 0 format linktitle artifact test artifact test rel item title 0 format linktitle artifact test artifact dir rel collection title 0 format linktitle artifact list test artifact this rel collection title 0 format linktitle artifact list get headers self auth', ''), ('artifact_head_request', 74, \"def artifact_head_request(self, path, status_code, headers=None):\\n    self.route_tester.artifact().route_params(bucket_name='test', path=path\\n        ).expect(status_code, headers=headers).head(headers=self.auth)\\n\", 'artifact head request self path status code headers none self route tester artifact route params bucket name test path path expect status code headers headers head headers self auth', ''), ('test_artifact_head_from_root', 81, 'def test_artifact_head_from_root(self):\\n    self.artifact_head_request(\\'\\', 204, headers={\\'Link\\': [\\n        \\'</test/artifact/empty>; rel=\"item\"; title=\"{0}\"\\'.format(LinkTitle.\\n        ARTIFACT), \\'</test/artifact/test>; rel=\"item\"; title=\"{0}\"\\'.format(\\n        LinkTitle.ARTIFACT),\\n        \\'</test/artifact/dir/>; rel=\"collection\"; title=\"{0}\"\\'.format(\\n        LinkTitle.ARTIFACT_LIST),\\n        \\'</test/artifact/this/>; rel=\"collection\"; title=\"{0}\"\\'.format(\\n        LinkTitle.ARTIFACT_LIST)]})\\n', 'test artifact head from root self self artifact head request 204 headers link test artifact empty rel item title 0 format linktitle artifact test artifact test rel item title 0 format linktitle artifact test artifact dir rel collection title 0 format linktitle artifact list test artifact this rel collection title 0 format linktitle artifact list', ''), ('test_head_request_on_artifact', 91, 'def test_head_request_on_artifact(self):\\n    self.artifact_head_request(\\'/test\\', 204, headers={\\'Link\\': [\\n        \\'</test/artifact/test>; rel=\"self\"; title=\"{0}\"\\'.format(LinkTitle.\\n        ARTIFACT),\\n        \\'</test/artifact/test/_meta>; rel=\"related\"; title=\"{0}\"\\'.format(\\n        LinkTitle.METADATA)]})\\n', 'test head request on artifact self self artifact head request test 204 headers link test artifact test rel self title 0 format linktitle artifact test artifact test meta rel related title 0 format linktitle metadata', ''), ('test_head_request_no_artifact', 99, \"def test_head_request_no_artifact(self):\\n    self.artifact_head_request('/LOL_DOES-nOtExist', 404)\\n\", 'test head request no artifact self self artifact head request lol does notexist 404', ''), ('test_head_no_permissions', 102, \"def test_head_no_permissions(self):\\n    self.artifact_head_request('dir/test', 401)\\n\", 'test head no permissions self self artifact head request dir test 401', ''), ('test_artifact_upload', 105, 'def test_artifact_upload(self):\\n    self.route_tester.artifact().route_params(bucket_name=\\'test\\', path=\\'test-2\\'\\n        ).expect(201, headers={\\'Link\\': [\\n        \\'</test/artifact/test-2>; rel=\"self\"; title=\"{0}\"\\'.format(LinkTitle\\n        .ARTIFACT),\\n        \\'</test/artifact/test-2/_meta>; rel=\"related\"; title=\"{0}\"\\'.format(\\n        LinkTitle.METADATA)]}).post(data={\\'file\\': (StringIO(\\'file contents\\'\\n        ), \\'test.txt\\')}, headers=self.auth)\\n', 'test artifact upload self self route tester artifact route params bucket name test path test 2 expect 201 headers link test artifact test 2 rel self title 0 format linktitle artifact test artifact test 2 meta rel related title 0 format linktitle metadata post data file stringio file contents test txt headers self auth', ''), ('test_artifact_upload_and_immediate_search_with_bucket_alias', 116, 'def test_artifact_upload_and_immediate_search_with_bucket_alias(self):\\n    self.route_tester.artifact().route_params(bucket_name=\\'b2\\', path=\\n        \\'nick-drake\\').expect(201, headers={\\'Link\\': [\\n        \\'</b2/artifact/nick-drake>; rel=\"self\"; title=\"{0}\"\\'.format(\\n        LinkTitle.ARTIFACT),\\n        \\'</b2/artifact/nick-drake/_meta>; rel=\"related\"; title=\"{0}\"\\'.\\n        format(LinkTitle.METADATA)]}).post(data={\\'file\\': (StringIO(\\n        \\'file contents\\'), \\'nick-drake.txt\\')}, headers=self.auth)\\n    self.search_wrapper.refresh_index()\\n    self.route_tester.search().route_params(bucket_name=\\'b2\\', path=\\'\\').expect(\\n        204, headers={\\'Link\\': [\\n        \\'</b2/artifact/nick-drake>; rel=\"item\"; title=\"{0}\"\\'.format(\\n        LinkTitle.ARTIFACT)]}).post(data={\\'search\\':\\n        \\'artifactPath=/b2/artifact/nick-drake\\'}, headers=self.auth)\\n', 'test artifact upload and immediate search with bucket alias self self route tester artifact route params bucket name path nick drake expect 201 headers link artifact nick drake rel self title 0 format linktitle artifact artifact nick drake meta rel related title 0 format linktitle metadata post data file stringio file contents nick drake txt headers self auth self search wrapper refresh index self route tester search route params bucket name path expect 204 headers link artifact nick drake rel item title 0 format linktitle artifact post data search artifactpath artifact nick drake headers self auth', ''), ('test_artifact_upload_no_permissions', 139, \"def test_artifact_upload_no_permissions(self):\\n    self.route_tester.artifact().route_params(bucket_name='test', path=\\n        'dir/test').expect(401, self.RESPONSE_401).post(data={'file': (\\n        StringIO('file contents'), 'test.txt')}, headers=self.auth)\\n\", 'test artifact upload no permissions self self route tester artifact route params bucket name test path dir test expect 401 self response 401 post data file stringio file contents test txt headers self auth', ''), ('test_artifact_upload_existing_artifact', 145, \"def test_artifact_upload_existing_artifact(self):\\n    self.route_tester.artifact().route_params(bucket_name='test', path='test'\\n        ).expect(403, self.RESPONSE_DUPLICATE).post(data={'file': (StringIO\\n        ('file contents'), 'test.txt')}, headers=self.auth)\\n\", 'test artifact upload existing artifact self self route tester artifact route params bucket name test path test expect 403 self response duplicate post data file stringio file contents test txt headers self auth', ''), ('test_illegal_artifact_upload', 151, \"def test_illegal_artifact_upload(self):\\n    self.route_tester.artifact().route_params(bucket_name='test', path='_test'\\n        ).expect(403, self.RESPONSE_INVALID_NAME).post(data={'file': (\\n        StringIO('file contents'), 'test.txt')}, headers=self.auth)\\n\", 'test illegal artifact upload self self route tester artifact route params bucket name test path test expect 403 self response invalid name post data file stringio file contents test txt headers self auth', ''), ('test_bucket_configuration_doesnt_exist', 158, \"def test_bucket_configuration_doesnt_exist(self):\\n    self.route_tester.artifact().route_params(bucket_name=\\n        'lol-this-doesnt-exist', path='hello/there').expect(404, self.\\n        RESPONSE_404).get(headers=self.auth)\\n\", 'test bucket configuration doesnt exist self self route tester artifact route params bucket name lol this doesnt exist path hello there expect 404 self response 404 get headers self auth', ''), ('test_bucket_doesnt_exist', 165, \"def test_bucket_doesnt_exist(self):\\n    self.route_tester.artifact().route_params(bucket_name=\\n        'thisBucketDoesntExistLol', path='hello/there').expect(500, self.\\n        response_500()).get(headers=self.auth)\\n\", 'test bucket doesnt exist self self route tester artifact route params bucket name thisbucketdoesntexistlol path hello there expect 500 self response 500 get headers self auth', ''), ('test_private_artifact', 172, \"def test_private_artifact(self):\\n    self.route_tester.artifact().route_params(bucket_name='test', path=\\n        'dir/dir2/_secret').expect(403, self.RESPONSE_INVALID_NAME).get(headers\\n        =self.auth)\\n\", 'test private artifact self self route tester artifact route params bucket name test path dir secret expect 403 self response invalid name get headers self auth', ''), ('test_non_private_artifact', 179, \"def test_non_private_artifact(self):\\n    self.route_tester.artifact().route_params(bucket_name='test', path=\\n        'dir/dir2/not_secret').expect(200, 'You can see this though').get(\\n        headers=self.auth)\\n\", 'test non private artifact self self route tester artifact route params bucket name test path dir not secret expect 200 you can see this though get headers self auth', ''), ('test_bucket_no_ref', 187, \"def test_bucket_no_ref(self):\\n    self.route_tester.artifact().route_params(bucket_name='bucket2', path=\\n        'nick-drake').expect(404, self.RESPONSE_404).get(headers=self.auth)\\n\", 'test bucket no ref self self route tester artifact route params bucket name path nick drake expect 404 self response 404 get headers self auth', '')] [('getStreamURL', 15, \"def getStreamURL(channel):\\n    url = jsonapi + channel\\n    res = http.get(url)\\n    streamerinfo = http.json(res)\\n    if not any('media' in s for s in streamerinfo):\\n        print('User offline or invalid')\\n        return\\n    else:\\n        streamdata = streamerinfo['media']\\n        streamurl = 'rtmp://' + streamdata['host'] + streamdata['app'\\n            ] + '/' + streamdata['stream']\\n    return streamurl\\n\", 'getstreamurl channel url jsonapi channel res http get url streamerinfo http json res if not any media in for in streamerinfo print user offline or invalid return else streamdata streamerinfo media streamurl rtmp streamdata host streamdata app streamdata stream return streamurl', ''), ('can_handle_url', 34, '@classmethod\\ndef can_handle_url(self, url):\\n    return _url_re.match(url)\\n', 'can handle url self url return url re match url', ''), ('_get_streams', 38, \"def _get_streams(self):\\n    match = _url_re.match(self.url)\\n    channel = match.group('channel')\\n    streamurl = getStreamURL(channel)\\n    if not streamurl:\\n        return\\n    streams = {}\\n    streams['live'] = RTMPStream(self.session, {'rtmp': streamurl, 'live':\\n        True})\\n    return streams\\n\", 'get streams self match url re match self url channel match group channel streamurl getstreamurl channel if not streamurl return streams streams live rtmpstream self session rtmp streamurl live true return streams', '')] [('str_in_dict_keys', 70, 'def str_in_dict_keys(s, d):\\n    for k in d:\\n        if s in k:\\n            return True\\n    return False\\n', 'str in dict keys for in if in return true return false', ''), ('_combine_date_time', 77, 'def _combine_date_time(d, t):\\n    if d is not None and t is not None:\\n        return datetime(d.year, d.month, d.day, t.hour, t.minute, t.second)\\n    return None\\n', 'combine date time if is not none and is not none return datetime year month day hour minute second return none', ''), ('_parse', 83, 'def _parse(dt):\\n    \"\"\"\\n    parse input to datetime\\n    \"\"\"\\n    try:\\n        if isinstance(dt, datetime):\\n            return dt\\n        if isinstance(dt, date):\\n            return _combine_date_time(dt, time(0, 0, 0))\\n        if isinstance(dt, time):\\n            return _combine_date_time(date.today(), dt)\\n        if isinstance(dt, (int, float)):\\n            return datetime.fromtimestamp(dt)\\n        if isinstance(dt, (str, bytes)):\\n            return parser.parse(dt, default=datetime.now())\\n        return None\\n    except ValueError:\\n        return None\\n', 'parse dt try if isinstance dt datetime return dt if isinstance dt date return combine date time dt time 0 0 0 if isinstance dt time return combine date time date today dt if isinstance dt int float return datetime fromtimestamp dt if isinstance dt str bytes return parser parse dt default datetime now return none except valueerror return none', 'parse input to datetime'), ('_format_relativedelta', 103, \"def _format_relativedelta(rdelta, full=False):\\n    if not isinstance(rdelta, relativedelta.relativedelta):\\n        raise ValueError('rdelta must be a relativedelta instance')\\n    keys = 'years', 'months', 'days', 'hours', 'minutes', 'seconds'\\n    result = []\\n    flag = None\\n    for k, v in rdelta.__dict__.items():\\n        if k in keys and v != 0:\\n            if flag is None:\\n                flag = True if v > 0 else False\\n            key = k\\n            if v == 1:\\n                key = key[:-1]\\n            if not full:\\n                return flag, '{} {}'.format(abs(v), key)\\n            else:\\n                result.append('{} {}'.format(abs(v), key))\\n    if len(result) == 0:\\n        return None, 'just now'\\n    if len(result) > 1:\\n        temp = result.pop()\\n        result = '{} and {}'.format(', '.join(result), temp)\\n    else:\\n        result = result[0]\\n    return flag, result\\n\", 'format relativedelta rdelta full false if not isinstance rdelta relativedelta relativedelta raise valueerror rdelta must be relativedelta instance keys years months days hours minutes seconds result flag none for in rdelta dict items if in keys and 0 if flag is none flag true if 0 else false key if 1 key key 1 if not full return flag format abs key else result append format abs key if len result 0 return none just now if len result 1 temp result pop result and format join result temp else result result 0 return flag result', ''), ('_format_time_ago', 131, \"def _format_time_ago(dt, now=None, full=False, ago_in=False):\\n    if not isinstance(dt, timedelta):\\n        if now is None:\\n            now = timezone.localtime(timezone=timezone.get_fixed_timezone(-\\n                int(t.timezone / 60)))\\n        dt = _parse(dt)\\n        now = _parse(now)\\n        if dt is None:\\n            raise ValueError(\\n                'The parameter `dt` should be datetime timedelta, or datetime formatted string.'\\n                )\\n        if now is None:\\n            raise ValueError(\\n                'the parameter `now` should be datetime, or datetime formatted string.'\\n                )\\n        result = relativedelta.relativedelta(dt, now)\\n        flag, result = _format_relativedelta(result, full)\\n        if ago_in and flag is not None:\\n            result = 'in {}'.format(result) if flag else '{} ago'.format(result\\n                )\\n        return result\\n\", 'format time ago dt now none full false ago in false if not isinstance dt timedelta if now is none now timezone localtime timezone timezone get fixed timezone int timezone 60 dt parse dt now parse now if dt is none raise valueerror the parameter dt should be datetime timedelta or datetime formatted string if now is none raise valueerror the parameter now should be datetime or datetime formatted string result relativedelta relativedelta dt now flag result format relativedelta result full if ago in and flag is not none result in format result if flag else ago format result return result', ''), ('_format_dt', 152, \"def _format_dt(dt, format='default'):\\n    if not dt:\\n        return None\\n    format_lowered = format.lower()\\n    if format_lowered == 'default':\\n        return dt.strftime(DEFAULT_DATE_FORMAT)\\n    if format_lowered == 'time ago':\\n        return _format_time_ago(dt, full=True, ago_in=True)\\n    if format_lowered == 'iso':\\n        return dt.strftime('%Y-%b-%dT%H:%M:%S')\\n    if format_lowered in ('js', 'javascript'):\\n        return dt.strftime('%a %b %d %Y %H:%M:%S')\\n    if format in FORMATS_MAP:\\n        return dt.strftime(FORMATS_MAP[format])\\n    else:\\n        temp_format = ''\\n        translate_format_list = []\\n        for char in format:\\n            if not char.isalpha():\\n                if temp_format != '':\\n                    translate_format_list.append(FORMATS_MAP.get(\\n                        temp_format, ''))\\n                    temp_format = ''\\n                translate_format_list.append(char)\\n            elif str_in_dict_keys('{}{}'.format(temp_format, char), FORMATS_MAP\\n                ):\\n                temp_format = '{}{}'.format(temp_format, char)\\n            else:\\n                if temp_format != '':\\n                    if temp_format in FORMATS_MAP:\\n                        translate_format_list.append(FORMATS_MAP.get(\\n                            temp_format, ''))\\n                    else:\\n                        return None\\n                if str_in_dict_keys(char, FORMATS_MAP):\\n                    temp_format = char\\n                else:\\n                    return None\\n        if temp_format != '':\\n            if temp_format in FORMATS_MAP:\\n                translate_format_list.append(FORMATS_MAP.get(temp_format, ''))\\n            else:\\n                return None\\n        format_result = ''.join(translate_format_list)\\n        if format_result:\\n            return dt.strftime(''.join(translate_format_list))\\n        return None\\n\", 'format dt dt format default if not dt return none format lowered format lower if format lowered default return dt strftime default date format if format lowered time ago return format time ago dt full true ago in true if format lowered iso return dt strftime dt if format lowered in js javascript return dt strftime if format in formats map return dt strftime formats map format else temp format translate format list for char in format if not char isalpha if temp format translate format list append formats map get temp format temp format translate format list append char elif str in dict keys format temp format char formats map temp format format temp format char else if temp format if temp format in formats map translate format list append formats map get temp format else return none if str in dict keys char formats map temp format char else return none if temp format if temp format in formats map translate format list append formats map get temp format else return none format result join translate format list if format result return dt strftime join translate format list return none', ''), ('get_args', 212, \"@staticmethod\\ndef get_args():\\n    return {'format': GraphQLArgument(type=GraphQLString, description=\\n        'A format given by dateutil module')}\\n\", 'get args return format graphqlargument type graphqlstring description format given by dateutil module', ''), ('resolve', 221, \"@staticmethod\\ndef resolve(value, directive, root, info, **kwargs):\\n    format_argument = [arg for arg in directive.arguments if arg.name.value ==\\n        'format']\\n    format_argument = format_argument[0] if len(format_argument) > 0 else None\\n    custom_format = (format_argument.value.value if format_argument else\\n        'default')\\n    dt = _parse(value)\\n    try:\\n        result = _format_dt(dt, custom_format)\\n        if isinstance(value, six.string_types):\\n            return result or value\\n        return CustomDateFormat(result or 'INVALID FORMAT STRING')\\n    except ValueError:\\n        return CustomDateFormat('INVALID FORMAT STRING')\\n\", 'resolve value directive root info kwargs format argument arg for arg in directive arguments if arg name value format format argument format argument 0 if len format argument 0 else none custom format format argument value value if format argument else default dt parse value try result format dt dt custom format if isinstance value six string types return result or value return customdateformat result or invalid format string except valueerror return customdateformat invalid format string', '')] [('clear', 50, 'def clear(stream):\\n    stream.truncate(0)\\n    stream.seek(0)\\n', 'clear stream stream truncate 0 stream seek 0', ''), ('test_pub_ls', 54, 'def test_pub_ls():\\n    tr = TransportMock()\\n    outstream = io.StringIO()\\n    tlm = Application(transport=tr, stdout=outstream)\\n    tlm.onecmd(\\'ls\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'pub --f32 topicA 0.4\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue(\\n        ) == \"Published on topic \\'topicA\\' : 0.4 [float32]\\\\n\"\\n    clear(outstream)\\n    tlm.onecmd(\\'ls\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'topicA\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'pub --f32 topicB 0.4\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue(\\n        ) == \"Published on topic \\'topicB\\' : 0.4 [float32]\\\\n\"\\n    clear(outstream)\\n    tlm.onecmd(\\'ls\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'topicA\\\\ntopicB\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'pub --i16 topicC 0.0\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \"Published on topic \\'topicC\\' : 0 [int16]\\\\n\"\\n    clear(outstream)\\n    tlm.onecmd(\\'pub --i16 topicC 0.1\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue(\\n        ) == \\'Aborted : Wrote decimal number (0.1) with integer flag.\\\\n\\'\\n    clear(outstream)\\n', 'test pub ls tr transportmock outstream io stringio tlm application transport tr stdout outstream tlm onecmd ls tlm runner update assert outstream getvalue clear outstream tlm onecmd pub topica 0 4 tlm runner update assert outstream getvalue published on topic topica 0 4 clear outstream tlm onecmd ls tlm runner update assert outstream getvalue topica clear outstream tlm onecmd pub topicb 0 4 tlm runner update assert outstream getvalue published on topic topicb 0 4 clear outstream tlm onecmd ls tlm runner update assert outstream getvalue topica ntopicb clear outstream tlm onecmd pub topicc 0 0 tlm runner update assert outstream getvalue published on topic topicc 0 clear outstream tlm onecmd pub topicc 0 1 tlm runner update assert outstream getvalue aborted wrote decimal number 0 1 with integer flag clear outstream', ''), ('test_connect_fail', 101, \"def test_connect_fail():\\n    tr = TransportMock()\\n    outstream = io.StringIO()\\n    tlm = Application(transport=tr, stdout=outstream)\\n    tlm.onecmd('serial com123')\\n    tlm.runner.update()\\n    assert outstream.getvalue(\\n        ) == 'Failed to connect to com123 at 9600 (bauds).\\\\n'\\n    clear(outstream)\\n    tlm.onecmd('serial com123 -b 115200')\\n    tlm.runner.update()\\n    assert outstream.getvalue(\\n        ) == 'Failed to connect to com123 at 115200 (bauds).\\\\n'\\n    clear(outstream)\\n    tlm.onecmd('serial com123 --bauds 57600')\\n    tlm.runner.update()\\n    assert outstream.getvalue(\\n        ) == 'Failed to connect to com123 at 57600 (bauds).\\\\n'\\n    clear(outstream)\\n\", 'test connect fail tr transportmock outstream io stringio tlm application transport tr stdout outstream tlm onecmd serial tlm runner update assert outstream getvalue failed to connect to at 9600 bauds clear outstream tlm onecmd serial 115200 tlm runner update assert outstream getvalue failed to connect to at 115200 bauds clear outstream tlm onecmd serial bauds 57600 tlm runner update assert outstream getvalue failed to connect to at 57600 bauds clear outstream', ''), ('test_print', 124, 'def test_print():\\n    tr = TransportMock()\\n    outstream = io.StringIO()\\n    tlm = Application(transport=tr, stdout=outstream)\\n    tlm.onecmd(\\'pub --i32 foo 2\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \"Published on topic \\'foo\\' : 2 [int32]\\\\n\"\\n    clear(outstream)\\n    tlm.onecmd(\\'pub --i32 foo 3\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \"Published on topic \\'foo\\' : 3 [int32]\\\\n\"\\n    clear(outstream)\\n    tlm.onecmd(\\'pub --s hello world\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue(\\n        ) == \"Published on topic \\'hello\\' : world [string]\\\\n\"\\n    clear(outstream)\\n    tlm.onecmd(\\'pub --i32 foo 4\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \"Published on topic \\'foo\\' : 4 [int32]\\\\n\"\\n    clear(outstream)\\n    tlm.onecmd(\\'print foo\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'4\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'print foo -l 3\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'2\\\\n3\\\\n4\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'print foo -l 2\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'3\\\\n4\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'print foo --limit 3\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'2\\\\n3\\\\n4\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'print foo --limit 10\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'2\\\\n3\\\\n4\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'print foo -a\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'2\\\\n3\\\\n4\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'print qux\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \"\"\"Topic \\'qux\\' unknown. Type \\'ls\\' to list all available topics.\\n\"\"\"\\n    clear(outstream)\\n    tlm.onecmd(\\'print hello\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'world\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'print foo -l 2.3\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue(\\n        ) == \"Could not cast --limit = \\'2.3\\' to integer. Using 1.\\\\n4\\\\n\"\\n    clear(outstream)\\n', 'test print tr transportmock outstream io stringio tlm application transport tr stdout outstream tlm onecmd pub foo 2 tlm runner update assert outstream getvalue published on topic foo 2 clear outstream tlm onecmd pub foo 3 tlm runner update assert outstream getvalue published on topic foo 3 clear outstream tlm onecmd pub hello world tlm runner update assert outstream getvalue published on topic hello world string clear outstream tlm onecmd pub foo 4 tlm runner update assert outstream getvalue published on topic foo 4 clear outstream tlm onecmd print foo tlm runner update assert outstream getvalue 4 clear outstream tlm onecmd print foo 3 tlm runner update assert outstream getvalue 2 clear outstream tlm onecmd print foo 2 tlm runner update assert outstream getvalue 3 clear outstream tlm onecmd print foo limit 3 tlm runner update assert outstream getvalue 2 clear outstream tlm onecmd print foo limit 10 tlm runner update assert outstream getvalue 2 clear outstream tlm onecmd print foo tlm runner update assert outstream getvalue 2 clear outstream tlm onecmd print qux tlm runner update assert outstream getvalue topic qux unknown type ls to list all available topics clear outstream tlm onecmd print hello tlm runner update assert outstream getvalue world clear outstream tlm onecmd print foo 2 3 tlm runner update assert outstream getvalue could not cast limit 2 3 to integer using 1 clear outstream', ''), ('test_count', 207, 'def test_count():\\n    tr = TransportMock()\\n    outstream = io.StringIO()\\n    tlm = Application(transport=tr, stdout=outstream)\\n    tlm.onecmd(\\'count\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'pub --i32 foo 2\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \"Published on topic \\'foo\\' : 2 [int32]\\\\n\"\\n    clear(outstream)\\n    tlm.onecmd(\\'count\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'foo : 1\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'pub --i32 foo 3\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \"Published on topic \\'foo\\' : 3 [int32]\\\\n\"\\n    clear(outstream)\\n    tlm.onecmd(\\'count\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'foo : 2\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'pub --f32 bar 4.2\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \"Published on topic \\'bar\\' : 4.2 [float32]\\\\n\"\\n    clear(outstream)\\n    tlm.onecmd(\\'count\\')\\n    tlm.runner.update()\\n    print(outstream.getvalue())\\n    for i in tlm.topics.topic_list.items():\\n        print(i)\\n    assert outstream.getvalue() == \\'bar : 1\\\\nfoo : 2\\\\n\\'\\n    clear(outstream)\\n', 'test count tr transportmock outstream io stringio tlm application transport tr stdout outstream tlm onecmd count tlm runner update assert outstream getvalue clear outstream tlm onecmd pub foo 2 tlm runner update assert outstream getvalue published on topic foo 2 clear outstream tlm onecmd count tlm runner update assert outstream getvalue foo 1 clear outstream tlm onecmd pub foo 3 tlm runner update assert outstream getvalue published on topic foo 3 clear outstream tlm onecmd count tlm runner update assert outstream getvalue foo 2 clear outstream tlm onecmd pub bar 4 2 tlm runner update assert outstream getvalue published on topic bar 4 2 clear outstream tlm onecmd count tlm runner update print outstream getvalue for in tlm topics topic list items print assert outstream getvalue bar 1 nfoo 2 clear outstream', ''), ('test_disconnect_quit', 257, \"def test_disconnect_quit():\\n    tr = TransportMock()\\n    outstream = io.StringIO()\\n    tlm = Application(transport=tr, stdout=outstream)\\n    tlm.onecmd('disconnect')\\n    assert outstream.getvalue() == 'Disconnected.\\\\n'\\n    clear(outstream)\\n    pytest.raises(SystemExit, tlm.onecmd, 'quit')\\n    assert outstream.getvalue() == 'Disconnected.\\\\nGood Bye!\\\\n'\\n    clear(outstream)\\n\", 'test disconnect quit tr transportmock outstream io stringio tlm application transport tr stdout outstream tlm onecmd disconnect assert outstream getvalue disconnected clear outstream pytest raises systemexit tlm onecmd quit assert outstream getvalue disconnected ngood bye clear outstream', ''), ('test_wrong_command', 272, \"def test_wrong_command():\\n    tr = TransportMock()\\n    outstream = io.StringIO()\\n    tlm = Application(transport=tr, stdout=outstream)\\n    tlm.onecmd('pub foo --i32 123')\\n    clear(outstream)\\n\", 'test wrong command tr transportmock outstream io stringio tlm application transport tr stdout outstream tlm onecmd pub foo 123 clear outstream', ''), ('test_info', 282, \"def test_info():\\n    tr = TransportMock()\\n    outstream = io.StringIO()\\n    tlm = Application(transport=tr, stdout=outstream)\\n    tlm.onecmd('info')\\n    clear(outstream)\\n\", 'test info tr transportmock outstream io stringio tlm application transport tr stdout outstream tlm onecmd info clear outstream', ''), ('test_topics_are_cleared_after_reconnect', 292, 'def test_topics_are_cleared_after_reconnect():\\n    tr = TransportMock()\\n    outstream = io.StringIO()\\n    tlm = Application(transport=tr, stdout=outstream)\\n    tlm.runner._start_thread = MagicMock()\\n    tr.authorizeConnect(True)\\n    tlm.onecmd(\\'serial com123\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'Connected to com123 at 9600 (bauds).\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'pub --f32 bar 4.2\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \"Published on topic \\'bar\\' : 4.2 [float32]\\\\n\"\\n    clear(outstream)\\n    tlm.onecmd(\\'count\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'bar : 1\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'disconnect\\')\\n    assert outstream.getvalue() == \\'Disconnected.\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'count\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'bar : 1\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'ls\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'bar\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'serial com123\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'Connected to com123 at 9600 (bauds).\\\\n\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'count\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'\\'\\n    clear(outstream)\\n    tlm.onecmd(\\'ls\\')\\n    tlm.runner.update()\\n    assert outstream.getvalue() == \\'\\'\\n    clear(outstream)\\n', 'test topics are cleared after reconnect tr transportmock outstream io stringio tlm application transport tr stdout outstream tlm runner start thread magicmock tr authorizeconnect true tlm onecmd serial tlm runner update assert outstream getvalue connected to at 9600 bauds clear outstream tlm onecmd pub bar 4 2 tlm runner update assert outstream getvalue published on topic bar 4 2 clear outstream tlm onecmd count tlm runner update assert outstream getvalue bar 1 clear outstream tlm onecmd disconnect assert outstream getvalue disconnected clear outstream tlm onecmd count tlm runner update assert outstream getvalue bar 1 clear outstream tlm onecmd ls tlm runner update assert outstream getvalue bar clear outstream tlm onecmd serial tlm runner update assert outstream getvalue connected to at 9600 bauds clear outstream tlm onecmd count tlm runner update assert outstream getvalue clear outstream tlm onecmd ls tlm runner update assert outstream getvalue clear outstream', ''), ('test_stats', 355, \"def test_stats():\\n    tr = TransportMock()\\n    outstream = io.StringIO()\\n    tlm = Application(transport=tr, stdout=outstream)\\n    tlm.runner._start_thread = MagicMock()\\n    tr.resetStats()\\n    tlm.runner.resetStats()\\n    tlm.telemetry.resetStats()\\n    tlm.onecmd('stats')\\n    assert 'Raw IO:\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_bytes : 0\\\\n' in outstream.getvalue()\\n    assert 'IO speeds:\\\\n' in outstream.getvalue()\\n    assert '\\\\tbaudspeed : 0.0\\\\n' in outstream.getvalue()\\n    assert '\\\\tbaudratio : 0.0\\\\n' in outstream.getvalue()\\n    assert '\\\\tbaudratio_avg : 0.0\\\\n' in outstream.getvalue()\\n    assert '\\\\tbaudspeed_avg : 0.0\\\\n' in outstream.getvalue()\\n    assert 'Framing:\\\\n' in outstream.getvalue()\\n    assert '\\\\ttx_encoded_frames : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_uncomplete_frames : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\ttx_processed_bytes : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_complete_frames : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\ttx_escaped_bytes : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_discarded_bytes : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_processed_bytes : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_escaped_bytes : 0\\\\n' in outstream.getvalue()\\n    assert 'Protocol:\\\\n' in outstream.getvalue()\\n    assert '\\\\ttx_encoded_frames : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_header : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_decoded_frames : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_payload : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_crc : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_eol : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_topic : 0\\\\n' in outstream.getvalue()\\n    tlm.onecmd('pub --i32 foo 2')\\n    clear(outstream)\\n    tlm.runner.update()\\n    tlm.onecmd('stats')\\n    speeds = tlm.runner.stats()\\n    assert 'Raw IO:\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_bytes : 14\\\\n' in outstream.getvalue()\\n    assert 'IO speeds:\\\\n' in outstream.getvalue()\\n    assert '\\\\tbaudspeed : {0}\\\\n'.format(speeds['baudspeed']\\n        ) in outstream.getvalue()\\n    assert '\\\\tbaudratio : {0}\\\\n'.format(speeds['baudratio']\\n        ) in outstream.getvalue()\\n    assert '\\\\tbaudratio_avg : {0}\\\\n'.format(speeds['baudratio_avg']\\n        ) in outstream.getvalue()\\n    assert '\\\\tbaudspeed_avg : {0}\\\\n'.format(speeds['baudspeed_avg']\\n        ) in outstream.getvalue()\\n    assert 'Framing:\\\\n' in outstream.getvalue()\\n    assert '\\\\ttx_encoded_frames : 1\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_uncomplete_frames : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\ttx_processed_bytes : 12\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_complete_frames : 1\\\\n' in outstream.getvalue()\\n    assert '\\\\ttx_escaped_bytes : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_discarded_bytes : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_processed_bytes : 14\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_escaped_bytes : 0\\\\n' in outstream.getvalue()\\n    assert 'Protocol:\\\\n' in outstream.getvalue()\\n    assert '\\\\ttx_encoded_frames : 1\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_header : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_decoded_frames : 1\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_payload : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_crc : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_eol : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_topic : 0\\\\n' in outstream.getvalue()\\n    tr.authorizeConnect(True)\\n    tlm.onecmd('serial com123')\\n    clear(outstream)\\n    tlm.onecmd('stats')\\n    assert 'Raw IO:\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_bytes : 0\\\\n' in outstream.getvalue()\\n    assert 'IO speeds:\\\\n' in outstream.getvalue()\\n    assert '\\\\tbaudspeed : 0.0\\\\n' in outstream.getvalue()\\n    assert '\\\\tbaudratio : 0.0\\\\n' in outstream.getvalue()\\n    assert '\\\\tbaudratio_avg : 0.0\\\\n' in outstream.getvalue()\\n    assert '\\\\tbaudspeed_avg : 0.0\\\\n' in outstream.getvalue()\\n    assert 'Framing:\\\\n' in outstream.getvalue()\\n    assert '\\\\ttx_encoded_frames : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_uncomplete_frames : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\ttx_processed_bytes : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_complete_frames : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\ttx_escaped_bytes : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_discarded_bytes : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_processed_bytes : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_escaped_bytes : 0\\\\n' in outstream.getvalue()\\n    assert 'Protocol:\\\\n' in outstream.getvalue()\\n    assert '\\\\ttx_encoded_frames : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_header : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_decoded_frames : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_payload : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_crc : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_eol : 0\\\\n' in outstream.getvalue()\\n    assert '\\\\trx_corrupted_topic : 0\\\\n' in outstream.getvalue()\\n\", 'test stats tr transportmock outstream io stringio tlm application transport tr stdout outstream tlm runner start thread magicmock tr resetstats tlm runner resetstats tlm telemetry resetstats tlm onecmd stats assert raw io in outstream getvalue assert trx bytes 0 in outstream getvalue assert io speeds in outstream getvalue assert tbaudspeed 0 0 in outstream getvalue assert tbaudratio 0 0 in outstream getvalue assert tbaudratio avg 0 0 in outstream getvalue assert tbaudspeed avg 0 0 in outstream getvalue assert framing in outstream getvalue assert ttx encoded frames 0 in outstream getvalue assert trx uncomplete frames 0 in outstream getvalue assert ttx processed bytes 0 in outstream getvalue assert trx complete frames 0 in outstream getvalue assert ttx escaped bytes 0 in outstream getvalue assert trx discarded bytes 0 in outstream getvalue assert trx processed bytes 0 in outstream getvalue assert trx escaped bytes 0 in outstream getvalue assert protocol in outstream getvalue assert ttx encoded frames 0 in outstream getvalue assert trx corrupted header 0 in outstream getvalue assert trx decoded frames 0 in outstream getvalue assert trx corrupted payload 0 in outstream getvalue assert trx corrupted crc 0 in outstream getvalue assert trx corrupted eol 0 in outstream getvalue assert trx corrupted topic 0 in outstream getvalue tlm onecmd pub foo 2 clear outstream tlm runner update tlm onecmd stats speeds tlm runner stats assert raw io in outstream getvalue assert trx bytes 14 in outstream getvalue assert io speeds in outstream getvalue assert tbaudspeed 0 format speeds baudspeed in outstream getvalue assert tbaudratio 0 format speeds baudratio in outstream getvalue assert tbaudratio avg 0 format speeds baudratio avg in outstream getvalue assert tbaudspeed avg 0 format speeds baudspeed avg in outstream getvalue assert framing in outstream getvalue assert ttx encoded frames 1 in outstream getvalue assert trx uncomplete frames 0 in outstream getvalue assert ttx processed bytes 12 in outstream getvalue assert trx complete frames 1 in outstream getvalue assert ttx escaped bytes 0 in outstream getvalue assert trx discarded bytes 0 in outstream getvalue assert trx processed bytes 14 in outstream getvalue assert trx escaped bytes 0 in outstream getvalue assert protocol in outstream getvalue assert ttx encoded frames 1 in outstream getvalue assert trx corrupted header 0 in outstream getvalue assert trx decoded frames 1 in outstream getvalue assert trx corrupted payload 0 in outstream getvalue assert trx corrupted crc 0 in outstream getvalue assert trx corrupted eol 0 in outstream getvalue assert trx corrupted topic 0 in outstream getvalue tr authorizeconnect true tlm onecmd serial clear outstream tlm onecmd stats assert raw io in outstream getvalue assert trx bytes 0 in outstream getvalue assert io speeds in outstream getvalue assert tbaudspeed 0 0 in outstream getvalue assert tbaudratio 0 0 in outstream getvalue assert tbaudratio avg 0 0 in outstream getvalue assert tbaudspeed avg 0 0 in outstream getvalue assert framing in outstream getvalue assert ttx encoded frames 0 in outstream getvalue assert trx uncomplete frames 0 in outstream getvalue assert ttx processed bytes 0 in outstream getvalue assert trx complete frames 0 in outstream getvalue assert ttx escaped bytes 0 in outstream getvalue assert trx discarded bytes 0 in outstream getvalue assert trx processed bytes 0 in outstream getvalue assert trx escaped bytes 0 in outstream getvalue assert protocol in outstream getvalue assert ttx encoded frames 0 in outstream getvalue assert trx corrupted header 0 in outstream getvalue assert trx decoded frames 0 in outstream getvalue assert trx corrupted payload 0 in outstream getvalue assert trx corrupted crc 0 in outstream getvalue assert trx corrupted eol 0 in outstream getvalue assert trx corrupted topic 0 in outstream getvalue', ''), ('__init__', 10, 'def __init__(self):\\n    self.q = queue.Queue()\\n    self.canConnect = False\\n    self.counter = 0\\n', 'init self self queue queue self canconnect false self counter 0', ''), ('authorizeConnect', 14, 'def authorizeConnect(self, value):\\n    self.canConnect = value\\n', 'authorizeconnect self value self canconnect value', ''), ('connect', 16, \"def connect(self, options):\\n    print('TransportMock trying to connect')\\n    if not self.canConnect:\\n        raise IOError('TransportMock denied connection')\\n    print('TransportMock connected')\\n\", 'connect self options print transportmock trying to connect if not self canconnect raise ioerror transportmock denied connection print transportmock connected', ''), ('disconnect', 21, 'def disconnect(self):\\n    pass\\n', 'disconnect self pass', ''), ('read', 23, 'def read(self, maxbytes=1):\\n    amount = maxbytes if self.q.qsize() > maxbytes else self.q.qsize()\\n    self.counter += amount\\n    data = []\\n    for i in range(amount):\\n        data.append(self.q.get())\\n    return data\\n', 'read self maxbytes 1 amount maxbytes if self qsize maxbytes else self qsize self counter amount data for in range amount data append self get return data', ''), ('readable', 30, 'def readable(self):\\n    return self.q.qsize()\\n', 'readable self return self qsize', ''), ('write', 32, 'def write(self, data):\\n    for c in data:\\n        self.q.put(c)\\n', 'write self data for in data self put', ''), ('writeable', 35, 'def writeable(self):\\n    return True\\n', 'writeable self return true', ''), ('resetStats', 37, 'def resetStats(self, averaging_window=1):\\n    self.counter = 0\\n', 'resetstats self averaging window 1 self counter 0', ''), ('stats', 39, \"def stats(self):\\n    return {'rx_bytes': self.counter, 'tx_bytes': 0, 'rx_chunks': 0,\\n        'tx_chunks': 0, 'rx_in_waiting': 0, 'rx_in_waiting_avg': 0,\\n        'rx_in_waiting_max': 0}\\n\", 'stats self return rx bytes self counter tx bytes 0 rx chunks 0 tx chunks 0 rx in waiting 0 rx in waiting avg 0 rx in waiting max 0', '')] [('testFilterRectanbles', 6, 'def testFilterRectanbles(self):\\n    self.assertEqual(filterRectangles([]), [])\\n    self.assertEqual(filterRectangles([((817, 597), (10, 12), -26), ((223, \\n        440), (319, 60), -88)], maxWidth=500), [((223, 440), (319, 60), -88)])\\n    self.assertEqual(filterRectangles([((298, 508), (58, 319), -15)],\\n        maxWidth=500), [((298, 508), (319, 58), 75)])\\n    self.assertEqual(filterRectangles([((982, 492), (29, 17), -45), ((951, \\n        507), (60, 84), -69)]), [])\\n', 'testfilterrectanbles self self assertequal filterrectangles self assertequal filterrectangles 817 597 10 12 26 223 440 319 60 88 maxwidth 500 223 440 319 60 88 self assertequal filterrectangles 298 508 58 319 15 maxwidth 500 298 508 319 58 75 self assertequal filterrectangles 982 492 29 17 45 951 507 60 84 69', ''), ('testStripPose', 14, 'def testStripPose(self):\\n    s = 0.3 / 319.0\\n    self.assertEqual(repr(stripPose(((223, 440), (319, 60), -88))), repr(\\n        Pose(s * (720 / 2 - 440), s * (-223 + 1280 / 2), math.radians(-2))))\\n    self.assertEqual(repr(stripPose(((298, 508), (319, 58), 75))), repr(\\n        Pose(s * (720 / 2 - 508), s * (-298 + 1280 / 2), math.radians(15))))\\n    s = 0.3 / float(319 / 2)\\n    self.assertEqual(repr(stripPose(((298 / 2, 508 / 2), (319 / 2, 58 / 2),\\n        75), highResolution=False)), repr(Pose(s * ((720 / 2 - 508) / 2), s *\\n        ((-298 + 1280 / 2) / 2), math.radians(15))))\\n', 'teststrippose self 0 3 319 0 self assertequal repr strippose 223 440 319 60 88 repr pose 720 2 440 223 1280 2 math radians 2 self assertequal repr strippose 298 508 319 58 75 repr pose 720 2 508 298 1280 2 math radians 15 0 3 float 319 2 self assertequal repr strippose 298 2 508 2 319 2 58 2 75 highresolution false repr pose 720 2 508 2 298 1280 2 2 math radians 15', ''), ('testZScalingBug', 21, 'def testZScalingBug(self):\\n    sWidth = 0.3 / 189.0\\n    s = 0.05 / 53.0\\n    self.assertEqual(repr(stripPose(((898, 257), (189, 53), -58))), repr(\\n        Pose(s * (720 / 2 - 257), s * (-898 + 1280 / 2), math.radians(-32))))\\n    self.assertEqual(repr(allStripPoses([((898, 257), (189, 53), -58)])[0]),\\n        repr(Pose(s * (720 / 2 - 257), s * (-898 + 1280 / 2), math.radians(\\n        -32))))\\n', 'testzscalingbug self swidth 0 3 189 0 0 05 53 0 self assertequal repr strippose 898 257 189 53 58 repr pose 720 2 257 898 1280 2 math radians 32 self assertequal repr allstripposes 898 257 189 53 58 0 repr pose 720 2 257 898 1280 2 math radians 32', ''), ('testAllStripPoses', 27, 'def testAllStripPoses(self):\\n    asp = allStripPoses([((318, 201), (134, 22), 82), ((293, 43), (90, 18),\\n        82)], highResolution=False)\\n    self.assertEqual(len(asp), 2)\\n    self.assertAlmostEqual(abs(asp[1].sub(asp[0]).y), 0.01, 2)\\n    self.assertAlmostEqual(abs(asp[1].sub(asp[0]).x), 0.36, 2)\\n    self.assertEqual(allStripPoses([]), [])\\n', 'testallstripposes self asp allstripposes 318 201 134 22 82 293 43 90 18 82 highresolution false self assertequal len asp 2 self assertalmostequal abs asp 1 sub asp 0 0 01 2 self assertalmostequal abs asp 1 sub asp 0 0 36 2 self assertequal allstripposes', ''), ('testRemoveDuplicities', 35, 'def testRemoveDuplicities(self):\\n    self.assertEqual(removeDuplicities([]), [])\\n    self.assertEqual(removeDuplicities([((400, 162), (128, 20), -72), ((400,\\n        161), (131, 24), -72)]), [((400, 162), (128, 20), -72)])\\n    self.assertEqual(removeDuplicities([((209, 292), (133, 30), -87), ((201,\\n        136), (95, 20), 77), ((198, 123), (143, 27), 78)]), [((209, 292), (\\n        133, 30), -87), ((198, 123), (143, 27), 78)])\\n    self.assertEqual(removeDuplicities([((599, 241), (89, 20), -18), ((353,\\n        183), (78, 18), 90), ((352, 161), (128, 23), 88)]), [((599, 241), (\\n        89, 20), -18), ((352, 161), (128, 23), 88)])\\n', 'testremoveduplicities self self assertequal removeduplicities self assertequal removeduplicities 400 162 128 20 72 400 161 131 24 72 400 162 128 20 72 self assertequal removeduplicities 209 292 133 30 87 201 136 95 20 77 198 123 143 27 78 209 292 133 30 87 198 123 143 27 78 self assertequal removeduplicities 599 241 89 20 18 353 183 78 18 90 352 161 128 23 88 599 241 89 20 18 352 161 128 23 88', ''), ('testRemoveDuplicitiesMistakes', 44, 'def testRemoveDuplicitiesMistakes(self):\\n    self.assertEqual(removeDuplicities([((155, 196), (160, 26), -63), ((141,\\n        188), (163, 62), -63)]), [((155, 196), (160, 26), -63)])\\n', 'testremoveduplicitiesmistakes self self assertequal removeduplicities 155 196 160 26 63 141 188 163 62 63 155 196 160 26 63', ''), ('testRemoveLargeStrips', 56, 'def testRemoveLargeStrips(self):\\n    self.assertEqual(filterRectangles([((452, 143), (289, 68), 88)]), [])\\n', 'testremovelargestrips self self assertequal filterrectangles 452 143 289 68 88', '')] [] [('init_logger', 29, 'def init_logger(name):\\n    \"\"\" Get logger with handler(s) \"\"\"\\n    logger = logging.getLogger(name)\\n    logger.setLevel(settings.LOGGER[\\'level\\'])\\n    logger.addHandler(__get_stderr_handler())\\n    return logger\\n', 'init logger name logger logging getlogger name logger setlevel settings logger level logger addhandler get stderr handler return logger', 'get logger with handler s'), ('__get_stderr_handler', 37, 'def __get_stderr_handler():\\n    \"\"\" Get stderr logger \"\"\"\\n    handler = logging.StreamHandler(sys.stderr)\\n    handler.setLevel(logging.DEBUG)\\n    formatter = logging.Formatter(\\n        \\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\\')\\n    handler.setFormatter(formatter)\\n    return handler\\n', 'get stderr handler handler logging streamhandler sys stderr handler setlevel logging debug formatter logging formatter asctime name levelname message handler setformatter formatter return handler', 'get stderr logger')] [] [('setup', 14, \"def setup(self):\\n    sys.argv = [sys.argv[0], 'result', 'list', '--target-dir', 'directory']\\n    self.abs_target_dir = os.path.abspath('directory')\\n    self.result = mock.Mock()\\n    kiwi.tasks.result_list.Help = mock.Mock(return_value=mock.Mock())\\n    self.task = ResultListTask()\\n\", 'setup self sys argv sys argv 0 result list target dir directory self abs target dir os path abspath directory self result mock mock kiwi tasks result list help mock mock return value mock mock self task resultlisttask', ''), ('teardown', 25, 'def teardown(self):\\n    sys.argv = argv_kiwi_tests\\n', 'teardown self sys argv argv kiwi tests', ''), ('_init_command_args', 28, \"def _init_command_args(self):\\n    self.task.command_args = {}\\n    self.task.command_args['help'] = False\\n    self.task.command_args['list'] = False\\n    self.task.command_args['--target-dir'] = 'directory'\\n\", 'init command args self self task command args self task command args help false self task command args list false self task command args target dir directory', ''), ('test_process_result_list', 34, \"@patch('kiwi.tasks.result_list.Result.load')\\ndef test_process_result_list(self, mock_load):\\n    mock_load.return_value = self.result\\n    self._init_command_args()\\n    self.task.command_args['list'] = True\\n    self.task.process()\\n    mock_load.assert_called_once_with(os.sep.join([self.abs_target_dir,\\n        'kiwi.result']))\\n    self.result.print_results.assert_called_once_with()\\n\", 'test process result list self mock load mock load return value self result self init command args self task command args list true self task process mock load assert called once with os sep join self abs target dir kiwi result self result print results assert called once with', ''), ('test_process_result_list_help', 45, \"def test_process_result_list_help(self):\\n    self._init_command_args()\\n    self.task.command_args['help'] = True\\n    self.task.command_args['list'] = True\\n    self.task.process()\\n    self.task.manual.show.assert_called_once_with('kiwi::result::list')\\n\", 'test process result list help self self init command args self task command args help true self task command args list true self task process self task manual show assert called once with kiwi result list', '')] [('test_main', 61, 'def test_main():\\n    support.run_unittest(PopenTest)\\n', 'test main support run unittest popentest', ''), ('_do_test_commandline', 22, 'def _do_test_commandline(self, cmdline, expected):\\n    cmd = \\'%s -c \"import sys; print(sys.argv)\" %s\\'\\n    cmd = cmd % (python, cmdline)\\n    with os.popen(cmd) as p:\\n        data = p.read()\\n    got = eval(data)[1:]\\n    self.assertEqual(got, expected)\\n', 'do test commandline self cmdline expected cmd import sys print sys argv cmd cmd python cmdline with os popen cmd as data read got eval data 1 self assertequal got expected', ''), ('test_popen', 30, 'def test_popen(self):\\n    self.assertRaises(TypeError, os.popen)\\n    self._do_test_commandline(\\'foo bar\\', [\\'foo\\', \\'bar\\'])\\n    self._do_test_commandline(\\'foo \"spam and eggs\" \"silly walk\"\\', [\\'foo\\',\\n        \\'spam and eggs\\', \\'silly walk\\'])\\n    self._do_test_commandline(\\'foo \"a \\\\\\\\\"quoted\\\\\\\\\" arg\" bar\\', [\\'foo\\',\\n        \\'a \"quoted\" arg\\', \\'bar\\'])\\n    support.reap_children()\\n', 'test popen self self assertraises typeerror os popen self do test commandline foo bar foo bar self do test commandline foo spam and eggs silly walk foo spam and eggs silly walk self do test commandline foo quoted arg bar foo quoted arg bar support reap children', ''), ('test_return_code', 46, \"def test_return_code(self):\\n    self.assertEqual(os.popen('exit 0').close(), None)\\n    if os.name == 'nt':\\n        self.assertEqual(os.popen('exit 42').close(), 42)\\n    else:\\n        self.assertEqual(os.popen('exit 42').close(), 42 << 8)\\n\", 'test return code self self assertequal os popen exit 0 close none if os name nt self assertequal os popen exit 42 close 42 else self assertequal os popen exit 42 close 42 8', ''), ('test_contextmanager', 53, \"def test_contextmanager(self):\\n    with os.popen('echo hello') as f:\\n        self.assertEqual(f.read(), 'hello\\\\n')\\n\", 'test contextmanager self with os popen echo hello as self assertequal read hello', ''), ('test_iterating', 57, \"def test_iterating(self):\\n    with os.popen('echo hello') as f:\\n        self.assertEqual(list(f), ['hello\\\\n'])\\n\", 'test iterating self with os popen echo hello as self assertequal list hello', '')] [('lambda_handler', 29, 'def lambda_handler(event, context):\\n    print(\\'Event: {}\\'.format(json.dumps(event)))\\n    if \\'warming\\' in event and \\'{}\\'.format(event[\\'warming\\']).lower() == \\'true\\':\\n        return {\\'message\\': \\'Warmed!\\'}\\n    client_id = os.environ[\\'COGNITO_USER_POOL_CLIENT_ID\\']\\n    client_secret = os.environ[\\'COGNITO_USER_POOL_CLIENT_SECRET\\']\\n    user_profile_dataset_name = os.environ[\\'COGNITO_USER_PROFILE_DATASET_NAME\\']\\n    user_pool_id = os.environ[\\'COGNITO_USER_POOL_ID\\']\\n    token = \\'\\'\\n    try:\\n        token = event[\\'queryStringParameters\\'][\\'token\\']\\n    except:\\n        pass\\n    if token == \\'\\':\\n        raise APIGatewayException(\\'Value for \"token\" must be specified in URL.\\'\\n            , 400)\\n    cognito_user_id = None\\n    if event[\\'resource\\'] == \\'/user/register/verify\\':\\n        cognito_user_id = event.get(\\'queryStringParameters\\', {}).get(\\n            \\'registration-id\\', \\'\\')\\n        if cognito_user_id == \\'\\':\\n            raise APIGatewayException(\\n                \\'Value for \"registration-id\" must be specified in URL.\\', 400)\\n        try:\\n            cognito_idp_client.confirm_sign_up(ClientId=client_id,\\n                SecretHash=generate_cognito_sign_up_secret_hash(\\n                cognito_user_id, client_id, client_secret), Username=\\n                cognito_user_id, ConfirmationCode=token)\\n        except botocore.exceptions.ClientError as e:\\n            if e.response[\\'Error\\'][\\'Code\\'] == \\'AliasExistsException\\':\\n                raise APIGatewayException(\\n                    \\'E-mail address already confirmed by another account.\\', 400\\n                    )\\n            elif e.response[\\'Error\\'][\\'Code\\'] == \\'NotAuthorizedException\\':\\n                raise APIGatewayException(\\n                    \\'Token provided is expired, invalid, or already used.\\', 400\\n                    )\\n            elif e.response[\\'Error\\'][\\'Code\\'] == \\'CodeMismatchException\\':\\n                raise APIGatewayException(\\'Token provided is incorrect.\\', 400)\\n            elif e.response[\\'Error\\'][\\'Code\\'] == \\'ExpiredCodeException\\':\\n                raise APIGatewayException(\\n                    \\'Invalid token provided. Please request another.\\', 400)\\n            raise\\n    elif event[\\'resource\\'] == \\'/user/email/verify\\':\\n        cognito_auth_provider_string = event[\\'requestContext\\'][\\'identity\\'][\\n            \\'cognitoAuthenticationProvider\\']\\n        cognito_idp_name = cognito_auth_provider_string.split(\\',\\')[0]\\n        cognito_user_pool_sub_value = cognito_auth_provider_string.split(\\',\\')[1\\n            ].split(\\':\\')[2]\\n        response = cognito_idp_client.list_users(UserPoolId=user_pool_id,\\n            AttributesToGet=[], Filter=\\'sub = \"{}\"\\'.format(\\n            cognito_user_pool_sub_value), Limit=1)\\n        cognito_user_id = response[\\'Users\\'][0][\\'Username\\']\\n        identity_id = event[\\'requestContext\\'][\\'identity\\'][\\'cognitoIdentityId\\']\\n        identity_pool_id = event[\\'requestContext\\'][\\'identity\\'][\\n            \\'cognitoIdentityPoolId\\']\\n        response = cognito_sync_client.list_records(IdentityPoolId=\\n            identity_pool_id, IdentityId=identity_id, DatasetName=\\n            user_profile_dataset_name)\\n        idp_credentials = None\\n        for each_record in response.get(\\'Records\\', []):\\n            if each_record[\\'Key\\'] == \\'idp-credentials\\':\\n                idp_credentials = json.loads(each_record[\\'Value\\'])\\n                break\\n        if idp_credentials is None or idp_credentials.get(\\'expires\\', 0) < int(\\n            time.time()):\\n            raise APIGatewayException(\\n                \\'Identity provider credentials expired. Please log in and try again.\\'\\n                , 400)\\n        access_token = idp_credentials[\\'access-token\\']\\n        try:\\n            cognito_idp_client.verify_user_attribute(AccessToken=\\n                access_token, AttributeName=\\'email\\', Code=token)\\n        except botocore.exceptions.ClientError as e:\\n            if e.response[\\'Error\\'][\\'Code\\'] == \\'CodeMismatchException\\':\\n                raise APIGatewayException(\\'Token provided is incorrect.\\', 400)\\n            elif e.response[\\'Error\\'][\\'Code\\'] == \\'NotAuthorizedException\\':\\n                raise APIGatewayException(\\n                    \\'Token provided is expired, invalid, or already used.\\', 400\\n                    )\\n            elif e.response[\\'Error\\'][\\'Code\\'] == \\'ExpiredCodeException\\':\\n                raise APIGatewayException(\\n                    \\'Invalid token provided. Please request another.\\', 400)\\n            else:\\n                raise\\n    response = cognito_idp_client.admin_get_user(UserPoolId=user_pool_id,\\n        Username=cognito_user_id)\\n    new_user_email = None\\n    for each_attribute_pair in response.get(\\'UserAttributes\\', []):\\n        if each_attribute_pair[\\'Name\\'] == \\'email\\':\\n            new_user_email = each_attribute_pair[\\'Value\\']\\n            break\\n    if new_user_email is None:\\n        raise Exception(\"Unable to determine user\\'s new e-mail address.\")\\n    return {\\'email-address\\': new_user_email, \\'message\\':\\n        \\'E-mail address confirmed successfully.\\'}\\n', 'lambda handler event context print event format json dumps event if warming in event and format event warming lower true return message warmed client id os environ cognito user pool client id client secret os environ cognito user pool client secret user profile dataset name os environ cognito user profile dataset name user pool id os environ cognito user pool id token try token event querystringparameters token except pass if token raise apigatewayexception value for token must be specified in url 400 cognito user id none if event resource user register verify cognito user id event get querystringparameters get registration id if cognito user id raise apigatewayexception value for registration id must be specified in url 400 try cognito idp client confirm sign up clientid client id secrethash generate cognito sign up secret hash cognito user id client id client secret username cognito user id confirmationcode token except botocore exceptions clienterror as if response error code aliasexistsexception raise apigatewayexception mail address already confirmed by another account 400 elif response error code notauthorizedexception raise apigatewayexception token provided is expired invalid or already used 400 elif response error code codemismatchexception raise apigatewayexception token provided is incorrect 400 elif response error code expiredcodeexception raise apigatewayexception invalid token provided please request another 400 raise elif event resource user email verify cognito auth provider string event requestcontext identity cognitoauthenticationprovider cognito idp name cognito auth provider string split 0 cognito user pool sub value cognito auth provider string split 1 split 2 response cognito idp client list users userpoolid user pool id attributestoget filter sub format cognito user pool sub value limit 1 cognito user id response users 0 username identity id event requestcontext identity cognitoidentityid identity pool id event requestcontext identity cognitoidentitypoolid response cognito sync client list records identitypoolid identity pool id identityid identity id datasetname user profile dataset name idp credentials none for each record in response get records if each record key idp credentials idp credentials json loads each record value break if idp credentials is none or idp credentials get expires 0 int time time raise apigatewayexception identity provider credentials expired please log in and try again 400 access token idp credentials access token try cognito idp client verify user attribute accesstoken access token attributename email code token except botocore exceptions clienterror as if response error code codemismatchexception raise apigatewayexception token provided is incorrect 400 elif response error code notauthorizedexception raise apigatewayexception token provided is expired invalid or already used 400 elif response error code expiredcodeexception raise apigatewayexception invalid token provided please request another 400 else raise response cognito idp client admin get user userpoolid user pool id username cognito user id new user email none for each attribute pair in response get userattributes if each attribute pair name email new user email each attribute pair value break if new user email is none raise exception unable to determine user new mail address return email address new user email message mail address confirmed successfully', ''), ('proxy_lambda_handler', 153, \"def proxy_lambda_handler(event, context):\\n    response_headers = get_response_headers(event, context)\\n    try:\\n        return_dict = lambda_handler(event, context)\\n    except APIGatewayException as e:\\n        return {'statusCode': e.http_status_code, 'headers':\\n            response_headers, 'body': json.dumps({'message': e.\\n            http_status_message})}\\n    return {'statusCode': 200, 'headers': response_headers, 'body': json.\\n        dumps(return_dict)}\\n\", 'proxy lambda handler event context response headers get response headers event context try return dict lambda handler event context except apigatewayexception as return statuscode http status code headers response headers body json dumps message http status message return statuscode 200 headers response headers body json dumps return dict', '')] [('__init__', 25, 'def __init__(self, spot, flock, *cnf):\\n    QGraphicsView.__init__(self, *cnf)\\n    self.__xsize = self.width() - 150\\n    self.__ysize = self.height() - 150\\n    self.__set_scale()\\n    self.__create_gs()\\n    self.__create_objects(len(flock))\\n    self.__spot = spot\\n    self.__flock = flock\\n    self.set_state(spot, flock)\\n    self.show()\\n', 'init self spot flock cnf qgraphicsview init self cnf self xsize self width 150 self ysize self height 150 self set scale self create gs self create objects len flock self spot spot self flock flock self set state spot flock self show', ''), ('__set_scale', 38, 'def __set_scale(self):\\n    xmin = -0.1\\n    ymin = -0.1\\n    xmax = 1.1\\n    ymax = 1.1\\n    self.__ay = -float(self.__ysize) / (ymax - ymin)\\n    self.__by = -self.__ay * ymax\\n    self.__ax = -self.__ay\\n    self.__bx = -self.__ax * xmin\\n', 'set scale self xmin 0 1 ymin 0 1 xmax 1 1 ymax 1 1 self ay float self ysize ymax ymin self by self ay ymax self ax self ay self bx self ax xmin', ''), ('__create_gs', 49, 'def __create_gs(self):\\n    self.gs = QGraphicsScene(0, 0, self.__xsize, self.__ysize)\\n    self.gs.setBackgroundBrush(QBrush(QColor(0, 0, 0)))\\n    self.setScene(self.gs)\\n', 'create gs self self gs qgraphicsscene 0 0 self xsize self ysize self gs setbackgroundbrush qbrush qcolor 0 0 0 self setscene self gs', ''), ('__create_objects', 55, 'def __create_objects(self, n_points):\\n    self.__dots = []\\n    s_pen = QPen(QColor(192, 0, 0), 2)\\n    s_brush = QBrush(QColor(192, 0, 0))\\n    s = self.gs.addEllipse(QRectF(0, 0, 1, 1), s_pen, s_brush)\\n    s.setZValue(300)\\n    s.show()\\n    self.__dots.append(s)\\n    b_pen = QPen(QColor(255, 255, 255), 2)\\n    b_brush = QBrush(QColor(255, 255, 255))\\n    for i in range(n_points):\\n        b = self.gs.addEllipse(QRectF(0, 0, 1, 1), b_pen, b_brush)\\n        b.setZValue(301 + i)\\n        b.show()\\n        self.__dots.append(b)\\n', 'create objects self points self dots pen qpen qcolor 192 0 0 2 brush qbrush qcolor 192 0 0 self gs addellipse qrectf 0 0 1 1 pen brush setzvalue 300 show self dots append pen qpen qcolor 255 255 255 2 brush qbrush qcolor 255 255 255 for in range points self gs addellipse qrectf 0 0 1 1 pen brush setzvalue 301 show self dots append', ''), ('set_state', 74, 'def set_state(self, spot, flock):\\n    self.__spot = spot\\n    self.__flock = flock\\n    radius = 5\\n    sx, sy = self.__transform(spot[0], spot[1])\\n    self.__dots[0].setRect(sx, sy, radius, radius)\\n    radius = 5\\n    for p, b in zip(flock, self.__dots[1:]):\\n        fx, fy = self.__transform(p[0], p[1])\\n        b.setRect(fx, fy, radius, radius)\\n', 'set state self spot flock self spot spot self flock flock radius 5 sx sy self transform spot 0 spot 1 self dots 0 setrect sx sy radius radius radius 5 for in zip flock self dots 1 fx fy self transform 0 1 setrect fx fy radius radius', ''), ('__transform', 88, 'def __transform(self, x, y):\\n    \"\"\"\\n        Transforms a pair of real world coordinates to screen coordinates.\\n        \"\"\"\\n    xr = int(self.__ax * x + self.__bx)\\n    yr = int(self.__ay * y + self.__by)\\n    return xr, yr\\n', 'transform self xr int self ax self bx yr int self ay self by return xr yr', 'transforms a pair of real world coordinates to screen coordinates'), ('resizeEvent', 97, 'def resizeEvent(self, event):\\n    self.__xsize = event.size().width()\\n    self.__ysize = event.size().height()\\n    self.__set_scale()\\n    self.set_state(self.__spot, self.__flock)\\n', 'resizeevent self event self xsize event size width self ysize event size height self set scale self set state self spot self flock', '')] [('get_calendar', 40, 'def get_calendar(name, open_time=None, close_time=None):\\n    \"\"\"\\n    Retrieves an instance of an MarketCalendar whose name is given.\\n\\n    :param name: The name of the MarketCalendar to be retrieved.\\n    :param open_time: Market open time override as datetime.time object. If None then default is used.\\n    :param close_time: Market close time override as datetime.time object. If None then default is used.\\n    :return: MarketCalendar of the desired calendar.\\n    \"\"\"\\n    canonical_name = _aliases.get(name, name)\\n    return _calendars[canonical_name](open_time, close_time)\\n', 'get calendar name open time none close time none canonical name aliases get name name return calendars canonical name open time close time', 'retrieves an instance of an marketcalendar whose name is given'), ('merge_schedules', 53, 'def merge_schedules(schedules, how=\\'outer\\'):\\n    \"\"\"\\n    Given a list of schedules will return a merged schedule. The merge method (how) will either return the superset\\n    of any datetime when any schedule is open (outer) or only the datetime where all markets are open (inner)\\n\\n    :param schedules: list of schedules\\n    :param how: outer or inner\\n    :return: schedule DataFrame\\n    \"\"\"\\n    result = schedules[0]\\n    for schedule in schedules[1:]:\\n        result = result.merge(schedule, how=how, right_index=True,\\n            left_index=True)\\n        if how == \\'outer\\':\\n            result[\\'market_open\\'] = result.apply(lambda x: min(x.\\n                market_open_x, x.market_open_y), axis=1)\\n            result[\\'market_close\\'] = result.apply(lambda x: max(x.\\n                market_close_x, x.market_close_y), axis=1)\\n        elif how == \\'inner\\':\\n            result[\\'market_open\\'] = result.apply(lambda x: max(x.\\n                market_open_x, x.market_open_y), axis=1)\\n            result[\\'market_close\\'] = result.apply(lambda x: min(x.\\n                market_close_x, x.market_close_y), axis=1)\\n        else:\\n            raise ValueError(\\'how argument must be \"inner\" or \"outer\"\\')\\n        result = result[[\\'market_open\\', \\'market_close\\']]\\n    return result\\n', 'merge schedules schedules how outer result schedules 0 for schedule in schedules 1 result result merge schedule how how right index true left index true if how outer result market open result apply lambda min market open market open axis 1 result market close result apply lambda max market close market close axis 1 elif how inner result market open result apply lambda max market open market open axis 1 result market close result apply lambda min market close market close axis 1 else raise valueerror how argument must be inner or outer result result market open market close return result', 'given a list of schedules will return a merged schedule the merge method how will either return the superset of any datetime when any schedule is open outer or only the datetime where all markets are open inner'), ('convert_freq', 78, 'def convert_freq(index, frequency):\\n    \"\"\"\\n    Converts a DateTimeIndex to a new lower frequency\\n\\n    :param index: DateTimeIndex\\n    :param frequency: frequency string\\n    :return: DateTimeIndex\\n    \"\"\"\\n    return pd.DataFrame(index=index).asfreq(frequency).index\\n', 'convert freq index frequency return pd dataframe index index asfreq frequency index', 'converts a datetimeindex to a new lower frequency'), ('date_range', 89, 'def date_range(schedule, frequency, closed=\\'right\\', force_close=True, **kwargs\\n    ):\\n    \"\"\"\\n    Given a schedule will return a DatetimeIndex will all of the valid datetime at the frequency given.\\n    The schedule values are assumed to be in UTC.\\n\\n    :param schedule: schedule DataFrame\\n    :param frequency: frequency in standard string\\n    :param closed: same meaning as pandas date_range. \\'right\\' will exclude the first value and should be used when the\\n      results should only include the close for each bar.\\n    :param force_close: if True then the close of the day will be included even if it does not fall on an even\\n      frequency. If False then the market close for the day may not be included in the results\\n    :param kwargs: arguments that will be passed to the pandas date_time\\n    :return: DatetimeIndex\\n    \"\"\"\\n    if pd.Timedelta(frequency) > pd.Timedelta(\\'1D\\'):\\n        raise ValueError(\\'Frequency must be 1D or higher frequency.\\')\\n    kwargs[\\'closed\\'] = closed\\n    ranges = list()\\n    for row in schedule.itertuples():\\n        dates = pd.date_range(row.market_open, row.market_close, freq=\\n            frequency, tz=\\'UTC\\', **kwargs)\\n        if force_close:\\n            if row.market_close not in dates:\\n                dates = dates.insert(len(dates), row.market_close)\\n        ranges.append(dates)\\n    index = pd.DatetimeIndex([], tz=\\'UTC\\')\\n    return index.union_many(ranges)\\n', 'date range schedule frequency closed right force close true kwargs if pd timedelta frequency pd timedelta raise valueerror frequency must be or higher frequency kwargs closed closed ranges list for row in schedule itertuples dates pd date range row market open row market close freq frequency tz utc kwargs if force close if row market close not in dates dates dates insert len dates row market close ranges append dates index pd datetimeindex tz utc return index union many ranges', 'given a schedule will return a datetimeindex will all of the valid datetime at the frequency given the schedule values are assumed to be in utc')] [('postprocess_data', 77, \"def postprocess_data(self, data):\\n    for key in data.keys():\\n        new_key = key.replace('company', 'companies')\\n        new_key = new_key.replace('other', 'miscellaneous')\\n        new_key = new_key.replace('distributor', 'distributors')\\n        if new_key != key:\\n            data[new_key] = data[key]\\n            del data[key]\\n    return data\\n\", 'postprocess data self data for key in data keys new key key replace company companies new key new key replace other miscellaneous new key new key replace distributor distributors if new key key data new key data key del data key return data', '')] [('read_mapping', 83, \"def read_mapping(mapping):\\n    if not mapping:\\n        return None\\n    redirects = {}\\n    with utf8_open(mapping) as f:\\n        for l in f:\\n            bits = l.rstrip().split('\\\\t')\\n            title = bits[0].replace(' ', '_')\\n            for r in bits[1:]:\\n                r = r.replace(' ', '_')\\n                redirects[r] = title\\n            redirects[title] = title\\n    return redirects\\n\", 'read mapping mapping if not mapping return none redirects with open mapping as for in bits rstrip split title bits 0 replace for in bits 1 replace redirects title redirects title title return redirects', ''), ('apply_mapping', 98, 'def apply_mapping(mapping, candidates):\\n    for c in candidates:\\n        kbid = normalise_link(c.eid)\\n        if mapping:\\n            c.eid = mapping.get(kbid, kbid)\\n        yield c\\n', 'apply mapping mapping candidates for in candidates kbid normalise link eid if mapping eid mapping get kbid kbid yield', ''), ('read_excluded_spans', 106, \"def read_excluded_spans(path):\\n    if path is None:\\n        return set()\\n    excluded = set()\\n    with open(path) as f:\\n        for l in f:\\n            doc_id, start, end = l.strip().split('\\\\t')[:3]\\n            for i in range(int(start), int(end) + 1):\\n                excluded.add((doc_id, str(i)))\\n    return excluded\\n\", 'read excluded spans path if path is none return set excluded set with open path as for in doc id start end strip split 3 for in range int start int end 1 excluded add doc id str return excluded', ''), ('__init__', 42, 'def __init__(self, system, queries, excluded_spans=None, mapping=None):\\n    self.system = system\\n    self.queries = queries\\n    self.excluded_offsets = read_excluded_spans(excluded_spans)\\n    self.mapping = read_mapping(mapping)\\n', 'init self system queries excluded spans none mapping none self system system self queries queries self excluded offsets read excluded spans excluded spans self mapping read mapping mapping', ''), ('__call__', 48, \"def __call__(self):\\n    return '\\\\n'.join(unicode(a) for a in self.annotations())\\n\", 'call self return join unicode for in self annotations', ''), ('add_arguments', 51, \"@classmethod\\ndef add_arguments(cls, p):\\n    p.add_argument('system', metavar='FILE', help='link annotations')\\n    p.add_argument('-q', '--queries', required=True, help='mention annotations'\\n        )\\n    p.add_argument('-x', '--excluded-spans', help=\\n        'file of spans to delete mentions in')\\n    p.add_argument('-m', '--mapping', help='mapping for titles')\\n    p.set_defaults(cls=cls)\\n    return p\\n\", 'add arguments cls add argument system metavar file help link annotations add argument queries required true help mention annotations add argument excluded spans help file of spans to delete mentions in add argument mapping help mapping for titles set defaults cls cls return', ''), ('annotations', 60, 'def annotations(self):\\n    \"\"\"Return list of annotation objects\"\"\"\\n    n_candidates = 0\\n    n_excluded = 0\\n    n_annotations = 0\\n    r = TacReader(self.system, self.queries)\\n    excluded = self.excluded_offsets\\n    for qid, docid, start, end, name, candidates in r:\\n        if (docid, start) in excluded or (docid, end) in excluded:\\n            n_excluded += 1\\n            continue\\n        if not candidates:\\n            raise ValueError(\\'No candidates found for query \\' + str(qid))\\n        n_candidates += len(candidates)\\n        mapped = list(apply_mapping(self.mapping, candidates))\\n        yield Annotation(docid, start, end, mapped)\\n        n_annotations += 1\\n    log.info(\\'Read {} candidates for {} annotations (excluded {}) from {}\\'.\\n        format(n_candidates, n_annotations, n_excluded, self.system))\\n', 'annotations self candidates 0 excluded 0 annotations 0 tacreader self system self queries excluded self excluded offsets for qid docid start end name candidates in if docid start in excluded or docid end in excluded excluded 1 continue if not candidates raise valueerror no candidates found for query str qid candidates len candidates mapped list apply mapping self mapping candidates yield annotation docid start end mapped annotations 1 log info read candidates for annotations excluded from format candidates annotations excluded self system', 'return list of annotation objects'), ('__init__', 120, 'def __init__(self, links_file, queries_file):\\n    self.links_file = links_file\\n    self.queries_file = queries_file\\n', 'init self links file queries file self links file links file self queries file queries file', ''), ('__iter__', 124, \"def __iter__(self):\\n    cdict = self.read_candidates()\\n    for (docid, start, end), queries in self.grouped_queries():\\n        qids, _, _, _, names = zip(*queries)\\n        candidates = sum((cdict.pop(qid, []) for qid in qids), [])\\n        candidates = sorted(candidates, reverse=True)\\n        yield qids, docid, start, end, names, candidates\\n    if cdict:\\n        raise ValueError('Remaining annotations unaligned to queries: {}'.\\n            format(cdict))\\n\", 'iter self cdict self read candidates for docid start end queries in self grouped queries qids names zip queries candidates sum cdict pop qid for qid in qids candidates sorted candidates reverse true yield qids docid start end names candidates if cdict raise valueerror remaining annotations unaligned to queries format cdict', ''), ('read_candidates', 137, 'def read_candidates(self):\\n    \"\"\"Return {qid: [(score, kbid, type)]} dictionary\"\"\"\\n    d = defaultdict(list)\\n    for line in utf8_open(self.links_file):\\n        cols = line.strip().split(\\'\\\\t\\')\\n        if len(cols) < 3:\\n            continue\\n        if cols[0] == \\'query_id\\':\\n            continue\\n        qid, kbid, type = cols[:3]\\n        score = 1.0 if len(cols) <= 3 else float(cols[3])\\n        d[qid].append(Candidate(kbid, score, type))\\n    return d\\n', 'read candidates self defaultdict list for line in open self links file cols line strip split if len cols 3 continue if cols 0 query id continue qid kbid type cols 3 score 1 0 if len cols 3 else float cols 3 qid append candidate kbid score type return', 'return qid'), ('iter_queries', 151, 'def iter_queries(self):\\n    \"\"\"Yield (qid, docid, start, end, name) tuples\"\"\"\\n    for event, elem in iterparse(self.queries_file):\\n        if elem.tag == QUERY_ELEM:\\n            yield self._query(elem)\\n', 'iter queries self for event elem in iterparse self queries file if elem tag query elem yield self query elem', 'yield qid docid start end name tuples'), ('grouped_queries', 157, 'def grouped_queries(self, key=operator.itemgetter(slice(1, 4))):\\n    return itertools.groupby(sorted(self.iter_queries(), key=key), key)\\n', 'grouped queries self key operator itemgetter slice 1 4 return itertools groupby sorted self iter queries key key key', ''), ('_query', 161, 'def _query(self, query_elem):\\n    \"\"\"Return (qid, docid, start, end, name) tuple\"\"\"\\n    qid = query_elem.get(QID_ATTR)\\n    d = {}\\n    for child in query_elem:\\n        d[child.tag] = child.text\\n    return qid, d[DOCID_ELEM], d[START_ELEM], d[END_ELEM], d[NAME_ELEM]\\n', 'query self query elem qid query elem get qid attr for child in query elem child tag child text return qid docid elem start elem end elem name elem', 'return qid docid start end name tuple'), ('__init__', 187, 'def __init__(self, system, excluded_spans=None, mapping=None):\\n    self.system = system\\n    self.excluded_offsets = read_excluded_spans(excluded_spans)\\n    self.mapping = read_mapping(mapping)\\n', 'init self system excluded spans none mapping none self system system self excluded offsets read excluded spans excluded spans self mapping read mapping mapping', ''), ('__call__', 192, \"def __call__(self):\\n    return '\\\\n'.join(unicode(a) for a in self.read_annotations(self.system))\\n\", 'call self return join unicode for in self read annotations self system', ''), ('add_arguments', 195, \"@classmethod\\ndef add_arguments(cls, p):\\n    p.add_argument('system', metavar='FILE', type=argparse.FileType('r'),\\n        help='link annotations')\\n    p.add_argument('-x', '--excluded-spans', help=\\n        'file of spans to delete mentions in')\\n    p.add_argument('-m', '--mapping', help='mapping of KB IDs to titles')\\n    p.set_defaults(cls=cls)\\n    return p\\n\", 'add arguments cls add argument system metavar file type argparse filetype help link annotations add argument excluded spans help file of spans to delete mentions in add argument mapping help mapping of kb ids to titles set defaults cls cls return', ''), ('_read_tab_delim', 203, \"@staticmethod\\ndef _read_tab_delim(f):\\n    for l in f:\\n        yield l.rstrip('\\\\n\\\\r').split('\\\\t')\\n\", 'read tab delim for in yield rstrip split', ''), ('read_annotations', 208, 'def read_annotations(self, f):\\n    \"\"\"Return list of annotation objects\"\"\"\\n    key_fn = operator.itemgetter(3)\\n    grouped = itertools.groupby(sorted(self._read_tab_delim(f), key=key_fn),\\n        key_fn)\\n    excluded = self.excluded_offsets\\n    n_candidates = 0\\n    n_annotations = 0\\n    n_excluded = 0\\n    for key, cand_data in grouped:\\n        docid, start, end = self.KEY_RE.match(key).groups()\\n        if (docid, start) in excluded or (docid, end) in excluded:\\n            n_excluded += 1\\n            continue\\n        cand_data = sorted(cand_data, key=lambda x: -float(x[7]))\\n        candidates = []\\n        for cand in cand_data:\\n            kbid, ne_type, mention_type, score = cand[4:8]\\n            type = \\'{}/{}\\'.format(ne_type, mention_type)\\n            candidates.append(Candidate(kbid, score, type))\\n        mapped = list(apply_mapping(self.mapping, candidates))\\n        yield Annotation(docid, start, end, mapped)\\n        n_annotations += 1\\n        n_candidates += len(mapped)\\n    log.info(\\'Read {} candidates for {} annotations (excluded {}) from {}\\'.\\n        format(n_candidates, n_annotations, n_excluded, self.system.name))\\n', 'read annotations self key fn operator itemgetter 3 grouped itertools groupby sorted self read tab delim key key fn key fn excluded self excluded offsets candidates 0 annotations 0 excluded 0 for key cand data in grouped docid start end self key re match key groups if docid start in excluded or docid end in excluded excluded 1 continue cand data sorted cand data key lambda float 7 candidates for cand in cand data kbid ne type mention type score cand 4 8 type format ne type mention type candidates append candidate kbid score type mapped list apply mapping self mapping candidates yield annotation docid start end mapped annotations 1 candidates len mapped log info read candidates for annotations excluded from format candidates annotations excluded self system name', 'return list of annotation objects')] [('__init__', 22, \"def __init__(self, input, name='NER Pipeline', withEntityAnnotation=False,\\n    withStanfordTagging=False, withNERFormatting=False):\\n    assert isinstance(withEntityAnnotation, bool)\\n    super(NERPipeline, self).__init__(input, name)\\n    self.__withEntityAnnotation = withEntityAnnotation\\n    self.__withStanfordTagging = withStanfordTagging\\n    self.__withNERFormatting = withNERFormatting\\n    self.addPipelinesBefore([(PosTaggerPipeline, {'name':\\n        'Pos Tagger Pipeline', 'input': {'source': 'main', 'data': input},\\n        'output': {'type': 'merge'}})])\\n    self.addTask((TaggerTagsTask(name='NER Task'), {'input': [{'key': 'pos',\\n        'source': 'internal-output', 'map-key': 'data'}], 'output': {'key':\\n        'entities', 'source': 'internal-output', 'type': 'json'}}))\\n    if self.__withStanfordTagging == True:\\n        self.addTask((StanfordTaggerTask(name='NER Task'), {'input': [{\\n            'key': 'json-loader', 'source': 'internal-output', 'map-key':\\n            'data'}], 'output': {'key': 'entities-stanford', 'source':\\n            'internal-output', 'type': 'json'}}))\\n    if self.__withNERFormatting == True:\\n        self.addTask((NERFormatTask(name='NER Format Task'), {'input': [{\\n            'key': 'entities', 'source': 'internal-output', 'map-key':\\n            'entities'}, {'key': 'entities-stanford', 'source':\\n            'internal-output', 'map-key': 'entities-stanford'}], 'output':\\n            {'key': 'entities-formatted', 'source': 'internal-output',\\n            'type': 'json'}}))\\n    if self.__withEntityAnnotation == True:\\n        self.addTask((EntityAnnotationTask(name='LinkedData TASK'), {\\n            'input': [{'key': 'entities', 'source': 'internal-output',\\n            'map-key': 'entities'}], 'output': {'key': 'entities-annotated',\\n            'source': 'internal-output', 'type': 'json'}}))\\n\", 'init self input name ner pipeline withentityannotation false withstanfordtagging false withnerformatting false assert isinstance withentityannotation bool super nerpipeline self init input name self withentityannotation withentityannotation self withstanfordtagging withstanfordtagging self withnerformatting withnerformatting self addpipelinesbefore postaggerpipeline name pos tagger pipeline input source main data input output type merge self addtask taggertagstask name ner task input key pos source internal output map key data output key entities source internal output type json if self withstanfordtagging true self addtask stanfordtaggertask name ner task input key json loader source internal output map key data output key entities stanford source internal output type json if self withnerformatting true self addtask nerformattask name ner format task input key entities source internal output map key entities key entities stanford source internal output map key entities stanford output key entities formatted source internal output type json if self withentityannotation true self addtask entityannotationtask name linkeddata task input key entities source internal output map key entities output key entities annotated source internal output type json', ''), ('execute', 50, 'def execute(self):\\n    super(NERPipeline, self).execute()\\n', 'execute self super nerpipeline self execute', '')] [('bin_sh', 27, \"def bin_sh():\\n    shellcode = '\\\\\\\\x31\\\\\\\\xC0'\\n    shellcode += '\\\\\\\\x50'\\n    shellcode += '\\\\\\\\x68\\\\\\\\x2F\\\\\\\\x2F\\\\\\\\x73\\\\\\\\x68'\\n    shellcode += '\\\\\\\\x68\\\\\\\\x2F\\\\\\\\x62\\\\\\\\x69\\\\\\\\x6E'\\n    shellcode += '\\\\\\\\x89\\\\\\\\xE3'\\n    shellcode += '\\\\\\\\x50\\\\\\\\x50\\\\\\\\x53'\\n    shellcode += '\\\\\\\\xB0\\\\\\\\x3B'\\n    shellcode += '\\\\\\\\x6A\\\\\\\\x2A'\\n    shellcode += '\\\\\\\\xCD\\\\\\\\x80'\\n    return shellcode\\n\", 'bin sh shellcode shellcode shellcode shellcode shellcode shellcode shellcode shellcode shellcode xcd return shellcode', '')] [('_test', 69, \"def _test():\\n    import os\\n    this_dir = os.path.dirname(globals()['__file__'])\\n    xml_path = this_dir + '/example/xml/'\\n    di = DoxyIndex(xml_path)\\n    aad = di.get_member('Aadvark')\\n    aad.brief_description\\n    import doctest\\n    return doctest.testmod()\\n\", 'test import os this dir os path dirname globals file xml path this dir example xml di doxyindex xml path aad di get member aadvark aad brief description import doctest return doctest testmod', '')] [('_has_timeout_settings', 42, 'def _has_timeout_settings(backoff_settings):\\n    return (backoff_settings.rpc_timeout_multiplier is not None and \\n        backoff_settings.max_rpc_timeout_millis is not None and \\n        backoff_settings.total_timeout_millis is not None and \\n        backoff_settings.initial_rpc_timeout_millis is not None)\\n', 'has timeout settings backoff settings return backoff settings rpc timeout multiplier is not none and backoff settings max rpc timeout millis is not none and backoff settings total timeout millis is not none and backoff settings initial rpc timeout millis is not none', ''), ('add_timeout_arg', 49, 'def add_timeout_arg(a_func, timeout, **kwargs):\\n    \"\"\"Updates a_func so that it gets called with the timeout as its final arg.\\n\\n    This converts a callable, a_func, into another callable with an additional\\n    positional arg.\\n\\n    Args:\\n      a_func (callable): a callable to be updated\\n      timeout (int): to be added to the original callable as it final positional\\n        arg.\\n      kwargs: Addtional arguments passed through to the callable.\\n\\n    Returns:\\n      callable: the original callable updated to the timeout arg\\n    \"\"\"\\n\\n    def inner(*args):\\n        \"\"\"Updates args with the timeout.\"\"\"\\n        updated_args = args + (timeout,)\\n        return a_func(*updated_args, **kwargs)\\n    return inner\\n', 'add timeout arg func timeout kwargs def inner args updates args with the timeout updated args args timeout return func updated args kwargs return inner', 'updates a func so that it gets called with the timeout as its final arg'), ('retryable', 73, 'def retryable(a_func, retry_options, **kwargs):\\n    \"\"\"Creates a function equivalent to a_func, but that retries on certain\\n    exceptions.\\n\\n    Args:\\n      a_func (callable): A callable.\\n      retry_options (RetryOptions): Configures the exceptions upon which the\\n        callable should retry, and the parameters to the exponential backoff\\n        retry algorithm.\\n      kwargs: Addtional arguments passed through to the callable.\\n\\n    Returns:\\n        Callable: A function that will retry on exception.\\n    \"\"\"\\n    delay_mult = retry_options.backoff_settings.retry_delay_multiplier\\n    max_delay_millis = retry_options.backoff_settings.max_retry_delay_millis\\n    has_timeout_settings = _has_timeout_settings(retry_options.backoff_settings\\n        )\\n    if has_timeout_settings:\\n        timeout_mult = retry_options.backoff_settings.rpc_timeout_multiplier\\n        max_timeout = (retry_options.backoff_settings.\\n            max_rpc_timeout_millis / _MILLIS_PER_SECOND)\\n        total_timeout = (retry_options.backoff_settings.\\n            total_timeout_millis / _MILLIS_PER_SECOND)\\n\\n    def inner(*args):\\n        \"\"\"Equivalent to ``a_func``, but retries upon transient failure.\\n\\n        Retrying is done through an exponential backoff algorithm configured\\n        by the options in ``retry``.\\n        \"\"\"\\n        delay = retry_options.backoff_settings.initial_retry_delay_millis\\n        exc = errors.RetryError(\\n            \\'Retry total timeout exceeded before anyresponse was received\\')\\n        if has_timeout_settings:\\n            timeout = (retry_options.backoff_settings.\\n                initial_rpc_timeout_millis / _MILLIS_PER_SECOND)\\n            now = time.time()\\n            deadline = now + total_timeout\\n        else:\\n            timeout = None\\n            deadline = None\\n        while deadline is None or now < deadline:\\n            try:\\n                to_call = add_timeout_arg(a_func, timeout, **kwargs)\\n                return to_call(*args)\\n            except Exception as exception:\\n                code = config.exc_to_code(exception)\\n                if code not in retry_options.retry_codes:\\n                    raise errors.RetryError(\\n                        \\'Exception occurred in retry method that was not classified as transient\\'\\n                        , exception)\\n                exc = errors.RetryError(\\n                    \\'Retry total timeout exceeded with exception\\', exception)\\n                to_sleep = random.uniform(0, delay * 2)\\n                time.sleep(to_sleep / _MILLIS_PER_SECOND)\\n                delay = min(delay * delay_mult, max_delay_millis)\\n                if has_timeout_settings:\\n                    now = time.time()\\n                    timeout = min(timeout * timeout_mult, max_timeout, \\n                        deadline - now)\\n        raise exc\\n    return inner\\n', 'retryable func retry options kwargs delay mult retry options backoff settings retry delay multiplier max delay millis retry options backoff settings max retry delay millis has timeout settings has timeout settings retry options backoff settings if has timeout settings timeout mult retry options backoff settings rpc timeout multiplier max timeout retry options backoff settings max rpc timeout millis millis per second total timeout retry options backoff settings total timeout millis millis per second def inner args equivalent to func but retries upon transient failure retrying is done through an exponential backoff algorithm configured by the options in retry delay retry options backoff settings initial retry delay millis exc errors retryerror retry total timeout exceeded before anyresponse was received if has timeout settings timeout retry options backoff settings initial rpc timeout millis millis per second now time time deadline now total timeout else timeout none deadline none while deadline is none or now deadline try to call add timeout arg func timeout kwargs return to call args except exception as exception code config exc to code exception if code not in retry options retry codes raise errors retryerror exception occurred in retry method that was not classified as transient exception exc errors retryerror retry total timeout exceeded with exception exception to sleep random uniform 0 delay 2 time sleep to sleep millis per second delay min delay delay mult max delay millis if has timeout settings now time time timeout min timeout timeout mult max timeout deadline now raise exc return inner', 'creates a function equivalent to a func but that retries on certain exceptions')] [('build_pattern', 53, \"def build_pattern(self):\\n    return '|'.join(build_pattern(self.mapping))\\n\", 'build pattern self return join build pattern self mapping', ''), ('compile_pattern', 56, 'def compile_pattern(self):\\n    self.PATTERN = self.build_pattern()\\n    super(FixImports, self).compile_pattern()\\n', 'compile pattern self self pattern self build pattern super fiximports self compile pattern', ''), ('match', 63, \"def match(self, node):\\n    match = super(FixImports, self).match\\n    results = match(node)\\n    if results:\\n        if 'bare_with_attr' not in results and any(match(obj) for obj in\\n            attr_chain(node, 'parent')):\\n            return False\\n        return results\\n    return False\\n\", 'match self node match super fiximports self match results match node if results if bare with attr not in results and any match obj for obj in attr chain node parent return false return results return false', ''), ('start_tree', 75, 'def start_tree(self, tree, filename):\\n    super(FixImports, self).start_tree(tree, filename)\\n    self.replace = {}\\n', 'start tree self tree filename super fiximports self start tree tree filename self replace', ''), ('transform', 79, \"def transform(self, node, results):\\n    import_mod = results.get('module_name')\\n    if import_mod:\\n        mod_name = import_mod.value\\n        new_name = unicode(self.mapping[mod_name])\\n        import_mod.replace(Name(new_name, prefix=import_mod.prefix))\\n        if 'name_import' in results:\\n            self.replace[mod_name] = new_name\\n        if 'multiple_imports' in results:\\n            results = self.match(node)\\n            if results:\\n                self.transform(node, results)\\n    else:\\n        import pdb\\n        pdb.set_trace()\\n        bare_name = results['bare_with_attr'][0]\\n        new_name = self.replace.get(bare_name.value)\\n        if new_name:\\n            bare_name.replace(Name(new_name, prefix=bare_name.prefix))\\n\", 'transform self node results import mod results get module name if import mod mod name import mod value new name unicode self mapping mod name import mod replace name new name prefix import mod prefix if name import in results self replace mod name new name if multiple imports in results results self match node if results self transform node results else import pdb pdb set trace bare name results bare with attr 0 new name self replace get bare name value if new name bare name replace name new name prefix bare name prefix', '')] [] [('setUpClass', 11, '@classmethod\\ndef setUpClass(self):\\n    \"\"\"Set up function called when class is consructed.\"\"\"\\n    self.test_case_list = []\\n', 'setupclass self self test case list', 'set up function called when class is consructed'), ('test_create', 16, \"def test_create(self):\\n    test_case_name = test_utils.get_test_case_name(self.test_case_list)\\n    batch_owner = Operation('ModelBatchOwner').create(name=test_case_name)\\n    self.test_case_list.append({'class': 'ModelBatchOwner', 'test_case':\\n        test_case_name})\\n    self.assertEqual(batch_owner.name, test_case_name)\\n\", 'test create self test case name test utils get test case name self test case list batch owner operation modelbatchowner create name test case name self test case list append class modelbatchowner test case test case name self assertequal batch owner name test case name', ''), ('test_read', 23, \"def test_read(self):\\n    test_case_name = test_utils.get_test_case_name(self.test_case_list)\\n    Operation('ModelBatchOwner').create(name=test_case_name)\\n    batch_owner_list = Operation('ModelBatchOwner').read(name=test_case_name)\\n    self.test_case_list.append({'class': 'ModelBatchOwner', 'test_case':\\n        test_case_name})\\n    self.assertEqual(batch_owner_list[0].name, test_case_name)\\n\", 'test read self test case name test utils get test case name self test case list operation modelbatchowner create name test case name batch owner list operation modelbatchowner read name test case name self test case list append class modelbatchowner test case test case name self assertequal batch owner list 0 name test case name', ''), ('test_update', 31, \"def test_update(self):\\n    test_case_name = test_utils.get_test_case_name(self.test_case_list)\\n    test_case_name_new = test_utils.get_test_case_name(self.test_case_list)\\n    batch_owner = Operation('ModelBatchOwner').create(name=test_case_name)\\n    batch_owner = Operation('ModelBatchOwner').update(id=batch_owner.id,\\n        name=test_case_name_new)\\n    self.test_case_list.append({'class': 'ModelBatchOwner', 'test_case':\\n        test_case_name_new})\\n    self.assertEqual(batch_owner.name, test_case_name_new)\\n\", 'test update self test case name test utils get test case name self test case list test case name new test utils get test case name self test case list batch owner operation modelbatchowner create name test case name batch owner operation modelbatchowner update id batch owner id name test case name new self test case list append class modelbatchowner test case test case name new self assertequal batch owner name test case name new', ''), ('test_delete', 40, \"def test_delete(self):\\n    test_case_name = test_utils.get_test_case_name(self.test_case_list)\\n    batch_owner = Operation('ModelBatchOwner').create(name=test_case_name)\\n    Operation('ModelBatchOwner').delete(id=batch_owner.id)\\n    batch_owner_list = Operation('ModelBatchOwner').read(name=test_case_name)\\n    self.assertEqual(batch_owner_list, [])\\n\", 'test delete self test case name test utils get test case name self test case list batch owner operation modelbatchowner create name test case name operation modelbatchowner delete id batch owner id batch owner list operation modelbatchowner read name test case name self assertequal batch owner list', ''), ('tearDownClass', 48, '@classmethod\\ndef tearDownClass(self):\\n    \"\"\"Tear down function called when class is deconstructed.\"\"\"\\n    for test_case in self.test_case_list:\\n        Operation(test_case[\\'class\\']).delete(name=test_case[\\'test_case\\'])\\n', 'teardownclass self for test case in self test case list operation test case class delete name test case test case', 'tear down function called when class is deconstructed')] [('test_humanized_time', 16, 'def test_humanized_time(self):\\n    \"\"\"\\n        tests the humanized_time utility function against different values.\\n        \"\"\"\\n    human_time = humanized_time(0)\\n    self.assertEqual(human_time, \\'0 minutes\\')\\n    human_time = humanized_time(1)\\n    self.assertEqual(human_time, \\'1 minute\\')\\n    human_time = humanized_time(10)\\n    self.assertEqual(human_time, \\'10 minutes\\')\\n    human_time = humanized_time(60)\\n    self.assertEqual(human_time, \\'1 hour\\')\\n    human_time = humanized_time(61)\\n    self.assertEqual(human_time, \\'1 hour and 1 minute\\')\\n    human_time = humanized_time(62)\\n    self.assertEqual(human_time, \\'1 hour and 2 minutes\\')\\n    human_time = humanized_time(120)\\n    self.assertEqual(human_time, \\'2 hours\\')\\n    human_time = humanized_time(121)\\n    self.assertEqual(human_time, \\'2 hours and 1 minute\\')\\n    human_time = humanized_time(150)\\n    self.assertEqual(human_time, \\'2 hours and 30 minutes\\')\\n    human_time = humanized_time(180)\\n    self.assertEqual(human_time, \\'3 hours\\')\\n    human_time = humanized_time(-60)\\n    self.assertEqual(human_time, \\'error\\')\\n', 'test humanized time self human time humanized time 0 self assertequal human time 0 minutes human time humanized time 1 self assertequal human time 1 minute human time humanized time 10 self assertequal human time 10 minutes human time humanized time 60 self assertequal human time 1 hour human time humanized time 61 self assertequal human time 1 hour and 1 minute human time humanized time 62 self assertequal human time 1 hour and 2 minutes human time humanized time 120 self assertequal human time 2 hours human time humanized time 121 self assertequal human time 2 hours and 1 minute human time humanized time 150 self assertequal human time 2 hours and 30 minutes human time humanized time 180 self assertequal human time 3 hours human time humanized time 60 self assertequal human time error', 'tests the humanized time utility function against different values'), ('test_emit_event', 58, 'def test_emit_event(self):\\n    \"\"\"\\n        Call through to emit event to the analytics pipeline.\\n        NOTE: We\\'re just testing one specific case where the context is None\\n        We get full coverage on other cases, via the test_api.py file\\n        \"\"\"\\n    _emit_event(\\'foo.bar\\', None, {\\'one\\': \\'two\\'})\\n', 'test emit event self emit event foo bar none one two', 'call through to emit event to the analytics pipeline note')] []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('one', 'w') as f:\n",
        "      \n",
        "    # using csv.writer method from CSV package\n",
        "    write = csv.writer(f)\n",
        "      \n",
        "    write.writerow(fields)\n",
        "    write.writerows(rows)"
      ],
      "metadata": {
        "id": "60aTl6z2UUEC",
        "outputId": "4c75947d-4618-431b-e6b8-97a1078645f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c42a15b2c7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'head'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "func_doc_list.append(get_function_docstring_pairs_list(df_list[1].content.tolist()))\n",
        "func_doc_list[1].to_csv('list_1.csv')"
      ],
      "metadata": {
        "id": "ey4pEqpopDWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_doc_list.append(get_function_docstring_pairs_list(df_list[2].content.tolist()))\n",
        "func_doc_list[2].to_csv('list_2.csv')"
      ],
      "metadata": {
        "id": "SLSXygUkpDT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_doc_list.append(get_function_docstring_pairs_list(df_list[3].content.tolist()))\n",
        "func_doc_list[3].to_csv('list_3.csv')"
      ],
      "metadata": {
        "id": "cxKflEoppDRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_doc_list.append(get_function_docstring_pairs_list(df_list[4].content.tolist()))\n",
        "func_doc_list[4].to_csv('list_4.csv')"
      ],
      "metadata": {
        "id": "RD5_fNBepDOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_doc_list.append(get_function_docstring_pairs_list(df_list[5].content.tolist()))\n",
        "func_doc_list[5].to_csv('list_5.csv')"
      ],
      "metadata": {
        "id": "7kCZStY6pDKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_doc_list.append(get_function_docstring_pairs_list(df_list[6].content.tolist()))\n",
        "func_doc_list[6].to_csv('list_6.csv')"
      ],
      "metadata": {
        "id": "2NxN8-chpDBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_doc_list.append(get_function_docstring_pairs_list(df_list[7].content.tolist()))\n",
        "func_doc_list[7].to_csv('list_7.csv')"
      ],
      "metadata": {
        "id": "1xonUD7lpOuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_doc_list.append(get_function_docstring_pairs_list(df_list[8].content.tolist()))\n",
        "func_doc_list[8].to_csv('list_8.csv')"
      ],
      "metadata": {
        "id": "pOWk-M9OpOcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_doc_list.append(get_function_docstring_pairs_list(df_list[9].content.tolist()))\n",
        "func_doc_list[9].to_csv('list_9.csv')"
      ],
      "metadata": {
        "id": "vRnzL-RHpN8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(func_doc_list)"
      ],
      "metadata": {
        "id": "10MSi2VxceMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat(func_doc_list, axis=0, ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uvrabv4xsMeW",
        "outputId": "1d11dc64-3cf9-4c37-b000-0cc3307249df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e818965a6d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_doc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "KSY9Y2O3cS64",
        "outputId": "c0ccd942-06f6-4582-f514-f30d4c72964f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c42a15b2c7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttqlk3702R4G",
        "outputId": "4f4fa630-efcc-4bd9-880c-d0803c13360f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123998"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(func_doc) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "vo6cNSP52R4G",
        "outputId": "132d2e02-47d5-4e9d-bda6-2e3eafa738f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     nwo                                          path  \\\n",
              "0    bitsanity/rateboard                               krakenticker.py   \n",
              "1  rusty1s/embedded_gcnn                             lib/tf/convert.py   \n",
              "2          mackorone/mms                               util/ttf2png.py   \n",
              "3     nicksergeant/snipt                            accounts/models.py   \n",
              "4     huaxz1986/git_book  chapters/Model_Selection/validation_curve.py   \n",
              "\n",
              "                                             content  \\\n",
              "0  #!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n\\n...   \n",
              "1  import numpy as np\\nimport tensorflow as tf\\n\\...   \n",
              "2  import os\\nimport sys\\nimport string\\n\\ndef en...   \n",
              "3  from annoying.functions import get_object_or_N...   \n",
              "4  # -*- coding: utf-8 -*-\\n\"\"\"\\n    模型选择\\n    ~~...   \n",
              "\n",
              "                                               pairs  \n",
              "0                                                 []  \n",
              "1  [(sparse_to_tensor, 5, def sparse_to_tensor(va...  \n",
              "2  [(end_of_path_index, 5, def end_of_path_index(...  \n",
              "3  [(get_blog_posts, 84, def get_blog_posts(self)...  \n",
              "4  [(test_validation_curve, 17, def test_validati...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-892ff961-e062-42a3-98c2-0f8f07367a3b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nwo</th>\n",
              "      <th>path</th>\n",
              "      <th>content</th>\n",
              "      <th>pairs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bitsanity/rateboard</td>\n",
              "      <td>krakenticker.py</td>\n",
              "      <td>#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n\\n...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rusty1s/embedded_gcnn</td>\n",
              "      <td>lib/tf/convert.py</td>\n",
              "      <td>import numpy as np\\nimport tensorflow as tf\\n\\...</td>\n",
              "      <td>[(sparse_to_tensor, 5, def sparse_to_tensor(va...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mackorone/mms</td>\n",
              "      <td>util/ttf2png.py</td>\n",
              "      <td>import os\\nimport sys\\nimport string\\n\\ndef en...</td>\n",
              "      <td>[(end_of_path_index, 5, def end_of_path_index(...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nicksergeant/snipt</td>\n",
              "      <td>accounts/models.py</td>\n",
              "      <td>from annoying.functions import get_object_or_N...</td>\n",
              "      <td>[(get_blog_posts, 84, def get_blog_posts(self)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>huaxz1986/git_book</td>\n",
              "      <td>chapters/Model_Selection/validation_curve.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\n    模型选择\\n    ~~...</td>\n",
              "      <td>[(test_validation_curve, 17, def test_validati...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-892ff961-e062-42a3-98c2-0f8f07367a3b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-892ff961-e062-42a3-98c2-0f8f07367a3b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-892ff961-e062-42a3-98c2-0f8f07367a3b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# The dataset containing a the name of the directory, its path, source code, and all the functions in a list extracted from the source code\n",
        "df['pairs'] = func_doc\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8hjZ2EL2R4H"
      },
      "source": [
        "### Preparing the dataset\n",
        "Once we have extracted our function-docstring pairs and their tokens which are free from decorators and other unwanted elements,we stack our findings in a data-frame with every row containing details about a function and its corresponding docstring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0pdgD9e2R4H",
        "outputId": "d7953ef5-36d5-4e97-961f-c3f28eff283b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 48.8 s, sys: 2.53 s, total: 51.3 s\n",
            "Wall time: 49.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# flatten pairs\n",
        "df = df.set_index(['nwo', 'path'])['pairs'].apply(pd.Series).stack()\n",
        "df = df.reset_index()\n",
        "df.columns = ['nwo', 'path', '_', 'pair']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDPHeozY2R4H",
        "outputId": "9f8e7adc-eb13-410a-f6ed-8e5ef66c0471"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     nwo               path  _  \\\n",
              "0  rusty1s/embedded_gcnn  lib/tf/convert.py  0   \n",
              "1          mackorone/mms    util/ttf2png.py  0   \n",
              "2          mackorone/mms    util/ttf2png.py  1   \n",
              "3          mackorone/mms    util/ttf2png.py  2   \n",
              "4          mackorone/mms    util/ttf2png.py  3   \n",
              "\n",
              "                                                pair  \n",
              "0  (sparse_to_tensor, 5, def sparse_to_tensor(val...  \n",
              "1  (end_of_path_index, 5, def end_of_path_index(f...  \n",
              "2  (get_path, 8, def get_path(full_path):\\n    re...  \n",
              "3  (get_name, 11, def get_name(full_path):\\n    r...  \n",
              "4  (ttf2png, 14, def ttf2png(chars_path, font_pat...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7335c164-7a81-4f43-a9b7-de9f1a71bd93\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nwo</th>\n",
              "      <th>path</th>\n",
              "      <th>_</th>\n",
              "      <th>pair</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rusty1s/embedded_gcnn</td>\n",
              "      <td>lib/tf/convert.py</td>\n",
              "      <td>0</td>\n",
              "      <td>(sparse_to_tensor, 5, def sparse_to_tensor(val...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mackorone/mms</td>\n",
              "      <td>util/ttf2png.py</td>\n",
              "      <td>0</td>\n",
              "      <td>(end_of_path_index, 5, def end_of_path_index(f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mackorone/mms</td>\n",
              "      <td>util/ttf2png.py</td>\n",
              "      <td>1</td>\n",
              "      <td>(get_path, 8, def get_path(full_path):\\n    re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mackorone/mms</td>\n",
              "      <td>util/ttf2png.py</td>\n",
              "      <td>2</td>\n",
              "      <td>(get_name, 11, def get_name(full_path):\\n    r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mackorone/mms</td>\n",
              "      <td>util/ttf2png.py</td>\n",
              "      <td>3</td>\n",
              "      <td>(ttf2png, 14, def ttf2png(chars_path, font_pat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7335c164-7a81-4f43-a9b7-de9f1a71bd93')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7335c164-7a81-4f43-a9b7-de9f1a71bd93 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7335c164-7a81-4f43-a9b7-de9f1a71bd93');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsLayciE2R4H",
        "outputId": "8a046a68-62d2-469d-c124-372940286270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12.2 s, sys: 118 ms, total: 12.3 s\n",
            "Wall time: 14.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# gathering all detailes about the function and its corresponding docstring\n",
        "df['function_name'] = df['pair'].apply(lambda p: p[0])\n",
        "df['lineno'] = df['pair'].apply(lambda p: p[1])\n",
        "df['original_function'] = df['pair'].apply(lambda p: p[2])\n",
        "df['function_tokens'] = df['pair'].apply(lambda p: p[3])\n",
        "df['docstring_tokens'] = df['pair'].apply(lambda p: p[4])\n",
        "df = df[['nwo', 'path', 'function_name', 'lineno', 'original_function', 'function_tokens', 'docstring_tokens']]\n",
        "df['url'] = df[['nwo', 'path', 'lineno']].apply(lambda x: 'https://github.com/{}/blob/master/{}#L{}'.format(x[0], x[1], x[2]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxI7wdPw2R4I",
        "outputId": "8f934fa4-2049-49bd-adf0-51ab58af5ee1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     nwo               path      function_name  lineno  \\\n",
              "0  rusty1s/embedded_gcnn  lib/tf/convert.py   sparse_to_tensor       5   \n",
              "1          mackorone/mms    util/ttf2png.py  end_of_path_index       5   \n",
              "2          mackorone/mms    util/ttf2png.py           get_path       8   \n",
              "3          mackorone/mms    util/ttf2png.py           get_name      11   \n",
              "4          mackorone/mms    util/ttf2png.py            ttf2png      14   \n",
              "\n",
              "                                   original_function  \\\n",
              "0  def sparse_to_tensor(value):\\n    \"\"\"Convert a...   \n",
              "1  def end_of_path_index(full_path):\\n    return ...   \n",
              "2  def get_path(full_path):\\n    return full_path...   \n",
              "3  def get_name(full_path):\\n    return full_path...   \n",
              "4  def ttf2png(chars_path, font_path, dest_path):...   \n",
              "\n",
              "                                     function_tokens  \\\n",
              "0  sparse to tensor value row np reshape value ro...   \n",
              "1  end of path index full path return full path r...   \n",
              "2  get path full path return full path end of pat...   \n",
              "3  get name full path return full path end of pat...   \n",
              "4  chars path font path dest path options backgro...   \n",
              "\n",
              "                                    docstring_tokens  \\\n",
              "0  convert a scipy sparse matrix to a tensorflow ...   \n",
              "1                                                      \n",
              "2                                                      \n",
              "3                                                      \n",
              "4                                                      \n",
              "\n",
              "                                                 url  \n",
              "0  https://github.com/rusty1s/embedded_gcnn/blob/...  \n",
              "1  https://github.com/mackorone/mms/blob/master/u...  \n",
              "2  https://github.com/mackorone/mms/blob/master/u...  \n",
              "3  https://github.com/mackorone/mms/blob/master/u...  \n",
              "4  https://github.com/mackorone/mms/blob/master/u...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f3744706-013f-4d7f-b6b7-cce49339907b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nwo</th>\n",
              "      <th>path</th>\n",
              "      <th>function_name</th>\n",
              "      <th>lineno</th>\n",
              "      <th>original_function</th>\n",
              "      <th>function_tokens</th>\n",
              "      <th>docstring_tokens</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rusty1s/embedded_gcnn</td>\n",
              "      <td>lib/tf/convert.py</td>\n",
              "      <td>sparse_to_tensor</td>\n",
              "      <td>5</td>\n",
              "      <td>def sparse_to_tensor(value):\\n    \"\"\"Convert a...</td>\n",
              "      <td>sparse to tensor value row np reshape value ro...</td>\n",
              "      <td>convert a scipy sparse matrix to a tensorflow ...</td>\n",
              "      <td>https://github.com/rusty1s/embedded_gcnn/blob/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mackorone/mms</td>\n",
              "      <td>util/ttf2png.py</td>\n",
              "      <td>end_of_path_index</td>\n",
              "      <td>5</td>\n",
              "      <td>def end_of_path_index(full_path):\\n    return ...</td>\n",
              "      <td>end of path index full path return full path r...</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/mackorone/mms/blob/master/u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mackorone/mms</td>\n",
              "      <td>util/ttf2png.py</td>\n",
              "      <td>get_path</td>\n",
              "      <td>8</td>\n",
              "      <td>def get_path(full_path):\\n    return full_path...</td>\n",
              "      <td>get path full path return full path end of pat...</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/mackorone/mms/blob/master/u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mackorone/mms</td>\n",
              "      <td>util/ttf2png.py</td>\n",
              "      <td>get_name</td>\n",
              "      <td>11</td>\n",
              "      <td>def get_name(full_path):\\n    return full_path...</td>\n",
              "      <td>get name full path return full path end of pat...</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/mackorone/mms/blob/master/u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mackorone/mms</td>\n",
              "      <td>util/ttf2png.py</td>\n",
              "      <td>ttf2png</td>\n",
              "      <td>14</td>\n",
              "      <td>def ttf2png(chars_path, font_path, dest_path):...</td>\n",
              "      <td>chars path font path dest path options backgro...</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/mackorone/mms/blob/master/u...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3744706-013f-4d7f-b6b7-cce49339907b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f3744706-013f-4d7f-b6b7-cce49339907b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f3744706-013f-4d7f-b6b7-cce49339907b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC5_72Bo2R4I"
      },
      "source": [
        "### Removing Duplicates\n",
        "All entries which were duplicates with respect to function definitions or function tokens were removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LyCWG5D2R4I",
        "outputId": "335615e3-d9b9-4db7-ce40-f01f928bfe98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 38,409 duplicate rows\n",
            "CPU times: user 1.34 s, sys: 5.97 ms, total: 1.35 s\n",
            "Wall time: 1.56 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# remove observations where the same function definition appears more than once\n",
        "before_dedup = len(df)\n",
        "df = df.drop_duplicates(['original_function'])\n",
        "after_dedup = len(df)\n",
        "\n",
        "print(f'Removed {before_dedup - after_dedup:,} duplicate rows')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm3Fmiq42R4I",
        "outputId": "ba78043c-9d1d-4ccb-fb84-04fe4afa6a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 9,978 duplicate rows\n",
            "CPU times: user 854 ms, sys: 6.93 ms, total: 861 ms\n",
            "Wall time: 802 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# remove observations which have the same function tokens appears more than once\n",
        "before_dedup = len(df)\n",
        "df = df.drop_duplicates(['function_tokens'])\n",
        "after_dedup = len(df)\n",
        "\n",
        "print(f'Removed {before_dedup - after_dedup:,} duplicate rows')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I___Ofou2R4I",
        "outputId": "95768294-8de3-4c89-f2c1-a55e54d65d01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(611875, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dae6G57J2R4J"
      },
      "source": [
        "### Separate function without docstrings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMR8aIiC2R4J",
        "outputId": "0f78ed3c-f5ef-4886-f068-4aa422ec557e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Funtion Snippets with docstring 143373\n",
            "Number of Funtion Snippets without docstring 468502\n"
          ]
        }
      ],
      "source": [
        "def listlen(x):\n",
        "    if not isinstance(x, list):\n",
        "        return 0\n",
        "    return len(x)\n",
        "\n",
        "# separate functions without docstrings\n",
        "# docstrings should be at least 3 words in the docstring to be considered a valid docstring\n",
        "\n",
        "with_docstrings = df[df.docstring_tokens.str.split().apply(listlen) >= 3]\n",
        "without_docstrings = df[df.docstring_tokens.str.split().apply(listlen) < 3]\n",
        "print('Number of Funtion Snippets with docstring',len(with_docstrings))\n",
        "print('Number of Funtion Snippets without docstring',len(without_docstrings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOt-QVGX2R4J"
      },
      "outputs": [],
      "source": [
        "# Save the dataset of functions with docstrings\n",
        "with_docstrings.to_csv('processed_full.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5lJJogK2R4J"
      },
      "outputs": [],
      "source": [
        "# Using SQLlite to analyse the dataset\n",
        "conn = sqlite3.connect('with_docstrings.sqlite')\n",
        "c=conn.cursor()\n",
        "conn.text_factory = str\n",
        "with_docstrings.to_sql('Data', conn, schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "iTDiK7R22R4J",
        "outputId": "3271794d-1fbc-4090-f9ea-bfc876083ceb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       nwo                                               path  \\\n",
              "0    rusty1s/embedded_gcnn                                  lib/tf/convert.py   \n",
              "13      huaxz1986/git_book       chapters/Model_Selection/validation_curve.py   \n",
              "14  xuleiboy1234/autoTitle  tensorflow/tensorflow/contrib/cloud/python/ops...   \n",
              "15  xuleiboy1234/autoTitle  tensorflow/tensorflow/contrib/cloud/python/ops...   \n",
              "16  xuleiboy1234/autoTitle  tensorflow/tensorflow/contrib/cloud/python/ops...   \n",
              "\n",
              "                function_name  lineno  \\\n",
              "0            sparse_to_tensor       5   \n",
              "13      test_validation_curve      17   \n",
              "14  _ConvertRowToExampleProto      77   \n",
              "15                _SetUpQueue     172   \n",
              "16                   __init__     104   \n",
              "\n",
              "                                    original_function  \\\n",
              "0   def sparse_to_tensor(value):\\n    \"\"\"Convert a...   \n",
              "13  def test_validation_curve():\\n    \"\"\"\\n    测试 ...   \n",
              "14  def _ConvertRowToExampleProto(row):\\n    \"\"\"Co...   \n",
              "15  def _SetUpQueue(reader):\\n    \"\"\"Sets up a que...   \n",
              "16  def __init__(self, address, port):\\n    \"\"\"Cre...   \n",
              "\n",
              "                                      function_tokens  \\\n",
              "0   sparse to tensor value row np reshape value ro...   \n",
              "13  test validation curve digits load digits digit...   \n",
              "14  convertrowtoexampleproto row example example e...   \n",
              "15  setupqueue reader queue data flow ops fifoqueu...   \n",
              "16  init self address port threading thread init s...   \n",
              "\n",
              "                                     docstring_tokens  \\\n",
              "0   convert a scipy sparse matrix to a tensorflow ...   \n",
              "13                       validation curve linearsvc c   \n",
              "14         converts the input row to an example proto   \n",
              "15                       sets up a queue for a reader   \n",
              "16                       creates a fakebigqueryserver   \n",
              "\n",
              "                                                  url  \n",
              "0   https://github.com/rusty1s/embedded_gcnn/blob/...  \n",
              "13  https://github.com/huaxz1986/git_book/blob/mas...  \n",
              "14  https://github.com/xuleiboy1234/autoTitle/blob...  \n",
              "15  https://github.com/xuleiboy1234/autoTitle/blob...  \n",
              "16  https://github.com/xuleiboy1234/autoTitle/blob...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d44a2c75-4020-4111-9374-2fa50eac678f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nwo</th>\n",
              "      <th>path</th>\n",
              "      <th>function_name</th>\n",
              "      <th>lineno</th>\n",
              "      <th>original_function</th>\n",
              "      <th>function_tokens</th>\n",
              "      <th>docstring_tokens</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rusty1s/embedded_gcnn</td>\n",
              "      <td>lib/tf/convert.py</td>\n",
              "      <td>sparse_to_tensor</td>\n",
              "      <td>5</td>\n",
              "      <td>def sparse_to_tensor(value):\\n    \"\"\"Convert a...</td>\n",
              "      <td>sparse to tensor value row np reshape value ro...</td>\n",
              "      <td>convert a scipy sparse matrix to a tensorflow ...</td>\n",
              "      <td>https://github.com/rusty1s/embedded_gcnn/blob/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>huaxz1986/git_book</td>\n",
              "      <td>chapters/Model_Selection/validation_curve.py</td>\n",
              "      <td>test_validation_curve</td>\n",
              "      <td>17</td>\n",
              "      <td>def test_validation_curve():\\n    \"\"\"\\n    测试 ...</td>\n",
              "      <td>test validation curve digits load digits digit...</td>\n",
              "      <td>validation curve linearsvc c</td>\n",
              "      <td>https://github.com/huaxz1986/git_book/blob/mas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>xuleiboy1234/autoTitle</td>\n",
              "      <td>tensorflow/tensorflow/contrib/cloud/python/ops...</td>\n",
              "      <td>_ConvertRowToExampleProto</td>\n",
              "      <td>77</td>\n",
              "      <td>def _ConvertRowToExampleProto(row):\\n    \"\"\"Co...</td>\n",
              "      <td>convertrowtoexampleproto row example example e...</td>\n",
              "      <td>converts the input row to an example proto</td>\n",
              "      <td>https://github.com/xuleiboy1234/autoTitle/blob...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>xuleiboy1234/autoTitle</td>\n",
              "      <td>tensorflow/tensorflow/contrib/cloud/python/ops...</td>\n",
              "      <td>_SetUpQueue</td>\n",
              "      <td>172</td>\n",
              "      <td>def _SetUpQueue(reader):\\n    \"\"\"Sets up a que...</td>\n",
              "      <td>setupqueue reader queue data flow ops fifoqueu...</td>\n",
              "      <td>sets up a queue for a reader</td>\n",
              "      <td>https://github.com/xuleiboy1234/autoTitle/blob...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>xuleiboy1234/autoTitle</td>\n",
              "      <td>tensorflow/tensorflow/contrib/cloud/python/ops...</td>\n",
              "      <td>__init__</td>\n",
              "      <td>104</td>\n",
              "      <td>def __init__(self, address, port):\\n    \"\"\"Cre...</td>\n",
              "      <td>init self address port threading thread init s...</td>\n",
              "      <td>creates a fakebigqueryserver</td>\n",
              "      <td>https://github.com/xuleiboy1234/autoTitle/blob...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d44a2c75-4020-4111-9374-2fa50eac678f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d44a2c75-4020-4111-9374-2fa50eac678f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d44a2c75-4020-4111-9374-2fa50eac678f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "with_docstrings.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4pQdCj22R4J"
      },
      "outputs": [],
      "source": [
        "# Number of unique repositories\n",
        "print(\"Number of repositories -\",len(list(set(with_docstrings['nwo']))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avcI9lLF2R4K",
        "outputId": "ad8cb9bb-02ea-439a-c164-2159ab16e922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# Getting the count of function tokens per function\n",
        "with_docstrings['function_tokens_count'] = [len(item.split()) for item in list(with_docstrings['function_tokens'].values)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoXA5ytM2R4K",
        "outputId": "6491633b-5c36-4695-e6fc-f63983bed304"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "143373"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "len(with_docstrings['function_tokens_count'] .values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "X15zTtOx2R4K",
        "outputId": "b3d1038a-e364-4222-dcf0-2642a0b05e33"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 684x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plotting the graph of sorted list of function tokens\n",
        "plt.plot(sorted(list(with_docstrings['function_tokens_count'] .values)))\n",
        "plt.ylabel('function_tokens_count')\n",
        "plt.rcParams['figure.figsize'] = [9.5, 6]\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6XQNduX2R4K"
      },
      "outputs": [],
      "source": [
        "# Analysing what must be the minimum count of function tokens by seeing the values returned in sql queries\n",
        "# KN: Error probably because of remove favor in cell 27\n",
        "\n",
        "#final = pd.read_sql_query(\"\"\"\n",
        "#SELECT *\n",
        "#FROM Data\n",
        "#WHERE function_tokens_count > 4\n",
        "#ORDER BY function_tokens_count\n",
        "#\"\"\", conn)\n",
        "#final[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWx4jUTJ2R4K",
        "outputId": "b1f93ede-8caa-4b9c-b7cf-957c616a93ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(143373, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "final.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB9WL2oC2R4L"
      },
      "outputs": [],
      "source": [
        "conn = sqlite3.connect('with_docstrings.sqlite')\n",
        "c=conn.cursor()\n",
        "conn.text_factory = str\n",
        "final.to_sql('Modified', conn, schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-LIN09Z2R4L"
      },
      "outputs": [],
      "source": [
        "# checking if functions which are overridden are of importance to us\n",
        "#check = pd.read_sql_query(\"\"\"\n",
        "#SELECT *\n",
        "#FROM Modified\n",
        "#WHERE function_name LIKE '!_!_%' ESCAPE'!'\n",
        "#ORDER BY function_tokens_count\n",
        "#\"\"\", conn)\n",
        "#check.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8Z7ZoFl2R4L"
      },
      "outputs": [],
      "source": [
        "final.to_csv('processed_full2.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__27aTSE2R4L"
      },
      "source": [
        "## Train-Validation-Test Split\n",
        "It was decided not to randomly split the data set instead to group the entries according to the repositories they belong and then split the data set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbtIVoj82R4L"
      },
      "outputs": [],
      "source": [
        "#Grouping entries by thie repository name\n",
        "grouped = final.groupby('nwo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMJASznH2R4L"
      },
      "outputs": [],
      "source": [
        "# train, valid, test splits\n",
        "train, valid = train_test_split(list(grouped), train_size=0.9, random_state=8081)\n",
        "train, test = train_test_split(train, train_size=0.9, random_state=8081)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxZgu8tA2R4L"
      },
      "outputs": [],
      "source": [
        "train = pd.concat([d for _, d in train]).reset_index(drop=True)\n",
        "valid = pd.concat([d for _, d in valid]).reset_index(drop=True)\n",
        "test = pd.concat([d for _, d in test]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPW1ZHwI2R4M",
        "outputId": "823feed8-c680-4c7f-b5e2-1631d87bb55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set num rows 116,109\n",
            "valid set num rows 14,573\n",
            "test set num rows 12,691\n",
            "without docstring rows 468,502\n"
          ]
        }
      ],
      "source": [
        "print(f'train set num rows {train.shape[0]:,}')\n",
        "print(f'valid set num rows {valid.shape[0]:,}')\n",
        "print(f'test set num rows {test.shape[0]:,}')\n",
        "print(f'without docstring rows {without_docstrings.shape[0]:,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glhtRalE2R4M"
      },
      "source": [
        "Preview what the training set looks like.  You can start to see how the data looks, the function tokens and docstring tokens are what will be fed downstream into the models.  The other information is important for diagnostics and bookeeping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "ucpBUY762R4M",
        "outputId": "38c3290c-5dfc-4568-942f-5a84188a5c93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index                 nwo                         path   function_name  \\\n",
              "0  88320     vmagamedov/hiku               hiku/compat.py  with_metaclass   \n",
              "1  24356  Contraz/demosys-py  demosys/effects/registry.py        get_dirs   \n",
              "2  24357  Contraz/demosys-py  demosys/effects/registry.py        polulate   \n",
              "3  24358  Contraz/demosys-py  demosys/effects/registry.py  get_effect_cls   \n",
              "4  96723  Contraz/demosys-py   demosys/view/controller.py             run   \n",
              "\n",
              "   lineno                                  original_function  \\\n",
              "0      11  def with_metaclass(meta, *bases):\\n    \"\"\"Crea...   \n",
              "1      30  def get_dirs(self):\\n    \"\"\"\\n        Get all ...   \n",
              "2      37  def polulate(self, effect_list):\\n    \"\"\"\\n   ...   \n",
              "3      52  def get_effect_cls(self, module_name):\\n    \"\"...   \n",
              "4      22  def run(manager=None):\\n    \"\"\"\\n    Initializ...   \n",
              "\n",
              "                                     function_tokens  \\\n",
              "0  with metaclass meta bases class metaclass meta...   \n",
              "1  get dirs self for in self effects items yield ...   \n",
              "2  polulate self effect list for effect in effect...   \n",
              "3  get effect cls self module name module importl...   \n",
              "4  run manager none global manager manager manage...   \n",
              "\n",
              "                                    docstring_tokens  \\\n",
              "0               create a base class with a metaclass   \n",
              "1  get all effect directories for registered effects   \n",
              "2                            find all effect modules   \n",
              "3        find and return an effect class in a module   \n",
              "4                            initialize load and run   \n",
              "\n",
              "                                                 url  \n",
              "0  https://github.com/vmagamedov/hiku/blob/master...  \n",
              "1  https://github.com/Contraz/demosys-py/blob/mas...  \n",
              "2  https://github.com/Contraz/demosys-py/blob/mas...  \n",
              "3  https://github.com/Contraz/demosys-py/blob/mas...  \n",
              "4  https://github.com/Contraz/demosys-py/blob/mas...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4be1cc95-f38d-45bd-8c08-ca6ad7a6c848\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>nwo</th>\n",
              "      <th>path</th>\n",
              "      <th>function_name</th>\n",
              "      <th>lineno</th>\n",
              "      <th>original_function</th>\n",
              "      <th>function_tokens</th>\n",
              "      <th>docstring_tokens</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>88320</td>\n",
              "      <td>vmagamedov/hiku</td>\n",
              "      <td>hiku/compat.py</td>\n",
              "      <td>with_metaclass</td>\n",
              "      <td>11</td>\n",
              "      <td>def with_metaclass(meta, *bases):\\n    \"\"\"Crea...</td>\n",
              "      <td>with metaclass meta bases class metaclass meta...</td>\n",
              "      <td>create a base class with a metaclass</td>\n",
              "      <td>https://github.com/vmagamedov/hiku/blob/master...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24356</td>\n",
              "      <td>Contraz/demosys-py</td>\n",
              "      <td>demosys/effects/registry.py</td>\n",
              "      <td>get_dirs</td>\n",
              "      <td>30</td>\n",
              "      <td>def get_dirs(self):\\n    \"\"\"\\n        Get all ...</td>\n",
              "      <td>get dirs self for in self effects items yield ...</td>\n",
              "      <td>get all effect directories for registered effects</td>\n",
              "      <td>https://github.com/Contraz/demosys-py/blob/mas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24357</td>\n",
              "      <td>Contraz/demosys-py</td>\n",
              "      <td>demosys/effects/registry.py</td>\n",
              "      <td>polulate</td>\n",
              "      <td>37</td>\n",
              "      <td>def polulate(self, effect_list):\\n    \"\"\"\\n   ...</td>\n",
              "      <td>polulate self effect list for effect in effect...</td>\n",
              "      <td>find all effect modules</td>\n",
              "      <td>https://github.com/Contraz/demosys-py/blob/mas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24358</td>\n",
              "      <td>Contraz/demosys-py</td>\n",
              "      <td>demosys/effects/registry.py</td>\n",
              "      <td>get_effect_cls</td>\n",
              "      <td>52</td>\n",
              "      <td>def get_effect_cls(self, module_name):\\n    \"\"...</td>\n",
              "      <td>get effect cls self module name module importl...</td>\n",
              "      <td>find and return an effect class in a module</td>\n",
              "      <td>https://github.com/Contraz/demosys-py/blob/mas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>96723</td>\n",
              "      <td>Contraz/demosys-py</td>\n",
              "      <td>demosys/view/controller.py</td>\n",
              "      <td>run</td>\n",
              "      <td>22</td>\n",
              "      <td>def run(manager=None):\\n    \"\"\"\\n    Initializ...</td>\n",
              "      <td>run manager none global manager manager manage...</td>\n",
              "      <td>initialize load and run</td>\n",
              "      <td>https://github.com/Contraz/demosys-py/blob/mas...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4be1cc95-f38d-45bd-8c08-ca6ad7a6c848')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4be1cc95-f38d-45bd-8c08-ca6ad7a6c848 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4be1cc95-f38d-45bd-8c08-ca6ad7a6c848');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrZqkbDt2R4M"
      },
      "source": [
        "## Sorting the datasets\n",
        "This was done for addition of minimal padding tokens while training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['function_tokens_count'] = train['function_tokens'].str.split().str.len()\n",
        "valid['function_tokens_count'] = valid['function_tokens'].str.split().str.len()\n",
        "test['function_tokens_count'] = test['function_tokens'].str.split().str.len()"
      ],
      "metadata": {
        "id": "YS6R-PwyVHSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-RVErvTEWUYO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfBGOmd_2R4M"
      },
      "outputs": [],
      "source": [
        "train.sort_values(by=['function_tokens_count'], inplace=True)\n",
        "valid.sort_values(by=['function_tokens_count'], inplace=True)\n",
        "test.sort_values(by=['function_tokens_count'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKUnY3NY2R4M"
      },
      "outputs": [],
      "source": [
        "train.to_csv('train_sorted.csv')\n",
        "valid.to_csv('valid_sorted.csv')\n",
        "test.to_csv('test_sorted.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Part-I Data Collection & Preprocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
